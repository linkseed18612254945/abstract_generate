Machine translation	A computer natural language translation system inputs source language text and outputs target language text. The target language text is generated from the source language text using stored translation data generated from examples of source and corresponding target language texts. The stored translation data includes a plurality of translation components, each having surface data representative of the order of occurrence of language units in the component; dependency data related to the semantic relationship between language units in the component; and link data linking dependency data of language components of the source language with corresponding dependency data of language components of the target language. The surface data of the source language is used in analyzing the source language text, and the surface date of the target language is used in generating the target language text. The dependency data and link data is used in transforming the analysis of the source text into an analysis for the target language.	2009	71	7.8888888889
BLEU: a method for automatic evaluation of machine translation	Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.	2002	6773	423.3125
IBM Research Report Bleu: a Method for Automatic Evaluation of Machine Translation	Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused.	2002	5442	340.125
Minimum error rate training in statistical machine translation	ABSTRACT  Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality.	2003	2574	171.6
EuroParl: A parallel corpus for statistical machine translation	Abstract We collected a corpus of parallel text in 11 lan-guages from the proceedings of the European Par-liament, which are published on the web 1 . This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine trans-lation (SMT). We trained SMT systems for 110 lan-guage pairs, which reveal interesting clues into the challenges ahead.	2005	2203	169.4615384615
A parallel corpus for statistical machine translation	In this lecture we continue the study of machine translation in more details. In the previous lectrue, the translation model 3 was introduced. We will explain four other translation models and compare them with model 3. We also describe a computationally cheap learning algorithm for these models, given a set of bilingual texts. at the end, a HMM-based alignment model will be discussed briefly. 8.2 Quick Review As usual, we consider translation from source language F to the target language E, let’s say French to English. Using base rule, we can rewrite p(e|f) as p(e)p(f|e). Brown et al. [1] introduced word-by-word alignment between pairs of sentences f and e. One can think of alignment in different ways. We focus on many-to-one alignment from f to e and vector a is used to represent this alignment. We assume e has length l and f has length m, so the size of a is also m. aj = i means that fj; the jth word of f is associated with ei; the ith word of e. To learn from pairs of translated sentences we should have some idea about the alignment of French and English words. We can consider all possible alignments and assign appropriate probabilities to them to accordingly compute other parameters of the model. Given sentences f and e, we should compute the probability of a specific alignment a. p(a|f, e) = p(a, f|e) p(f|e) p(a, f|e) a ′ p(a ′, f|e)	2005	2120	163.0769230769
A Statistical Approach to Machine Translation	This chapter contains sections titled: Introduction, The Language Model, The Translation Model, Searching, Parameter Estimation, Two Pilot Experiments, Plans, References	1990	2041	72.8928571429
Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary	We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well 鈥 for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.	2002	2184	136.5
Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary	We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well 鈥 for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.	2002	2080	130
Automatic evaluation of machine translation quality using n-gram co-occurrence statistics	"Abstract Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an ""evaluation understudy"", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research."	2002	1410	88.125
A Hierarchical Phrase-Based Model for Statistical Machine Translation.	ABSTRACT  We present a statistical phrase-based transla- tion model that uses hierarchical phrases— phrases that contain subphrases. The model is formally a synchronous context-free gram- mar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax- based translation systems without any lin- guistic commitment. In our experiments us- ing BLEU as a metric, the hierarchical phrase- based model achieves a relative improve- ment of 7.5% over Pharaoh, a state-of-the-art phrase-based system.	2005	1394	107.2307692308
Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics	In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency.	2004	1421	101.5
Discriminative training and maximum entropy models for statistical machine translation	We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source -channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.	2002	1402	87.625
Machine translation and translation theory /	"ABSTRACT  ve interpreting. It consists in the mapping of a text onto a cognitive macrostructural scene that specifies the communicative intentions and strategies of the speaker. Knowing the latter is of considerable help in human interpreting, and is therefore likely to be useful for machine interpreting as well. Unfortunately, since speakers seldom make their intentions explicit, the macrostructural scenes have to be provided manually to the translation system, presumably in a training or preediting phase, and on the specifics of this training phase the author is very brief: ""If I have understood the literature correctly, a connectionist, self-adaptive system will be most suitable for the necessary training, or at least a hybrid model"" (p. 33). Equally brief are the comments on the feasibility of this enterprise, but with the insistence on the importance of macrostructural properties of texts the author strikes a chord that reverberates throughout the entire volume. Monika Doherty's ""Textual g"	1997	1315	62.619047619
Statistical Significance Tests for Machine Translation Evaluation	Abstract If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statis.	2004	934	66.7142857143
Findings of the 2011 Workshop on Statistical Machine Translation	This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.	2011	681	97.2857142857
Findings of the 2012 workshop on statistical machine translation	This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. 1	2012	611	101.8333333333
Findings of the 2009 workshop on statistical machine translation	This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.	2009	770	85.5555555556
Automatic evaluation of machine translation quality using n-gram co-occurrence statistics	react-text: 444 Evaluation plays a crucial role in development of Machine translation systems. In order to judge the quality of an existing MT system i.e. if the translated output is of human translation quality or not, various automatic metrics exist. We here present the implementation results of different metrics when used on Hindi language along with their comparisons, illustrating how effective are these...  /react-text  react-text: 445   /react-text [Show full abstract]	2002	1310	81.875
The Alignment Template Approach to Statistical Machine Translation	Abstract A phrase-based statistical machine translation approach 鈥 the alignment template approach 鈥 is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various sys- tem components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine transla- tion evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.	2004	1071	76.5
A comparative study on reordering constraints in statistical machine translation	Abstract In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary wordreorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.	2003	965	64.3333333333
Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation	This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation.	2010	713	89.125
Further meta-evaluation of machine translation	Abstract This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involv- ing hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the cor- relation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodol- ogy by measuring intra- and inter-annotator agreement, and collecting timing information.	2008	436	43.6
Clause restructuring for statistical machine translation	We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2 % Bleu score for a baseline system to 26.8 % Bleu score for the system with reordering, a statistically significant improvement.	2005	545	41.9230769231
Improved Alignment Models for Statistical Machine Translation	Abstract In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 Statistical Machine Translation The goal of machine trans...	1999	683	35.9473684211
Exploiting Similarities among Languages for Machine Translation	Abstract:  Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.	2013	316	63.2
Findings of the 2009 workshop on statistical machine translation	This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.	2009	711	79
Tree-to-string alignment template for statistical machine translation	Abstract We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment be- tween a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and per- forming reordering at both low and high levels. The model is linguistically syntax- based because TATs are extracted auto- matically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to pro- duce a source parse tree and then ap- ply TATs to transform the tree into a tar- get string. Our experiments show that the TAT-based model significantly outper- forms Pharaoh, a state-of-the-art decoder for phrase-based models.	2006	506	42.1666666667
A phrase-based, joint probability model for statistical machine translation	A machine translation (MT) system utilizes a phrase-based joint probability model. The model is used to generate source and target language sentences simultaneously. In an embodiment, the model learns phrase-to-phrase alignments from word-to-word alignments generated by a word-to-word statistical MT system. The system utilizes the joint probability model for both source-to-target and target-to-source translation applications.	2002	552	34.5
Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models	We describe Pharaoh, a freely available decoder for phrase-based statistical machine translation models. The decoder is the implement at ion of an efficient dynamic programming search algorithm with lattice generation and XML markup for external components.	2004	802	57.2857142857
(Meta-) evaluation of machine translation	ABSTRACT  This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies.	2007	438	39.8181818182
Large language models in machine translation	Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.	2012	282	47
Large Language Models in Machine Translation	This paper reports on the benefits of large- scale statistical language modeling in ma- chine translation. A distributed infrastruc- ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabil- ities for fast, single-pass decoding. We in- troduce a new smoothing method, dubbed	2007	412	37.4545454545
Large Language Models in Machine Translation	This paper reports on the benefits of large- scale statistical language modeling in ma- chine translation. A distributed infrastruc- ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabil- ities for fast, single-pass decoding. We in- troduce a new smoothing method, dubbed	2007	391	35.5454545455
Building a Large-Scale Knowledge Base for Machine Translation	Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PANGLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.	1994	538	22.4166666667
Findings of the 2013 Workshop on Statistical Machine Translation	This paper presents the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 1	2013	246	49.2
Findings of the 2013 Workshop on Statistical Machine Translation	We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 1	2013	246	49.2
Findings of the 2009 Workshop on Statistical Machine Translation	j schroeder ed ac uk This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.	2013	246	49.2
Discriminative training and maximum entropy models for statistical machine translation	We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source -channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.	2002	457	28.5625
Phrase-Based Statistical Machine Translation	This paper is based on the work carried out in the framework of the Verbmobil project, which is a limited-domain speech translation task (German-English). 	2002	417	26.0625
Confidence Estimation for Machine Translation	We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT output is correct are investigated, for both whole sentences and words. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation.	2009	308	34.2222222222
Improving Machine Translation Performance by Exploiting Non-Parallel Corpora	ABSTRACT  We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.	2005	390	30
Improving Machine Translation Performance by Exploiting Non-Parallel Corpora	Summary: We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.	2005	369	28.3846153846
Improving Statistical Machine Translation Using Word Sense Disambiguation	We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task鈥 and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary.	2007	346	31.4545454545
Batch tuning strategies for statistical machine translation	There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.	2012	224	37.3333333333
Minimum Bayes-Risk Decoding for Statistical Machine Translation	We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.	2004	386	27.5714285714
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation	Abstract We present Minimum Bayes-Risk (MBR) de- coding over translation lattices that compactly encode a huge number of translation hypothe- ses. We describe conditions on the loss func- tion that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Pap- ineni et al., 2001) that satisfies these condi- tions. The MBR decoding under this approx- imate BLEU is realized using Weighted Fi- nite State Automata. Our experiments show that the Lattice MBR decoder yields mod- erate, consistent gains in translation perfor- mance over N-best MBR decoding on Arabic- to-English, Chinese-to-English and English- to-Chinese translation tasks. We conduct a range of experiments to understand why Lat- tice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.	2008	312	31.2
Maximum entropy based phrase reordering model for statistical machine translation	CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1	2006	351	29.25
Maximum entropy based phrase reordering model for statistical machine translation	Abstract We propose a novel reordering model for phrase-based statistical machine transla- tion (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hier- archical phrasal reordering with general- ization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reorder- ing events of neighbor blocks from bilin- gual data. In our experiments on Chinese- to-English translation, this MaxEnt-based reordering model obtains significant im- provements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.	2006	347	28.9166666667
The mathematics of machine translation: parameter estimation	ABSTRACT We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.	1993	422	16.88
Fast and optimal decoding for machine translation ☆	A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to a set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. Unfortunately, examining more of the space leads to unacceptably slow decodings. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast but non-optimal greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.	2004	334	23.8571428571
Improving statistical machine translation using word sense disambiguation	CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): We show for the first time that incorporating the predictions of a word sense disambigua-tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese-English test sets, as well as producing sta-tistically significant improvements on the larger NIST Chinese-English MT task鈥 and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used au-tomatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation qual-ity still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strat-egy for integrating WSD into an SMT sys-tem, that performs fully phrasal multi-word disambiguation. Instead of directly incor-porating a Senseval-style WSD system, we redefine the WSD task to match the ex-act same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empir-ical evidence that lexical semantics are in-deed useful for SMT, despite claims to the contrary. 鈭桾his material is based upon work supported in part by	2007	323	29.3636363636
SPMT: statistical machine translation with syntactified target language phrases	We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1	2006	332	27.6666666667
Batch Tuning Strategies for Statistical Machine Translation	ABSTRACT  There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.	2012	220	36.6666666667
A Smorgasbord of Features for Statistical Machine Translation	ABSTRACT  We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.	2004	340	24.2857142857
A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model	ABSTRACT  In this paper, we propose a novel string-to- dependency algorithm for statistical machine translation. With this new framework, we em- ploy a target dependency language model dur- ing decoding to exploit long distance word relations, which are unavailable with a tra- ditional n-gram language model. Our ex- periments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-to- string system on the NIST 04 Chinese-English evaluation set.	2008	294	29.4
Better hypothesis testing for statistical machine translation: Controlling for optimizer instability	In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately	2011	223	31.8571428571
Syntax augmented machine translation via chart parsing	"ABSTRACT  We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" gener-ated by a chart parsing decoder operating on phrase tables augmented and general-ized with target language syntactic cate-gories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual train-ing corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to aug-ment (declare a syntactically motivated category for a phrase pair) and general-ize (form mixed terminal and nonterminal phrases) the phrase table into a synchro-nous bilingual grammar. We present re-sults on the French-to-English task for this workshop, representing significant im-provements over the workshop's baseline system. Our translation system is avail-able open-source under the GNU General Public License."	2006	301	25.0833333333
Syntax augmented machine translation via chart parsing	"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" gener-ated by a chart parsing decoder operating on phrase tables augmented and general-ized with target language syntactic cate-gories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual train-ing corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to aug-ment (declare a syntactically motivated category for a phrase pair) and general-ize (form mixed terminal and nonterminal phrases) the phrase table into a synchro-nous bilingual grammar. We present re-sults on the French-to-English task for this workshop, representing significant im-provements over the workshop's baseline system. Our translation system is avail-able open-source under the GNU General Public License."	2006	298	24.8333333333
The Mathematics of Machine Translation: Parameter Estimation	this paper, we focus on the translation modeling problem. Before we turn to this problem, however, we should address an issue that may be a concern to some readers: Why do we estimate Pr(e) and Pr(fle) rather than estimate Pr(elf ) directly? We are really interested in this latter probability. Wouldn't we reduce our problems from three to two by this direct approach? If we can estimate Pr(fle) adequately, why can't we just turn the whole process around to estimate Pr(elf)? To understand this, imagine that we divide French and English strings into those that are well-formed and those that are ill-formed. This is not a precise notion. We have in mind that strings like Il va la bibliothque, or I live in a house, or even Colorless green ideas sleep furiously are well-formed, but that strings like lava I1 bibliothque or a I in live house are not. When we translate a French string into English, we can think of ourselves as springing from a well-formed French string into the sea of well-formed English strings with the hope of landing on a good one. It is important, therefore, that our model for Pr(elf ) concentrate its probability as much as possible on wellformed English strings. But it is not important that our model for Pr(fle ) concentrate its probability on well-formed French strings. If we were to reduce the probability of all well-formed French strings by the same factor, spreading the probability thus 265 liberated over ill-formed French strings, there would be no effect on our translations: the argument that maximizes some function f(x) also maximizes cf(x) for any positive constant c. As we shall see below, our translation models are prodigal, spraying probability all over the place, most of it on ill-formed French strings. In fact, as we discuss in Section 4.5, two...	1993	392	15.68
Improved statistical machine translation using paraphrases	ABSTRACT  Parallel corpora are crucial for training SMT systems. However, for many lan- guage pairs they are available only in very limited quantities. For these lan- guage pairs a huge portion of phrases en- countered at run-time will be unknown. We show how techniques from paraphras- ing can be used to deal with these oth- erwise unknown source language phrases. Our results show that augmenting a state- of-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we in- crease the coverage of unique test set un- igrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.	2006	301	25.0833333333
Learning non-isomorphic tree mappings for Machine Translation	ABSTRACT  Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. Unlike previous statistical formalisms (limited to isomorphic trees), allows local distortion of the tree topology. We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.	2003	337	22.4666666667
Word sense disambiguation improves statistical machine translation	Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems. In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD system improves the performance of a state-of-the-art statistical MT system on an actual translation task. Furthermore, the improvement is statistically significant. 漏 2007 Association for Computational Linguistics.	2007	288	26.1818181818
Optimizing Chinese word segmentation for machine translation performance	Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance. We find that other factors such as segmentation consistency and granularity of Chinese words can be more important for machine translation. Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU. We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase. 1	2008	264	26.4
Experiments in domain adaptation for statistical machine translation	ABSTRACT  The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical ma-chine translation systems to a special do-main (here: news commentary), when most of the training data is from a dif-ferent domain (here: European Parliament speeches). This paper also gives a descrip-tion of the submission of the University of Edinburgh to the shared task.	2007	282	25.6363636364
Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.	Abstract In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on long- est common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into ac- count sentence level structure similarity natu- rally and identifies longest co-occurring in- sequence n-grams automatically. The second method relaxes strict n-gram matching to skip- bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empiri- cal results show that both methods correlate with human judgments very well in both ade- quacy and fluency.	2004	332	23.7142857143
Better hypothesis testing for statistical machine translation: Controlling for optimizer instability	In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately	2011	212	30.2857142857
Statistical Machine Translation by Parsing	Designers of statistical machine translation (SMT) systems have begun trying to exploit tree-structured syntactic information. This article offers a coherent algorithmic framework to facilitate such efforts. Our main contribution is a generalization of the common notion of parsing. In an ordinary parser, the input is a single string, and the grammar ranges over strings. In order to use syntactic information, an SMT system requires generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Three particular generalizations, connected by some trivial glue, are all that is necessary for syntax-aware SMT: A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the orrespondence relation between these structures. When a parser's input can have fewer dimensions than the parser's grammar, it is a translator. When a parser's grammar can have fewer dimensions than the parser's input, it is a synchronizer. This article offers a guided tour of these generalized parsing algorithms. It culminates with a recipe for using generalized parsing algorithms to train and apply a syntax-aware SMT system.	2004	183	13.0714285714
Discriminative Reranking for Machine Translation	ABSTRACT  This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n- best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.	2004	183	13.0714285714
Synchronous binarization for machine translation	ABSTRACT  Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla- tion output, but are often very computa- tionally intensive. 	2006	169	14.0833333333
Word-sense disambiguation for machine translation	ABSTRACT  In word sense disambiguation, a system attempts to determine the sense of a word from contextual fea- tures. Major barriers to building a high-performing word sense disambiguation system include the dif- ficulty of labeling data for this task and of pre- dicting fine-grained sense distinctions. These is- sues stem partly from the fact that the task is be- ing treated in isolation from possible uses of au- tomatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a signif- icant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machine- translation task and can effectively and accurately prune the set of candidate translations for a word.	2005	169	13
Machine translation and telecommunications system using user ID data to select dictionaries	A machine translation and telecommunications system includes a machine translation engine for translation of input text from a source language to a target language, a dictionary database including a core dictionary and a plurality of sublanguage (domain) dictionaries usable for translation from a source to a target language, a receiving interface for receiving text input from any of a plurality of users, each text input being accompanied by control information including user ID data indicative of one or more sublanguages preferred by a particular user, an output interface, and a dictionary control module coupled to the receiving interface responsive to the user ID data indicative of a sublanguage preference of a particular user for selecting a corresponding sublanguage dictionary of the dictionary database to be used by the machine translation engine along with the core dictionary for performing translation of the particular user's text input. User dictionaries can be maintained and selected to enhance translation accuracy in the same manner. The dictionary database encompassing core, sublanguage (domain), and user dictionaries is cumulated for greater capability over time through the use of dictionary maintenance utilities for updating the dictionaries.	1996	214	9.7272727273
Active Learning and Crowd-Sourcing for Machine Translation.	In recent years, corpus based approaches to machine translation have become predominant, with Statistical Machine Translation (SMT) being the most actively progressing area. Success of these approaches depends on the availability of parallel corpora. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	2010	133	16.625
HMM Word and Phrase Alignment for Statistical Machine Translation	Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.	2008	154	15.4
Large-Scale Dictionary Construction for Foreign Language Tutoring and Interlingual Machine Translation	This paper describes techniques for automatic construction of dictionaries for use in large-scale foreign language tutoring (FLT) and interlingual machine translation (MT) systems. The dictionaries are based on a language-independent representation called “lexical conceptual structure” (LCS). A primary goal of the LCS research is to demonstrate that synonymous verb senses share distributional patterns. We show how the syntax–semantics relation can be used to develop a lexical acquisition approach that contributes both toward the enrichment of existing online resources and toward the development of lexicons containing more complete information than is provided in any of these resources alone. We start by describing the structure of the LCS and showing how this representation is used in FLT and MT. We then focus on the problem of building LCS dictionaries for large-scale FLT and MT. First, we describe authoring tools for manual and semi-automatic construction of LCS dictionaries; we then present a more sophisticated approach that uses linguistic techniques for building word definitions automatically. These techniques have been implemented as part of a set of lexicon-development tools used in the milt FLT project.	1997	202	9.619047619
Machine translation: an introductory guide	The article provides insights for dental hygienists on using machine translation (MT). Tips on using MT are enumerated, one of which is avoiding complex sentence to avoid confusing translation software. The methods through which MT operates include those based on dictionary entries and based statistical rules. Also cited are popular and free MT sites such as Yahoo! Babel Fish at http://translation2.paralink.com/ and http://www.systranet.com.	1994	315	13.125
Machine translation and telecommunications system	A machine translation and telecommunications system automatically translates input text in a source language to output text in a target language using a dictionary database (22) containing core language dictionaries for general words, a plurality of sublanguage dictionaries for specialized words of different domains or user groups, and a plurality of user dictionaries for individualized words used by different users. The system includes a receiving interface (11) for receiving input from a sender, in the form of electronic text, facsimile (graphics) input, or page image data, and an output module (30) for sending translated output text to any designated recipient(s). The input text is accompanied by a cover page or header (50) identifying the sender, one or more recipients, their addresses, the source/target languages of the text, any sublanguage(s) applicable to the input text, and any formatting requirements for the output text. The system uses the cover page or header data to select the core language, sublanguage, and/or user dictionaries to be used for translation processing, to format the translated output text, and to send the output to the recipient(s) at the designated address(es). The dictionary database (22) can cumulate and evolve over time by adding new words as scratch entries to the user dictionaries and, through the use of dictionary maintenance utilities, by updating and/or moving the scratch entries to higher-level subdomain, domain, or even core dictionaries as their usage gains currency.	1996	309	14.0454545455
Statistical Machine Translation for Query Expansion in Answer Retrieval	ABSTRACT  We present an approach to query expan- sion in answer retrieval that uses Statisti- cal Machine Translation (SMT) techniques to bridge the lexical gap between ques- tions and answers. SMT-based query ex- pansion is done by i) using a full-sentence paraphraser to introduce synonyms in con- text of the entire query, and ii) by trans- lating query terms into answer terms us- ing a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion tech- niques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMT- based expansion improves retrieval perfor- mance over local expansion and over re- trieval without expansion.	2008	235	23.5
Loosely tree-based alignment for machine translation	ABSTRACT  We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.	2003	279	18.6
Chinese Syntactic Reordering for Statistical Machine Translation	ABSTRACT  Syntactic reordering approaches are an ef- fective method for handling word-order dif- ferences between source and target lan- guages in statistical machine translation (SMT) systems. This paper introduces a re- ordering approach for translation from Chi- nese to English. We describe a set of syntac- tic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to an- alyze the accuracy and impact of different types of reordering rules.	2007	241	21.9090909091
Manual and automatic evaluation of machine translation between European languages	We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the Bleu score and manually on fluency and adequacy.	2006	246	20.5
A comparison of alignment models for statistical machine translation	ABSTRACT  In this paper, we present and compare various alignment models for statistical machine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a rened annotation scheme to produce suitable reference alignments. The presented alignment models are then compared according to this error criterion. We also compare the impact of dierent alignment models on the translation quality of the Alignment Template system. 1 Introduction In statistical machine translation (SMT) it is necessary to model the translation probability P r(f J 1 je I 1 ). Here f J 1 = f denotes the (French) source and e I 1 = e denotes the (English) target string. Most SMT models (Brown et al., 1993; Vogel et al., 1996) try to model word-to-word correspondences between source and target words using an alignment mapping from source position j to target position i = a j . Formally, we can rewrite the pro...	2000	306	17
PHRASE-BASED JOINT PROBABILITY MODEL FOR STATISTICAL MACHINE TRANSLATION	A machine translation (MT) system utilizes a phrase-based joint probability model. The model is used to generate source and target language sentences simultaneously. In an embodiment, the model learns phrase-to-phrase alignments from word-to-word alignments generated by a word-to-word statistical MT system. The system utilizes the joint probability model for both source-to-target and target-to-source translation applications.	2011	175	25
Re-examining machine translation metrics for paraphrase identification	ABSTRACT  We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community.	2012	91	15.1666666667
Recent Advances in Example-Based Machine Translation	Book Review: Recent Advances in Example-Based Machine Translation Michael Carl and Andy Way (editors) (Universitat des Saarlandes and Dublin City University) Dordrecht : Kluwer Academic Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean Veronis, volume 21), 2003 , xxxi+482 pp; hardbound, ISBN 1-4020-1400-7 , $173.00, f115.00, 180.00	2004	158	11.2857142857
Deeper sentiment analysis using machine translation technology	ABSTRACT  This paper proposes a new paradigm for senti-ment analysis: translation from text documents to a set of sentiment units. The techniques of deep language analysis for machine translation are applicable also to this kind of text mining task. We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based ma-chine translation engine.	2004	158	11.2857142857
Following directions using statistical machine translation	Mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. In this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and a map of an environment built by a robot. Our approach uses training data to learn to translate from natural language instructions to an automatically-labeled map. The complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map. As a result, our technique can efficiently handle uncertainty in both map labeling and parsing. Our experiments demonstrate the promising capabilities achieved by our approach.	2010	108	13.5
Generation of Word Graphs in Statistical Machine Translation	ABSTRACT  Statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. We describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. The advantage is that these hypotheses can be rescored using a refined language or translation model.	2002	164	10.25
Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios	Corpus based approaches to automatic translation such as Example Based and Statistical Machine Translation systems use large amounts of parallel data created by humans to train mathematical models for automatic language translation. Large scale parallel data generation for new language pairs requires intensive human effort and availability of fluent bilinguals or expert translators. Therefore it becomes immensely difficult and expensive to provide state-of-the-art Machine Translation (MT) systems for rare languages.	2012	98	16.3333333333
A comparative study on reordering constraints in statistical machine translation	ABSTRACT  In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schröder numbers.We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%.	2003	157	10.4666666667
A novel string-to-string distance measure with applications to machine translation evaluation	Abstract We introduce a string-to-string distance measure which extends the edit distance by block transpositions as constant cost edit operation. An algorithm for the calculation of this distance measure in polynomial time is presented. We then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. The correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. In general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level.	2003	161	10.7333333333
The Candide system for machine translation	ABSTRACT  We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 1. Introduction Candide is an experimental computer program, now in its fifth year of development at IBM, for translation of French text to English text. Our goal is to perform fully-automatic, high-quality text-to-text translation. However, because we are still far from achieving this goal, the program can be used in both fully-automatic and translator's-assistant modes. Our approach is founded upon the statistical analysis of language....	1994	179	7.4583333333
Word-sense disambiguation for machine translation	ABSTRACT  In word sense disambiguation, a system attempts to determine the sense of a word from contextual fea- tures. Major barriers to building a high-performing word sense disambiguation system include the dif- ficulty of labeling data for this task and of pre- dicting fine-grained sense distinctions. These is- sues stem partly from the fact that the task is be- ing treated in isolation from possible uses of au- tomatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a signif- icant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machine- translation task and can effectively and accurately prune the set of candidate translations for a word.	2005	144	11.0769230769
Domain adaptation for machine translation by mining unseen words	We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs. 1	2011	98	14
Improving machine translation quality with automatic named entity recognition	ABSTRACT  Named entities create serious problems for state-of-the-art commercial machine translation (MT) systems and often cause translation failures beyond the local context, affecting both the overall morphosyntactic well-formedness of sentences and word sense disambiguation in the source text. We report on the results of an experiment in which MT input was processed using output from the named entity recognition module of Sheffield's GATE information extraction (IE) system. The gain in MT quality indicates that specific components of IE technology could boost the performance of current MT systems.	2003	147	9.8
Translation Engines: Techniques for Machine Translation	Machine translation (MT) is the area of computer science and applied linguistics dealing with the translation of human languages such as English and German. MT on the Internet has become an important tool by providing fast, economical and useful translations. With globalisation and expanding trade, demand for translation is set to grow. Translation Engines covers theoretical and practical aspects of MT, both classic and new, including: - Character sets and formatting languages - Translation memory - Linguistic and computational foundations - Basic computational linguistic techniques - Transfer and interlingua MT - Evaluation Software accompanies the text, providing readers with hands on experience of the main algorithms.	1999	167	8.7894736842
Tailoring Word Alignments to Syntactic Machine Translation	Domain AdaptationJohn Blitzer and Hal Daumé IIIClassical “Single-domain” LearningPredict:Horrible book, horrible. This book was horrible. I read half, suffering from a headache the entire time, and eventually i lit it on fire. 1 less copy in the world. Don't waste your money. I wish i had the time spent reading this book back. It wasted my lifeSo the topic of ah the talk today is online learningDomain AdaptationSo the topic of ah the talk today is online learningEverything is happening online. Even the	2007	125	11.3636363636
Reordering constraints for phrase-based statistical machine translation	ABSTRACT  In statistical machine translation, the gen-eration of a translation hypothesis is com-putationally expensive. If arbitrary re-orderings are permitted, the search prob-lem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We in-vestigate different reordering constraints for phrase-based statistical machine trans-lation, namely the IBM constraints and the ITG constraints. We present effi-cient dynamic programming algorithms for both constraints. We evaluate the con-straints with respect to translation quality on two Japanese鈥揈nglish tasks. We show that the reordering constraints improve translation quality compared to an un-constrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield sta-tistically significant improvements com-pared to the unconstrained search.	2004	149	10.6428571429
Consensus Network Decoding for Statistical Machine Translation System Combination	This paper presents a simple and robust consensus decoding approach for combining multiple machine translation (MT) system outputs. A consensus network is constructed from an N-best list by aligning the hypotheses against an alignment reference, where the alignment is based on minimising the translation edit rate (TER). The minimum Bayes risk (MBR) decoding technique is investigated for the selection of an appropriate alignment reference. Several alternative decoding strategies proposed to retain coherent phrases in the original translations. Experimental results are presented primarily based on three-way combination of Chinese-English translation outputs, and also presents results for six-way system combination. It is shown that worthwhile improvements in translation performance can be obtained using the methods discussed.	2007	128	11.6363636364
Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval	Abstract. In this paper we present experiments concerning translation model adaptation for statistical machine translation. We develop a method to adapt translation models using information retrieval. The approach selects sentences similar to the test set.	2005	138	10.6153846154
Perplexity minimization for translation model domain adaptation in statistical machine translation	We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT). While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. We also explore adapting multiple (4鈥10) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set.	2012	88	14.6666666667
N-gram-based Machine Translation	This article describes in detail an n-gram approach to statistical machine translation. This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions. Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).	2006	200	16.6666666667
Machine Translation with Inferred Stochastic Finite-State Transducers	Summary: Finite-state transducers are models that are being used in different areas of pattern recognition and computational linguistics. One of these areas is machine translation, in which the approaches that are based on building models automatically from training examples are becoming more and more attractive. Finite-state transducers are very adequate for use in constrained tasks in which training samples of pairs of sentences are available. A technique for inferring finite-state transducers is proposed in this article. This technique is based on formal relations between finite-state transducers and rational grammars. Given a training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an $n$-gram) is inferred. This grammar is finally converted into a finite-state transducer. The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project.	2006	199	16.5833333333
Machine translation divergences: a formal description and proposed solution	ABSTRACT  This paper demonstrates that a systematic solution to the divergence problem can be derived from the forrealization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This forrealization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system	1994	269	11.2083333333
Improvements in Phrase-Based Statistical Machine Translation	ABSTRACT  In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system.	2004	225	16.0714285714
Joshua: An Open Source Toolkit for Parsing-based Machine Translation	We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. 1	2009	164	18.2222222222
Europarl: A Multilingual Corpus for Evaluation of Machine Translation	This paper reports results of the 1992 Evaluation of machine translation (MT) systems in the DARPA MT initiative and results of a Pre-test to the 1993 Evaluation. The DARPA initiative is unique in that the evaluated systems differ radically in languages translated, theoretical approach to system design, and intended end-user application. In the 1992 suite, a Comprehension Test compared the accuracy and interpretability of system and control outputs; a Quality Panel for each language pair judged the fidelity of translations from each source version. The 1993 suite evaluated adequacy and fluency and investigated three scoring methods.	2002	230	14.375
Minimum Error-Rate Training in Statistical Machine Translation Using Structural SVMs	Different works on training of log-linear interpolation models for statistical machine translation reported performance improvements by optimizing parameters with respect to translation quality.	2009	174	19.3333333333
Lattice-based Minimum Error Rate Training for Statistical Machine Translation	Abstract Minimum Error Rate Training (MERT) is an effective means to estimate the feature func- tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func- tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N - best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre- senting the exact error surface of all trans- lations that are encoded in a phrase lattice. Compared to N -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi- ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N -best MERT.	2008	178	17.8
ORANGE: a method for evaluating automatic evaluation metrics for machine translation	ABSTRACT  Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson's product moment correlation coefficient or Spearman's rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, Orange, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using Orange.	2004	222	15.8571428571
A non-contiguous tree sequence alignment-based model for statistical machine translation	Abstract The tree sequence based translation model al- lows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of sub- trees. This paper goes further to present a trans- lation model based on non-contiguous tree se- quence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps. Compared with the contiguous tree sequence- based model, the proposed model can well han- dle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the non- contiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chi- nese-English translation task show that the pro- posed model statistically significantly outper- forms the baseline systems.	2009	176	19.5555555556
Synchronous binarization for machine translation	ABSTRACT  Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla- tion output, but are often very computa- tionally intensive. The complexity is ex- ponential in the size of individual gram- mar rules due to arbitrary re-orderings be- tween the two languages, and rules ex- tracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by bi- narizing synchronous rules when possible and show that the resulting rule set signif- icantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.	2006	186	15.5
Review Article: Example-based Machine Translation	In the last ten years there has been a significant amount ofresearch in Machine Translation within a ``new'' paradigm ofempirical approaches, often labelled collectively as``Example-based'' approaches. The first manifestation of thisapproach caused some surprise and hostility among observers moreused to different ways of working, but the techniques were quicklyadopted and adapted by many researchers, often creating hybridsystems. This paper reviews the various research efforts withinthis paradigm reported to date, and attempts a categorisation ofdifferent manifestations of the general approach.	1999	232	12.2105263158
Combining Outputs from Multiple Machine Translation Systems	ABSTRACT  Currently there are several approaches to machine translation (MT) based on differ- ent paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple sys- tems. This paper describes three differ- ent approaches to MT system combina- tion. These combination methods oper- ate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.	2007	183	16.6363636364
Statistical machine translation : from single word models to alignment templates /	In diesear Arbeit werden neue Ans01tze zur Sprachübersetzung basierend auf statistischen Verfahren vorgestellt. Als Verallgemeinerung zu dem üblicherweise verwendeten source-channel Modell wird ein allgemeineres Modell basierend auf dem Maximum-Entropie-Prinzip vorgeschlagen. Es werden verschiedene Verfahren zur Bestimmung von Wort-Alignments unter Nutzung von statistischen und heuristischen Modellen beschrieben. Dabei werden insbesondere verschiedene Gl01ttungsverfahren, Methoden zur Integration zus01tzlicher Lexika und Trainingsverfahren verglichen. Eine detaillierte Bewertung der Alignment-Qualit01t wird durchgeführt indem die automatisch erstellten Wort-Alignments mit manuell erstellten Alignments verglichen werden. Aufbauend auf diesen grundlegenden einzelwortbasierten Alignment-Modellen wird dann ein phrasenbasiertes statistisches 05bersetzungsmodell, das Alignment Template Modell, vorgeschlagen. Für dieses Modell wird ein Trainingsverfahren und ein effizienter Suchalgorithmus basierend auf dem Prinzip der dynamischer Programmierung und Strahlsuche entwickelt. Weiterhin werden für zwei spezielle Anwendungsszenarien (interaktive 05bersetzung und 05bersetzung basierend auf verschiedenen mehrsprachigen Quelltexten) spezielle Suchverfahren entwickelt. Der beschriebene 05bersetzungsansatz wurde getestet für das deutsch-englische Verbmobil Korpus, das franz02sisch-englische Hansards Korpus und für chinesisch-englische Nachrichtentexte. Das entwickelte System erzielt dabei h01ufig deutlich bessere Ergebnisse als alternative Verfahren zur maschinellen 05bersetzung. In this work, new approaches for machine translation using statistical methods are described. In addition to the standard source-channel approach to statistical machine translation, a more general approach based on the maximum entropy principle is presented. Various methods for computing single-word alignments using statistical or heuristic models are described. Various smoothing techniques, methods to integrate a conventional dictionary and training methods are analyzed. A detailed evaluation of these models is performed by comparing the automatically produced word alignment with a manually produced reference alignment. Based on these fundamental single-word based alignment models, a new phrase-based translation model - the alignment template model - is suggested. For this model, a training and an efficient search algorithm is developed. For two specific applications (interactive translation and multi-source translation) specific search algorithms are developed. The suggested machine translation approach has been tested for the German-English Verbmobil task, the French-English Hansards task and for Chinese-English news text translation. Often, the obtained results have been significantly better than those obtained with alternative approaches to machine translation.Och, Franz Josef; Ney, Hermann	2002	207	12.9375
A polynomial-time algorithm for statistical machine translation	We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy.	1996	226	10.2727272727
Online Large-Margin Training for Statistical Machine Translation	Abstract We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-to-English translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1	2007	182	16.5454545455
Learning for semantic parsing with statistical machine translation	ABSTRACT  We present a novel statistical approach to semantic parsing, WASP, for construct- ing a complete, formal meaning represen- tation of a sentence. A semantic parser is learned given a set of sentences anno- tated with their correct meaning represen- tations. The main innovation of WASP is its use of state-of-the-art statistical ma- chine translation techniques. A word alignment model is used for lexical acqui- sition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods re- quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.	2006	192	16
A tree-to-tree alignment-based model for statistical machine translation	ABSTRACT  This paper presents a novel statistical machine translation (SMT) model that uses tree-to-tree alignment between a source parse tree and a target parse tree. The model is formally a probabilistic synchronous tree-substitution grammar (STSG) that is a collection of aligned elementary tree pairs with mapping probabilities (which are automatically learned from word-aligned bi-parsed parallel texts). Unlike previous syntax-based SMT models, this new model supports multi-level global structure distortion of the tree typology and can fully utilize the source and target parse tree structure features, which gives our system more expressive power and flexibility. The experimental results on the HIT bi-parsed text show that our method performs significantly better than Pharaoh, a state-of-the-art phrase-based SMT system, and other syntax-based methods, such as the synchronous CFG-based method on the small dataset.	2007	184	16.7272727273
Computing Consensus Translation for Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment	ABSTRACT  This paper describes a novel method for computing a consensus translation from the outputs of multiple machine trans-lation (MT) systems. The outputs are combined and a possibly new transla-tion hypothesis can be generated. Simi-larly to the well-established ROVER ap-proach of (Fiscus, 1997) for combining speech recognition hypotheses, the con-sensus translation is computed by voting on a confusion network. To create the con-fusion network, we produce pairwise word alignments of the original machine trans-lation hypotheses with an enhanced sta-tistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	2006	190	15.8333333333
Discriminative instance weighting for domain adaptation in statistical machine translation	We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.	2010	140	17.5
Machine Translation: Past, Present, Future	Most computational linguists are probably aware that machine translation (MT) has provided the impetus for a number of important advances in linguistics and computing over the past 40 years. But even those who have worked on MT could not have fully appreciated.	1986	237	7.40625
Machine translation and telecommunications system using user ID data to select dictionaries	A machine translation and telecommunications system includes a machine translation engine for translation of input text from a source language to a target language, a dictionary database including a core dictionary and a plurality of sublanguage (domain) dictionaries usable for translation from a source to a target language, a receiving interface for receiving text input from any of a plurality of users, each text input being accompanied by control information including user ID data indicative of one or more sublanguages preferred by a particular user, an output interface, and a dictionary control module coupled to the receiving interface responsive to the user ID data indicative of a sublanguage preference of a particular user for selecting a corresponding sublanguage dictionary of the dictionary database to be used by the machine translation engine along with the core dictionary for performing translation of the particular user's text input. User dictionaries can be maintained and selected to enhance translation accuracy in the same manner. The dictionary database encompassing core, sublanguage (domain), and user dictionaries is cumulated for greater capability over time through the use of dictionary maintenance utilities for updating the dictionaries.	1996	214	9.7272727273
Active Learning and Crowd-Sourcing for Machine Translation	In recent years, corpus based approaches to machine translation have become predominant, with Statistical Machine Translation (SMT) being the most actively progressing area. Success of these approaches depends on the availability of parallel corpora. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	2010	135	16.875
Predicting the Sentence-Level Quality of Machine Translation Systems	We investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available. The problem is addressed as a regression task and a method that takes into account t...	2009	146	16.2222222222
The METEOR metric for automatic evaluation of machine translation	The Meteor  Automatic Metric for Machine Translation evaluation, originally developed and released in 2004, was designed with the explicit goal of producing sentence-level scores which correlate well with human judgments of translation quality. Several key design decisions were incorporated into Meteor  in support of this goal. In contrast with IBM鈥檚 Bleu , which uses only precision-based features, Meteor  uses and emphasizes recall in addition to precision, a property that has been confirmed by several metrics as being critical for high correlation with human judgments. Meteor  also addresses the problem of reference translation variability by utilizing flexible word matching, allowing for morphological variants and synonyms to be taken into account as legitimate correspondences. Furthermore, the feature ingredients within Meteor  are parameterized, allowing for the tuning of the metric鈥檚 free parameters in search of values that result in optimal correlation with human judgments. Optimal parameters can be separately tuned for different types of human judgments and for different languages. We discuss the initial design of the Meteor  metric, subsequent improvements, and performance in several independent evaluations in recent years.	2009	150	16.6666666667
HMM Word and Phrase Alignment for Statistical Machine Translation	Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.	2008	154	15.4
Apertium: A free/open-source platform for rule-based machine translation	Apertium is a free/open-source platform for rule-based machine translation. It is being widely used to build machine translation systems for a variety of l...	2011	125	17.8571428571
Demonstration of Joshua: an open source toolkit for parsing-based machine translation	ABSTRACT  We describe Joshua, an open source toolkit for statistical machine transla- tion. Joshua implements all of the algo- rithms required for synchronous context free grammars (SCFGs): chart-parsing, n- gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and dis- tributed computing techniques for scala- bility. We demonstrate that the toolkit achieves state of the art translation per- formance on the WMT09 French-English translation task.	2009	147	16.3333333333
Statistical Machine Translation by Parsing	Designers of statistical machine translation (SMT) systems have begun trying to exploit tree-structured syntactic information. This article offers a coherent algorithmic framework to facilitate such efforts. Our main contribution is a generalization of the common notion of parsing. In an ordinary parser, the input is a single string, and the grammar ranges over strings. In order to use syntactic information, an SMT system requires generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Three particular generalizations, connected by some trivial glue, are all that is necessary for syntax-aware SMT: A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the orrespondence relation between these structures. When a parser's input can have fewer dimensions than the parser's grammar, it is a translator. When a parser's grammar can have fewer dimensions than the parser's input, it is a synchronizer. This article offers a guided tour of these generalized parsing algorithms. It culminates with a recipe for using generalized parsing algorithms to train and apply a syntax-aware SMT system.	2004	183	13.0714285714
Discriminative Reranking for Machine Translation	ABSTRACT  This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n- best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.	2004	183	13.0714285714
Synchronous binarization for machine translation	ABSTRACT  Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla- tion output, but are often very computa- tionally intensive. The complexity is ex- ponential in the size ...	2006	169	14.0833333333
Word-sense disambiguation for machine translation	ABSTRACT  In word sense disambiguation, a system attempts to determine the sense of a word from contextual fea- tures. Major barriers to building a high-performing word sense disambiguation system include the dif- ficulty of labeling data for this task and of pre- dicting fine-grained sense distinctions. These is- sues stem partly from the fact that the task is be- ing treated in isolation from possible uses of au- tomatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a signif- icant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machine- translation task and can effectively and accurately prune the set of candidate translations for a word.	2005	169	13
Morphological analysis for statistical machine translation	We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities. The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into the sequence of prefix(es)-stem-suffix(es) and part-of-speech tagging of the parallel corpus. The algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry. The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs.	2004	176	12.5714285714
Active Learning and Crowd-Sourcing for Machine Translation.	In recent years, corpus based approaches to machine translation have become predominant, with Statistical Machine Translation (SMT) being the most actively progressing area. Success of these approaches depends on the availability of parallel corpora. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	2010	133	16.625
Experiments And Prospects Of Example-Based Machine Translation	"ABSTRACT  EBMT (Example-Based Machine Translation) is proposed. EBMT retrieves similar examples (pairs of source phrases, sentences, or texts and their translations) from a tahase of examples, adapting the examples to franslate a new input. EBMT has the following features: (1) It is easily upgr, zled simply by inputting appropriate examples to the database; (2) It assigns a reliability factor to the translation result; (3) It is accelerated effectively by both indexing axi parallel computing; (4) It is robust because of best-match reasoning; (5) It well utilizes translator expertise. A prototype system has been implemented to deal with a difficult Iranslation problem fee conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form 'lq a no N2"" into English. The system has achieved about a 78% success rate on average. This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the broad applicability of EBMT to several difficult translation problems fee RBMT discusses the advantages of integrating EBMT with RBMT."	1991	211	7.8148148148
Word reordering and a dynamic programming beam search algorithm for statistical machine translation	"Summary: In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in [{\it P. F. Brown}, {\it S. A. Della Pietra}, {\it V. J. Della Pietra} and {\it R. L. Mercer}, 鈥淭he mathematics of statistical machine translation: parameter estimation"", ibid. 19, No. 2, 263鈥311 (1993)]. Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article."	2003	178	11.8666666667
Large-Scale Dictionary Construction for Foreign Language Tutoring and Interlingual Machine Translation	This paper describes techniques for automatic construction of dictionaries for use in large-scale foreign language tutoring (FLT) and interlingual machine translation (MT) systems. The dictionaries are based on a language-independent representation called “lexical conceptual structure” (LCS). A primary goal of the LCS research is to demonstrate that synonymous verb senses share distributional patterns. We show how the syntax–semantics relation can be used to develop a lexical acquisition approach that contributes both toward the enrichment of existing online resources and toward the development of lexicons containing more complete information than is provided in any of these resources alone. We start by describing the structure of the LCS and showing how this representation is used in FLT and MT. We then focus on the problem of building LCS dictionaries for large-scale FLT and MT. First, we describe authoring tools for manual and semi-automatic construction of LCS dictionaries; we then present a more sophisticated approach that uses linguistic techniques for building word definitions automatically. These techniques have been implemented as part of a set of lexicon-development tools used in the milt FLT project.	1997	202	9.619047619
Machine translation of languages	, Machine translation of languages (pp. 15-23). New York : John Wiley & Sons.W. Weaver.Translation (1949). Machine Translation of Languages . 1955Weaver W. (1955). Translation (1949). In: Machine Translation of Languages, MIT ...	1955	227	3.6031746032
Statistical machine translation by parsing	ABSTRACT  In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algo- rithms that allow the input to consist of string tu- ples and/or the grammar to range over string tu- ples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statisti- cal machine translation system.	2004	167	11.9285714286
The CMU Statistical Machine Translation System	ABSTRACT  In this paper we describe the components of our statistical machine translation system.	2003	169	11.2666666667
Distortion Models for Statistical Machine Translation.	ABSTRACT  In this paper, we argue that n-gram lan- guage models are not sufficient to address word reordering required for Machine Trans- lation. We propose a new distortion model that can be used with existing phrase-based SMT decoders ...	2006	157	13.0833333333
Word-Level Confidence Estimation for Machine Translation	ABSTRACT  This article introduces and evaluates several different word-level confidence measures for ma- chine translation. These measures provide a method for labeling each word in an automatically generated translation as correct or incorrect. All approaches to confidence estimation presented here are based on word posterior probabilities. Different concepts of word posterior probabilities as well as different ways of calculating them will be introduced and compared. They can be divided into two categories: System-based methods that explore knowledge provided by the translation system that generated the translations, and direct methods that are independent of the translation system. The system-based techniques make use of system output, such as word graphs or N-best lists. The word posterior probability is determined by summing the probabilities of the sentences in the translation hypothesis space that contains the target word. The direct confidence measures take other knowledge sources, such as word or phrase lexica, into account. They can be applied to output from nonstatistical machine translation systems as well. Experimental assessment of the different confidence measures on various translation tasks and in several language pairs will be presented. Moreover, the application of confidence measures for rescoring of translation hypotheses will be investigated.	2007	150	13.6363636364
Computing consensus translation from multiple machine translation systems	We address the problem of computing a consensus translation given the outputs from a set of machine translation (MT) systems. The translations from the MT systems are aligned with a multiple string alignment algorithm and the consensus translation is then computed. We describe the multiple string alignment algorithm and the consensus MT hypothesis computation. We report on the subjective and objective performance of the multilingual acquisition approach on a limited domain spoken language application. We evaluate five domain-independent off-the-shelf MT systems and show that the consensus-based translation performance is equal to or better than any of the given MT systems, in terms of both objective and subjective measures.	2001	179	10.5294117647
A Maximum Entropy Word Aligner for Arabic-English Machine Translation.	Abstract This paper presents a maximum entropy word alignment algorithm for Arabic- English based on supervised training data. We demonstrate that it is feasible to cre- ate training material for problems in ma- chine translation and that a mixture of su- pervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly mod- els the link decisions. Significant improve- ment over traditional word alignment tech- niques is shown as well as improvement on several machine translation tests. Perfor- mance of the algorithm is contrasted with human annotation performance.	2005	161	12.3846153846
Active learning and crowdsourcing for machine translation in low resource scenarios	Ambati, V., Vogel, S., Carbonell, J.: Active learning and crowd-sourcing for machine translation. Language Resources and Evaluation (LREC) 7, 2169–2174 (2010)Ambati, V., Vogel, S., & Carbonell J. 2010. Active learning ...	2012	98	16.3333333333
The World Wide Web as a Resource for Example-Based Machine Translation Tasks	Abstract The WWW is two orders of magnitude larger than the largest corpora. Although noisy, web text presents language as it is used, and statistics derived from the Web can have practical uses in many NLP applications. For this reason, the WWW should be seen and studied as any other computationally available linguistic resource. In this article, we illustrate this by showing that an Example-Based approach to lexical choice for machine translation can use the Web as an adequate and free resource.	1999	187	9.8421052632
Recent Advances in Example-Based Machine Translation	Book Review: Recent Advances in Example-Based Machine Translation Michael Carl and Andy Way (editors) (Universitat des Saarlandes and Dublin City University) Dordrecht : Kluwer Academic Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean Veronis, volume 21), 2003 , xxxi+482 pp; hardbound, ISBN 1-4020-1400-7 , $173.00, f115.00, 180.00	2004	158	11.2857142857
Deeper sentiment analysis using machine translation technology	ABSTRACT  This paper proposes a new paradigm for senti-ment analysis: translation from text documents to a set of sentiment units. The techniques of deep language analysis for machine translation are applicable also to this kind of text mining task. We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based ma-chine translation engine.	2004	158	11.2857142857
Generation of Word Graphs in Statistical Machine Translation	ABSTRACT  Statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. We describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. The advantage is that these hypotheses can be rescored using a refined language or translation model.	2002	164	10.25
Two Approaches to Matching in Example-Based Machine Translation	"ABSTRACT  This paper describes two approaches to matching input strings with strings from a translation archive in the example-based machine translation paradigm - the more canonical ""chunking + matching + recombination"" method and an alternative method of matching at the level of complete sentences. The latter produces less exact matches while the former suffers from (often serious) translation quality lapses at the boundaries of recombined chunks. A set of text matching criteria was selected to reflect the trade-off between utility and computational price of each criterion. A metric for comparing text passages was devised and calibrated with the help of a specially constructed diagnostic example set. A partitioning algorithm was developed for finding an optimum ""cover"" of an input string by a set of best-matching shorter chunks. The results were evaluated in a monolingual setting using an existing MT post-editing tool: the distance between the input and its best match in the archive was calculated in terms of the number of keystrokes necessary to reduce the latter to the former. As a result, the metric was adjusted and an experiment was run to test the two EBMT methods, both on the training corpus and on the working corpus (or ""archive"") of some 6,500 sentences."	1993	189	7.56
Proceedings of the Fourth Workshop on Statistical Machine Translation	We describe two systems for English-to-Czech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-s...	2009	124	13.7777777778
Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios	Corpus based approaches to automatic translation such as Example Based and Statistical Machine Translation systems use large amounts of parallel data created by humans to train mathematical models for automatic language translation. Large scale parallel data generation for new language pairs requires intensive human effort and availability of fluent bilinguals or expert translators. Therefore it becomes immensely difficult and expensive to provide state-of-the-art Machine Translation (MT) systems for rare languages.	2012	98	16.3333333333
A Productivity Test of Statistical Machine Translation Post-Editing in a Typical Localisation Context	We evaluated the productivity increase of statistical MT post-editing as compared to traditional translation in a two-day test involving twelve participants translating from English to French, Italian, German, and Spanish. The test setup followed an empirical methodology. A random subset of the entire new content produced in our company during a given year was translated with statistical MT engines trained on data from the previous year. The translation environment recorded translation and post-editing times for each sentence. The results show a productivity increase for each participant, with significant variance across inviduals.	2010	118	14.75
A novel string-to-string distance measure with applications to machine translation evaluation	Abstract We introduce a string-to-string distance measure which extends the edit distance by block transpositions as constant cost edit operation. An algorithm for the calculation of this distance measure in polynomial time is presented. We then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. The correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. In general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level.	2003	161	10.7333333333
Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation	"Summary: In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in [{\it P. F. Brown}, {\it S. A. Della Pietra}, {\it V. J. Della Pietra} and {\it R. L. Mercer}, 鈥淭he mathematics of statistical machine translation: parameter estimation"", ibid. 19, No. 2, 263鈥311 (1993)]. Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article."	2003	164	10.9333333333
The Candide system for machine translation	ABSTRACT  We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 1. Introduction Candide is an experimental computer program, now in its fifth year of development at IBM, for translation of French text to English text. Our goal is to perform fully-automatic, high-quality text-to-text translation. However, because we are still far from achieving this goal, the program can be used in both fully-automatic and translator's-assistant modes. Our approach is founded upon the statistical analysis of language....	1994	179	7.4583333333
Word-sense disambiguation for machine translation	ABSTRACT  In word sense disambiguation, a system attempts to determine the sense of a word from contextual fea- tures. Major barriers to building a high-performing word sense disambiguation system include the dif- ficulty of labeling dat...	2005	144	11.0769230769
Re-examining machine translation metrics for paraphrase identification	ABSTRACT  We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community.	2012	91	15.1666666667
Translation Engines: Techniques for Machine Translation	Machine translation (MT) is the area of computer science and applied linguistics dealing with the translation of human languages such as English and German. MT on the Internet has become an important tool by providing fast, economical and useful translations. With globalisation and expanding trade, demand for translation is set to grow. Translation Engines covers theoretical and practical aspects of MT, both classic and new, including: - Character sets and formatting languages - Translation memory - Linguistic and computational foundations - Basic computational linguistic techniques - Transfer and interlingua MT - Evaluation Software accompanies the text, providing readers with hands on experience of the main algorithms.	1999	167	8.7894736842
Following directions using statistical machine translation	Mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. In this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and a map of an environment built by a robot. Our approach uses training data to learn to translate from natural language instructions to an automatically-labeled map. The complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map. As a result, our technique can efficiently handle uncertainty in both map labeling and parsing. Our experiments demonstrate the promising capabilities achieved by our approach.	2010	108	13.5
Word sense disambiguation vs. statistical machine translation	ABSTRACT  We directly investigate a subject of much recent debate: do word sense disambiga- tion models help statistical machine trans- lation quality? We present empirical re- sults casting doubt on this common, but unproved, assumption. Using a state-of- the-art Chinese word sense disambigua- tion model to choose translation candi- dates for a typical IBM statistical MT system, we find that word sense disam- biguation does not yield significantly bet- ter translation quality than the statistical machine translation system alone. Error analysis suggests several key factors be- hind this surprising finding, including in- herent limitations of current statistical MT architectures.	2005	149	11.4615384615
Lattice Minimum Bayes-Risk decoding for statistical machine translation	ABSTRACT  We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.	2008	127	12.7
Reordering constraints for phrase-based statistical machine translation	ABSTRACT  In statistical machine translation, the gen-eration of a translation hypothesis is com-putationally expensive. If arbitrary re-orderings are permitted, the search prob-lem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We in-vestigate different reordering constraints for phrase-based statistical machine trans-lation, namely the IBM constraints and the ITG constraints. We present effi-cient dynamic programming algorithms for both constraints. We evaluate the con-straints with respect to translation quality on two Japanese鈥揈nglish tasks. We show that the reordering constraints improve translation quality compared to an un-constrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield sta-tistically significant improvements com-pared to the unconstrained search.	2004	149	10.6428571429
A comparative study on reordering constraints in statistical machine translation	ABSTRACT  In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm.In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schröder numbers.We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%.	2003	157	10.4666666667
Machine translation evaluation versus quality estimation	Most evaluation metrics for machine translation (MT) require reference translations for each sentence in order to produce a score reflecting certain aspects of its quality. The de facto metrics, BLEU and NIST, are known to have good correlation with human evaluation at the corpus level, but this is not the case at the segment level. As an attempt to overcome these two limitations, we address the problem of evaluating the quality of MT as a prediction task, where reference-independent features are extracted from the input sentences and their translation, and a quality score is obtained based on models produced from training data. We show that this approach yields better correlation with human evaluation as compared to commonly used metrics, even with models trained on different MT systems, language-pairs and text domains.	2010	84	10.5
Machine Translation With A Stochastic Grammatical Channel	Abstract We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation. As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversiontransduction model. However, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model.	1998	131	6.55
Multi-engine machine translation guided by explicit word matching	"Abstract We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to pro- duce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as ""black boxes"" and does not require any explicit cooperation from the original MT systems. An explicit word matcher is first used in order to identify the words that are common between the MT engine outputs. A decoding algorithm then uses this information, in conjunction with confidence estimates for the vari- ous engines and a trigram language model in order to score and rank a collection of sen- tence hypotheses that are synthetic combinations of words from the various original en- gines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments conducted using three Chinese-to-English online translation systems demon- strate that our multi-engine combination system provides an improvement of about 6% over the best original system, and is about equal in translation quality to an ""oracle"" capable of selecting the best of the original systems on a sentence-by-sentence basis. A second oracle experiment shows that our new approach produces synthetic combination sentence hy- potheses that are far superior to the hypotheses currently selected by the system, but our current scoring is not yet capable of adequately identifying the best hypothesis."	2005	108	8.3076923077
Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora	ABSTRACT  Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation es- timates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploit- ing multiple translations of the same source phrase. Central to our approach is triangula- tion, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for train- ing, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated mod- els over a standard phrase-based system.	2007	103	9.3636363636
Bilingual framenet dictionaries for machine translation	ABSTRACT  This paper describes issues surrounding the planning and design of GermanFrameNet (GFN), a counterpart to the English-based FrameNet project. The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation.	2002	119	7.4375
Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora.	The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionall...	2004	115	8.2142857143
Applying Morphology Generation Models to Machine Translation	ABSTRACT  We improve the quality of statistical machine translation (SMT) by applying models that predict word forms from their stems using extensive morphological and syntactic infor- mation from both the source and target lan- guages. Our inflection generation models are trained independently of the SMT system. We investigate different ways of combining the in- flection prediction component with the SMT system by training the base MT system on fully inflected forms or on word stems. We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judge- ments.	2008	94	9.4
A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation	Abstract Inspired by previous preprocessing ap- proaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of re- ordered inputs, which are then fed to stan- dard phrase-based decoder to produce the optimal translation. Experiments show that, for the NIST MT-05 task of Chinese-to- English translation, the proposal leads to BLEU improvement of 1.56%.	2007	103	9.3636363636
Word-dependent transition models in HMM based word alignment for statistical machine translation	A word alignment modeler uses probabilistic learning techniques to train “word-dependent transition models” for use in constructing phrase level Hidden Markov Model (HMM) based word alignment models. As defined herein, “word-dependent transition models” provide a probabilistic model wherein for each source word in training data, a self-transition probability is modeled in combination with a probability of jumping from that particular word to a different word, thereby providing a full transition model for each word in a source phrase. HMM based word alignment models are then used for various word alignment and machine translation tasks. In additional embodiments sparse data problems (i.e., rarely used words) are addressed by using probabilistic learning techniques to estimate word-dependent transition model parameters by maximum a posteriori (MAP) training.	2011	75	10.7142857143
Machine translation system	A machine translation system includes a database for storing various information, database management section for performing database management, a bilingual correspondence data record subsystem for performing recording/learning processing of translation examples, a translation subsystem for performing translation processing, and dictionary management utilities for performing dictionary management and database transmission/reception processing. The bilingual correspondence data recording section records English and Japanese bilingual correspondences by using English and Japanese sentences stored in the same file or different files. The recorded bilingual correspondences are linked in units of parts by a bilingual correspondence learning section. In performing translation, an English-to-Japanese translation section and the like generate a translation of an original sentence as a translation target by using parts and the like which have undergone learning/recording processing. The dictionary management utilities perform database transmission/reception processing to/from another machine translation system.	1997	99	4.7142857143
Robust machine translation evaluation with entailment features	ABSTRACT  Existing evaluation metrics for machine translation lack crucial robustness: their correlations with hu- man quality judgments vary considerably across lan- guages and genres. We believe that the main reason is their inability to properly capturemeaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and ar- gument structure overlap. We compare this metric against a combination metric of four state-of-the- art scores (BLEU, NIST, TER, and METEOR) in two different settings. The combination metric out- performs the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements.	2009	66	7.3333333333
Platform-independent automated machine translation system	A client computer program designed to run in operating systems with various primary character sets is provided. The client computer program further allows the user to choose a preferred destination for translated documents. For example, the translation can be appended to the original document, written into a new document, sent to the clipboard, or turned out to any other supported destination according to the specifications of the user. In addition, the client computer program provides a translation job queue window that lists documents awaiting analysis and transmission for translation. Through the translation job queue window, the user can dynamically add to and delete from the translation job queue, as well as suspend or reorder jobs, and open completed files directly from the list. Furthermore, the user can choose to display either the requested translation alone or the requested translation together with the untranslated email message in the same window. Finally, the client computer program interacts with an email program so that the user can initiate translation directly from the email program's user interface.	2003	88	5.8666666667
Syntactic preprocessing for statistical machine translation	ABSTRACT  We describe an approach to automatic source-language syntactic preprocessing in the context of Arabic-English phrase-based machine translation. Source-language labeled dependencies, that are word aligned with target language words in a parallel corpus, are used to automatically extract syntactic reordering rules in the same spirit of Xia and McCord (2004) and Zhang et al. (2007). The extracted rules are used to reorder the source-language side of the training and test data. Our results show that when using monotonic decoding and translations for unigram source-language phrases only, source-language reordering gives very significant gains over no reordering (25% relative increase in BLEU score). With decoder distortion turned on and with access to all phrase translations, the differences in BLEU scores are diminished. However, an analysis of sentence-level BLEU scores shows reordering outperforms no-reordering in over 40% of the sentences. These results suggest that the approach holds big promise but much more work on Arabic parsing may be needed.	2007	75	6.8181818182
Phrase-Based Backoff Models for Machine Translation of Highly Inflected Languages	We propose a backoff model for phrase- based machine translation that translates unseen word forms in foreign-language text by hierarchical morphological ab- stractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and Finnish- English translation and shows improve- ments over state-of-the-art phrase-based models.	2006	79	6.5833333333
Using multiple edit distances to automatically rank machine translation output	ABSTRACT  This paper addresses the challenging problem of automatically evaluating output from machine translation (MT) systems in order to support the developers of these systems. Conventional approaches to the problem include methods that automatically assign a rank such as A, B, C, or D to MT output according to a single edit distance between this output and a correct translation example. The single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. This inhibits improving accuracy of rank assignment. To overcome this obstacle, this paper proposes an automatic ranking method that, by using multiple edit distances, encodes machine-translated sentences with a rank assigned by humans into multi-dimensional vectors from which a classifier of ranks is learned in the form of a decision tree (DT). The proposed method assigns a rank to MT output through the learned DT. The proposed method is evaluated using transcribed texts of real conversations in the travel arrangement domain. Experimental results show that the proposed method is more accurate than the single-edit-distance-based ranking methods, in both closed and open tests. Moreover, the proposed method could estimate MT quality within 3% error in some cases.	2001	92	5.4117647059
Towards the use of word stems and suffixes for statistical machine translation	ABSTRACT  In this paper we present methods for improving the quality of translation from an inflected language into English by making use of part-of-speech tags and word stems and suffixes in the source language. Results for translations from Spanish and Catalan into English are presented on the LC-STAR trilingual corpus which consists of spontaneously spoken dialogues in the domain of travelling and appointment scheduling. Results for translation from Serbian into English are presented on the Assimil language course, the bilingual corpus from unrestricted domain. We achieve up to 5% relative reduction of error rates for Spanish and Catalan and about 8% for Serbian.	2007	75	6.8181818182
Cross-Lingual Ontology Mapping --- An Investigation of the Impact of Machine Translation	Ontologies are at the heart of knowledge management and make use of information that is not only written in English but also in many other natural languages. In order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. This paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in cross-lingual ontology mapping scenarios. In particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in English and Chinese are presented. Based on findings derived from these studies, limitations of this generic approach are discussed. It is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in cross-lingual ontology mapping. Finally, to address the identified challenges, a semantic-oriented cross-lingual ontology mapping (SOCOM) framework is proposed and discussed.	2009	66	7.3333333333
Translation system, translation communication system, machine translation method, and medium embodying program	A translation means determination section of a translation system determines whether translation processing of language data input from an input section is to be performed in an internal translation section or on an external translation server connected by a communication line through a communication control section. In the latter case, it is also determined what processing the translation server is requested to perform. The translation means determination section flexibly switches between internal processing and processing on the translation server by using information such as a language pair to which translation is applied, the communication line state, and a comparison of abilities between the translation system and the translation server. A translation result of the translation section is output from an output section. This configuration allows easy implementation of function extension viewed from a user while minimizing communication cost and overhead.	2006	79	6.5833333333
462 machine translation systems for Europe	We built 462 machine translation systems for all language pairs of the Acquis Communautaire corpus. We report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multi-pivot, multisource) that are possible due to the available systems. 1	2009	65	7.2222222222
Statistical machine translation. Final report	"Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include ""spam"") Enter a search phrase. You can also specify a CiteULike article id(123456),. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678)."	1999	93	4.8947368421
Using lexicalized tags for machine translation	"Lexicalized Tree Adjoining Grammar (LTAG) is an attractive formalism for linguistic description mainly because of its extended domain of locality and its factoring recursion out from the domain of local dependencies (Joshi, 1984, Kroch and Joshi, 1985, Abeill茅, 1988). LTAG's extended domain of locality enables one to localize syntactic dependencies (such as filler-gap), as well as semantic dependencies (such as predicate-arguments). The aim of this paper is to show that these properties combined with the lexicalized property of LTAG are especially attractive for machine translation. The transfer between two languages, such as French and English, can be done by putting directly into correspondence large elementary universe without going through some interlingual representation and without major changes to the source and target grammars. The underlying formalism from the transfer is ""synchronous Tree Adjoining Grammars"" (Sheiber and Schabes [1990]). Transfer rules are stated as correspondences between nodes of trees of large domain of locality which are associated with words. We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach. We rely on the French and English LTAG grammars (Abeill茅 [1988], Abeill茅 [1990(b)], Abeill茅 et al. [1990], Abeill茅 and Schabes [1989, 1990]) that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu."	1990	102	3.6428571429
Target-Text Mediated Interactive Machine Translation	The use of Machine Translation as a tool for professional or other highly skilled translators is for the most part currently limited to postediting arrangements in which the translator invokes MT when desired and then manually cleans up the results. A theoretically promising but hitherto largely unsuccessful alternative to postediting for this application is interactive machine translation (IMT), in which the translator and MT system work in tandem. We argue that past failures to make IMT viable as a tool for skilled translators have been the result of an infelicitous mode of interaction rather than any inherent flaw in the idea. As a solution, we propose a new style of IMT in which the target text under construction serves as the medium of communication between an MT system and its user. We describe the design, implementation, and performance of an automatic word completion system for translators which is intended to demonstrate the feasibility of the proposed approach, albeit in a very rudimentary form.	1997	96	4.5714285714
A matching technique in Example-Based Machine Translation	This paper addresses an important problem in Example-Based Machine Translation (EBMT), namely how to measure similarity between a sentence fragment and a set of stored examples. A new method is proposed that measures similarity according to both surface structure and content. A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient. Results on a large number of test cases from the CELEX database are presented.Cranias, L; Papageorgiou, H; Piperidis, S	1994	100	4.1666666667
An example-based method for transfer-driven machine translation	Abstract This paper presents a method called Transfer-Driven Machine Translation (TDMT), which utilizes an example-based framework for various process and combines multi-level knowledge. An example-based framework can achieve quick processing and consistently describe knowledge. It is useful for spoken-language translation, which needs robust and efficient translation. TDMT strengthens the example-based framework by integrating it with other frameworks. The feasibility of TDMT and the advantages of the example-based framework have been confirmed with a prototype system, which translates spoken dialog sentences from Japanese to English.	1992	102	3.9230769231
The use of lexical semantics in interlingual machine translation	This paper describes the lexical-semantic basis for UNITRAN, an implemented scheme for translating Spanish, English, and German bidirectionally. Two claims made here are that the current representation handles many distinctions (or divergences ) across languages without recourse to language-specific rules and that the lexical-semantic framework provides the basis for a systematic mapping between the interlingua and the syntactic structure. The representation adopted is an extended version of lexical conceptual structure  which is suitable to the task of translating between divergent structures for two reasons: (1) it provides an abstraction  of language-independent properties from structural idiosyncrasies; and (2) it is compositional  in nature. The lexical-semantic approach addresses the divergence problem by using a linguistically grounded mapping that has access to parameter settings in the lexicon. We will examine a number of relevant issues including the problem of defining primitives, the issue of interlinguality, the cross-linguistic coverage of the system, and the mapping between the syntactic structure and the interlingua. A detailed example of lexical-semantic composition will be presented.	1992	102	3.9230769231
MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation	ABSTRACT  We propose an automatic machine translation (MT) evaluation metric that calculates a sim- ilarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then find a maxi- mum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to in- corporate different information in our com- parison, such as n-grams, dependency rela- tions, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judge- ments than all 11 automatic MT evaluation metrics that were evaluated during the work- shop.	2008	70	7
Segmentation for English-to-Arabic statistical machine translation	Abstract In this paper, we report on a set of ini- tial results for English-to-Arabic Statistical Machine Translation (SMT). We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombina- tion techniques. We also report on the use of Factored Translation Models for English- to-Arabic translation.	2008	70	7
Novel reordering approaches in phrase-based statistical machine translation	ABSTRACT  This paper presents novel approaches to reordering in phrase-based statistical ma-chine translation. We perform consistent reordering of source sentences in train-ing and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine transla-tion approach, for which we develop an ef-ficient and flexible reordering framework that allows to easily introduce different re-ordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as in-put. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly im-proves translation quality.	2005	80	6.1538461538
Machine translation system	Methods and apparatus for machine translation are disclosed. In one embodiment of the invention, information is stored in a memory which is contained in a computer, or some other device. The stored information includes a set of eigens for a number of languages, a cross-language eigen dictionary, a pattern dictionary, and a cross-language pattern dictionary. The first step of the translation is the conversion of a sentence in a first language to an instantiated pattern form. A corresponding pattern is then found in the cross-language pattern dictionary. Eigens are then found using the cross-language eigen dictionary, and a translation in a second language is assembled.	2003	85	5.6666666667
The LRC machine translation system: Linguistics Research Center	Source-language text is analyzed according to phrasestructure grammar rules augmented with procedures incorporating syntactic and semantic restrictions, surface and deep case analysis, and transformations; transformed to target-language structures and lexical items representing an equivalent utterance; then synthesized into target-language text.	1982	106	2.9444444444
Example-based machine translation using DP-matching between word sequences	Summary: We propose a new approach to the example-based machine translation paradigm. First, the proposed approach retrieves the most similar example by carrying out DP-matching of the input sentence and source sentences in an example database while measuring the semantic distances of the words. Second, the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary. We demonstrate its high coverage and accuracy through a computational experiment for a limited domain.	2001	88	5.1764705882
Machine translation system incorporating syntactic dependency treelets into a statistical framework	In one embodiment of the present invention, a decoder receives a dependency tree as a source language input and accesses a set of statistical models that produce outputs combined in a log linear framework. The decoder also accesses a table of treelet translation pairs and returns a target dependency tree based on the source dependency tree, based on access to the table of treelet translation pairs, and based on the application of the statistical models.	2010	59	7.375
Shallow Parsing for Portuguese--Spanish Machine Translation	To produce fast, reasonably intelligible and easily correctable translations between related languages, it suffices to use a machine translation strategy which uses shallow parsing techniques to refine what would usually be called word-for-word machine translation. This paper describes the application of shallow parsing techniques (morphological analysis, lexical disambiguation, and flat, local parsing) in a Portuguese–Spanish, Spanish–Portuguese machine translation system which is currently being developed by our group and is publicly and freely available at http://copacabana.dlsi.ua.es.	2003	84	5.6
Can Crowds Build Parallel Corpora For Machine Translation Systems?	ABSTRACT  Corpus based approaches to machine transla-tion (MT) rely on the availability of parallel corpora. In this paper we explore the effec-tiveness of Mechanical Turk for creating par-allel corpora. We explore the task of sen-tence translation, both into and out of a lan-guage. We also perform preliminary experi-ments for the task of phrase translation, where ambiguous phrases are provided to the turker for translation in isolation and in the context of the sentence it originated from.	2010	59	7.375
Multiple-parts-of-speech disambiguating method and apparatus for machine translation system	A machine translation system comprises input means for inputting a sentence written in a natural language, processor for parsing the input sentence, a word dictionary memory referred to by the processor, and a memory for storing multiple-parts-of-speech disambiguating rules in the form of a table. The parts of speech of words capable of functioning as multiple parts of speech should be in the inputted sentence are determined in consideration of an array of the parts of speech by applying the multiple-parts-of-speech disambiguating rules. Additionally, rate of appearance of each part of speech which the word of the input sentence can function as is previously calculated, and the part of speech which can not be determined by consulting the disambiguating rule table is determined in dependence on whether the rate of appearance exceeds a predetermined threshold value.	1987	102	3.2903225806
Cross-Lingual Ontology Mapping – An Investigation of the Impact of Machine Translation	Ontologies are at the heart of knowledge management and make use of information that is not only written in English but also in many other natural languages. In order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. This paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in cross-lingual ontology mapping scenarios. In particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in English and Chinese are presented. Based on findings derived from these studies, limitations of this generic approach are discussed. It is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in cross-lingual ontology mapping. Finally, to address the identified challenges, a semantic-oriented cross-lingual ontology mapping (SOCOM) framework is proposed and discussed.	2009	64	7.1111111111
Dialectal to standard Arabic paraphrasing to improve Arabic-English statistical machine translation	This paper is interested in improving the quality of Arabic-English statistical machine translation (SMT) on highly dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics and transfer rules. The generated paraphrase lattices are input to a state-of-the-art phrase-based SMT system resulting in improved BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative).	2011	54	7.7142857143
Incremental hypothesis alignment for building confusion networks with application to machine translation system combination	ABSTRACT  Confusion network decoding has been the most successful approach in combining out- puts from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations. Due to the vary- ing word order between outputs from differ- ent MT systems, the hypothesis alignment presents the biggest challenge in confusion network decoding. This paper describes an incremental alignment method to build confu- sion networks based on the translation edit rate (TER) algorithm. This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBN's submission to the WMT08 shared translation task.	2008	68	6.8
Syntax-based language models for machine translation	react-text: 435 We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language...  /react-text  react-text: 436   /react-text [Show full abstract]	2003	83	5.5333333333
Displaying and correcting method for machine translation system	In a system wherein a first text in a first natural language is translated into a second text in a second natural language; a text displaying and correcting system comprising a first memory area for storing parts of the first text divided in predetermined units, with identifications assigned to the respective parts, and a second memory area for storing predetermined units of the second text corresponding to the aforementioned units of the first text, with the same identifications assigned thereto, so that the first and second texts are simultaneously displayed on a screen of a display unit, and that the text is revised in each unit with identification assigned.	1986	99	3.09375
Online learning for interactive statistical machine translation	ABSTRACT  State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework. In this framework, the knowledge of a human translator is combined with a MT system. The vast majority of the existing work on IMT makes use of the well-known batch learning paradigm. In the batch learning paradigm, the training of the IMT system and the interactive translation process are carried out in separate stages. This paradigm is not able to take advantage of the new knowledge produced by the user of the IMT system. In this paper, we present an application of the online learning paradigm to the IMT framework. In the online learning paradigm, the training and prediction stages are no longer separated. This feature is particularly useful in IMT since it allows the user feedback to be taken into account. The online learning techniques proposed here incrementally update the statistical models involved in the translation process. Empirical results show the great potential of online learning in the IMT framework.	2010	57	7.125
E-services translation utilizing machine translation and translation memory	A system and method for translating data from a source language to a target language is provided wherein machine generated target translation of a source sentence is compared to a database of human generated target sentences. If a matching human generated target sentence is found, the human generated target sentence may be used instead of the machine generated sentence, since the human generated target sentence is more likely to be a well-formed sentence than the machine generated sentence. The system and method does not rely on a translation memory containing pairs of sentences in both source and target languages, and minimizes the reliance on a human translator to correct a translation generated by machine translation.	2011	52	7.4285714286
Translating with examples: a new approach to machine translation	ATR Interpreting Telephony Research Laboratories Sanpeidani, Inuidani Seika-cho, Soraku-gun Kyoto 619-02, Japan e-mail: sumita%atr-la.atr.co.jp@uunet.uu.net    This paper proposes Example-Based Machine Translation (EBMT). EBMT retrieves similar examples	1990	100	3.5714285714
Error Detection for Statistical Machine Translation Using Linguistic Features	ABSTRACT  Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from N-best lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.	2010	58	7.25
MAXSIM: A maximum similarity metric for machine translation evaluation	We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop. 漏 2008 Association for Computational Linguistics.	2008	65	6.5
Stochastic Finite-State Models for Spoken Language Machine Translation	The problem of machine translation can be viewed as consisting of twosubproblems (a) lexical selection and (b) lexical reordering. In thispaper, we propose stochastic finite-state models for these two subproblems. Stochastic finite-state models are efficiently learnablefrom data, effective for decoding and are associated with a calculusfor composing models which allows for tight integration of constraintsfrom various levels of language processing. We present a method forlearning stochastic finite-state models for lexical selection andlexical reordering that are trained automatically from pairs of sourceand target utterances. We use this method to develop models forEnglish–Japanese and English–SPANISH translation and present the performance of these models for translation on speech and text. We also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.	2002	85	5.3125
Language model adaptation for statistical machine translation based on information retrieval	Abstract Language modeling is an important part for both speech recognition and machine translation systems. Adaptation has been successfully applied to language models for speech recognition. In this paper we present experiments concerning language model adaptation for statistical machine translation. We develop a method to adapt language models using information retrieval methods. The adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.	2004	79	5.6428571429
Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation	This comprehensive handbook, written by leading experts in the field, details the groundbreaking research conducted under the breakthrough GALE program - The Global Autonomous Language Exploitation within the Defense Advanced Research Projects Agency (DARPA), while placing it in the context of previous research in the fields of natural language and signal processing, artificial intelligence and machine translation. The most fundamental contrast between GALE and its predecessor programs was its holistic integration of previously separate or sequential processes. In earlier language research proOlive, Joseph P; Christianson, Caitlin; McCary, John	2011	53	7.5714285714
Machine translation using vector space representations	An embodiment of the present invention provides a method for automatically translating text. First, a conceptual representation space is generated based on source-language documents and target-language documents, wherein respective terms from the source-language and target-language documents have a representation in the conceptual representation space. Second, a new source-language document is represented in the conceptual representation space, wherein a subset of terms in the new source-language document is represented in the conceptual representation space, such that each term in the subset has a representation in the conceptual representation space. Then, a term in the new source-language document is automatically translated into a corresponding target-language term based on a similarity between the representation of the term and the representation of the corresponding target-language term.	2010	56	7
Statistical Machine Translation Gains Respect	As business, finance, education, and the Internet become increasingly international and multilingual, Google and other organizations are investing more time, money, and talent into researching the effectiveness of machine-translation technologies.	2005	75	5.7692307692
Generation-Heavy Hybrid Machine Translation	Abstract This paper describes GenerationHeavy Hybrid Machine Translation (GHMT), a novel approach for trans- lating between structurally-divergent language pairs with asymmetrical resources. The approach depends on the existence of rich target language resources such as word lexical semantics, categorial variations and subcategorization frames. These resources are used to overgenerate multiple lexico-structural variations from a target-glossed syntactic dependency representation of the source language sentence. This symbolic overgeneration, which accounts for a wide range of possible variations, is constrained by a statistical targetlanguage model. The exploitation of target language resources (symbolic and statistical) to handle a problem usually reserved for Transfer and Interlingual MT is useful for translation from source languages with scarce linguistic resources. A preliminary evaluation on the application of this approach to Spanish-English MT is conducted with promising results.	2002	81	5.0625
A finite-state approach to machine translation	The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection; (b) lexical reordering. We propose stochastic finite-state models for these two subproblems. Stochastic finite-state models are efficiently able to learn from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. We present a method for learning stochastic finite-state models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese translation and present the performance of these models for translation of speech and text. We also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.	2001	84	4.9411764706
The LRC machine translation system: an overview of the linguistic component of METAL	"The LRC machine translation system: an overview of the linguistic component of METALdoi:10.3115/990100.990105Winfield S. BennettThe University of Texas at AustinAcademia PrahaConference on Computational LinguisticsBennett, W. and Slocum, J., ""The LRC Machine Translation System"", Computational Linguistics, Vol. 11, No. 2-3, pp. 111-121, 1985...."	1982	100	2.7777777778
Cache-based Document-level Statistical Machine Translation	Abstract Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation.	2011	51	7.2857142857
Purest ever example-based machine translation: Detailed presentation and assessment	We have designed, implemented and assessed an EBMT system that can be dubbed the 'purest ever built': it strictly does not make any use of variables, templates or patterns, does not have any explicit transfer component, and does not require any preprocessing or training of the aligned examples. It only uses a specific operation, proportional analogy, that implicitly neutralises divergences between languages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. Exactly the same genuine implementation of such a core engine was evaluated on different tasks and language pairs. To begin with, we compared our system on two tasks of a previous MT evaluation campaign to rank it among other current state-of-the-art systems. Then, we illustrated the 'universality' of our system by participating in a recent MT evaluation campaign, with exactly the same core engine, for a wide variety of language pairs. Finally, we studied the in uence of extra data like dictionaries and paraphrases on the system performance.	2007	68	6.1818181818
Cohesive Phrase-Based Decoding for Statistical Machine Translation	ABSTRACT  Phrase-based decoding produces state-of-the- art translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree's structure. In this way, we target the phrasal decoder's weakness in order model- ing, without affecting its strengths. To fur- ther increase flexibility, we incorporate cohe- sion as a decoder feature, creating a soft con- straint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations.	2008	64	6.4
System and method for machine learning a confidence metric for machine translation	A machine translation system is trained to generate confidence scores indicative of a quality of a translation result. A source string is translated with a machine translator to generate a target string. Features indicative of translation operations performed are extracted from the machine translator. A trusted entity-assigned translation score is obtained and is indicative of a trusted entity-assigned translation quality of the translated string. A relationship between a subset of the extracted features and the trusted entity-assigned translation score is identified.	2009	60	6.6666666667
Neural Machine Translation of Rare Words with Subword Units	Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.	2016	169	84.5
Neural Machine Translation by Jointly Learning to Align and Translate	Abstract:  Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.	2014	1890	472.5
On the Properties of Neural Machine Translation: Encoder-Decoder Approaches	Abstract:  Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.	2014	467	116.75
Findings of the 2014 Workshop on Statistical Machine Translation	This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries	2014	391	97.75
Effective Approaches to Attention-based Neural Machine Translation	Abstract:  An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.	2015	301	100.3333333333
Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation	"Abstract:  Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."	2016	223	111.5
On Using Very Large Target Vocabulary for Neural Machine Translation	Abstract:  Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English->German translation and almost as high performance as state-of-the-art English->French translation system.	2014	188	47
Fast and Robust Neural Network Joint Models for Statistical Machine Translation	Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380, Baltimore, Maryland, USA, June 23-25 2014. cO2014 Association for Computational LinguisticsFast and Robust Neural Network Joint Models for Statistical Machine TranslationJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA {jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com AbstractRecent work	2014	234	58.5
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation	Abstract:  In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.	2014	1174	293.5
Modeling Coverage for Neural Machine Translation	Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attention-based NMT ignores past alignment information, which often leads to over-translation and under-translation. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which guides NMT to consider more about the untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over traditional attention-based NMT.	2016	53	26.5
Minimum Risk Training for Neural Machine Translation	Abstract:  We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.	2015	70	23.3333333333
Findings of the 2016 Conference on Machine Translation	This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT qual- ity), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 in- stitutions (plus 36 anonymized online sys- tems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments).	2016	60	30
Addressing the Rare Word Problem in Neural Machine Translation	Abstract:  Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.	2014	156	39
Findings of the 2015 Workshop on Statistical Machine Translation	This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic post- editing task had a total of 4 teams, submitting 7 entries.	2015	133	44.3333333333
Optimizing Chinese Word Segmentation for Machine Translation Performance	Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segment.	2016	56	28
Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism	We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.	2016	60	30
Improved statistical machine translation for resource-poor languages using related resource-rich languages	"We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X_1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X_1-Y and a larger bi-text for X_2-Y for some resource-rich language X_2 that is closely related to X_1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X_1 and X_2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian- >English using Malay and for Spanish -> English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary ""real training data by a factor of 2--5."	2014	82	20.5
A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation	Abstract:  The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.	2016	86	43
A Character-level Decoder without Explicit Segmentation for Neural Machine Translation	Abstract The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.	2016	55	27.5
Is machine translation ready yet?	"The default option of the Google Translator Toolkit (GTT), released in June 2009, is to ""pre-fill with machine translation"" all segments for which a 'no match' has been returned by the memories, while the window clearly advises that most users should not modify this"". To confirm whether this approach indeed benefits translators and translation quality, we designed and performed tests whereby trainee translators used the GTT to translate passages from English into Chinese either entirely from the source text, or after seeding of empty segments by the Google Translate engine as recommended. The translations were timed, and their quality assessed by independent experienced markers following Australian NAATI test criteria. Our results show that, while time differences were not significant, the machine translation seeded passages were more favourably assessed by the markers in thirty three of fifty six cases. This indicates that, at least for certain tasks and language combinations — and against the received wisdom of translation professionals and translator trainers — translating by proofreading machine translation may be advantageous. "	2016	43	21.5
Neural Machine Translation in Linear Time	Abstract:  We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.	2016	41	20.5
Character-based Neural Machine Translation	We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.	2015	70	23.3333333333
Character-based Neural Machine Translation	Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affixaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.	2016	47	23.5
Edinburgh Neural Machine Translation Systems for WMT 16	Abstract:  We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English2Czech, English2German, English2Romanian and English2Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.	2016	40	20
On Using Monolingual Corpora in Neural Machine Translation	Abstract:  Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.	2015	58	19.3333333333
Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation	Abstract:  We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\rightarrow$French and surpasses state-of-the-art results for English$\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\rightarrow$English and German$\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.	2016	49	24.5
Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models	Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The two-fold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new state-of-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.	2016	51	25.5
Character-based Neural Machine Translation	Abstract Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English WMT task.	2016	33	16.5
Improving Neural Machine Translation Models with Monolingual Data	Abstract:  Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task EnglishGerman (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.	2015	53	17.6666666667
Minimum Risk Training for Neural Machine Translation	Abstract We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.	2016	39	19.5
Computerized statistical machine translation with phrasal decoder	A computerized system for performing statistical machine translation with a phrasal decoder is provided. The system may include a phrasal decoder trained prior to run-time on a monolingual parallel corpus, the monolingual parallel corpus including a machine translation output of source language documents of a bilingual parallel corpus and a corresponding target human translation output of the source language documents, to thereby learn mappings between the machine translation output and the target human translation output. The system may further include a statistical machine translation engine configured to receive a translation input and to produce a raw machine translation output, at run-time. The phrasal decoder may be configured to process the raw machine translation output, and to produce a corrected translation output based on the learned mappings for display on a display associated with the system.	2015	44	14.6666666667
Neural versus Phrase-Based Machine Translation Quality: a Case Study	Abstract:  Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.	2016	37	18.5
Neural versus Phrase-Based Machine Translation Quality: a Case Study	react-text: 270 Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a...  /react-text  react-text: 271   /react-text [Show full abstract]	2016	34	17
Knowledge-Based Question Answering as Machine Translation	Abstract A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.	2014	49	12.25
Bilingually-constrained Phrase Embeddings for Machine Translation	We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn phrase embeddings (compact vector representations for phrases), which can distinguish the phrases in different semantic meanings. The BRAE is trained with the objective to minimize the semantic distance of translation equivalents and maximize the semantic distance of nontranslation pairs. The learned model can embed any phrase semantically in two languages and can transform semantic space in one language to the other. We evaluate the BRAE on two end-to-end SMT tasks (phrase table pruning and translation hypotheses reranking) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is spectacularly successful in these two tasks.	2014	58	14.5
Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis	Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts. In recent years, due to the growth in the quantity and fast spreading of user-generated contents online and the impact such information has on events, people and companies worldwide, this task has been approached in an important body of research in the field. Despite different methods having been proposed for distinct types of text, the research community has concentrated less on developing methods for languages other than English. In the above-mentioned context, the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less/no resources are available for this task when compared to English, stressing upon the impact of translation quality on the sentiment classification performance. Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can, in combination to appropriate machine learning algorithms and carefully chosen features, be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.	2014	48	12
Dual Learning for Machine Translation	Abstract:  While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.	2016	31	15.5
Lium Smt Machine Translation System For Wmt 2010	This paper describes the development of French--English and English--French machine translation systems for the 2010 WMT shared task evaluation. These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data. We first collected bilingual data by performing automatic translations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.	2017	29	29
Nematus: a Toolkit for Neural Machine Translation	Abstract:  We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.	2017	29	29
OpenNMT: Open-Source Toolkit for Neural Machine Translation	Abstract:  We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.	2017	31	31
Linguistic Input Features Improve Neural Machine Translation	Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to EnglishGerman, and English->Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An open-source implementation of our neural MT system is available, as are sample files and configurations.	2016	31	15.5
Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models	react-text: 428 In this paper, we describe FBK's neural machine translation (NMT) systems submitted at the International Workshop on Spoken Language Translation (IWSLT) 2016. The systems are based on the state-of-the-art NMT architecture that is equipped with a bi-directional encoder and an attention mechanism in the decoder. They leverage linguistic information such as lemmas and part-of-speech tags of the...  /react-text  react-text: 429   /react-text [Show full abstract]	2016	29	14.5
A Recursive Recurrent Neural Network for Statistical Machine Translation	In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof- the-art baseline by about 1.5 points in BLEU.	2014	46	11.5
Integrating an Unsupervised Transliteration Model into Statistical Machine Translation	ABSTRACT  We investigate three methods for integrat-ing an unsupervised transliteration model into an end-to-end SMT system. We in-duce a transliteration model from parallel data and use it to translate OOV words. Our approach is fully unsupervised and language independent. In the methods to integrate transliterations, we observed improvements from 0.23-0.75 (62 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora.	2014	43	10.75
Integrating an Unsupervised Transliteration Model into Statistical Machine Translation	Reduced mechanical stress to bone in bedridden patients and astronauts leads to bone loss and increase in fracture risk which is one of the major medical and health issues in modern aging society and space medicine. However, no molecule involved in the mechanisms underlying this phenomenon has been identified to date. Osteopontin (OPN) is one of the major noncollagenous proteins in bone matrix, but its function in mediating physical-force effects on bone in vivo has not been known. To investigate the possible requirement for OPN in the transduction of mechanical signaling in bone metabolism in vivo, we examined the effect of unloading on the bones of OPN 61/ 61 mice using a tail suspension model. In contrast to the tail suspension–induced bone loss in wild-type mice, OPN 61/ 61 mice did not lose bone. Elevation of urinary deoxypyridinoline levels due to unloading was observed in wild-type but not in OPN 61/ 61 mice. Analysis of the mechanisms of OPN deficiency–dependent reduction in bone on the cellular basis resulted in two unexpected findings. First, osteoclasts, which were increased by unloading in wild-type mice, were not increased by tail suspension in OPN 61/ 61 mice. Second, measures of osteoblastic bone formation, which were decreased in wild-type mice by unloading, were not altered in OPN 61/ 61 mice. These observations indicate that the presence of OPN is a prerequisite for the activation of osteoclastic bone resorption and for the reduction in osteoblastic bone formation in unloaded mice. Thus, OPN is a molecule required for the bone loss induced by mechanical stress that regulates the functions of osteoblasts and osteoclasts.	2014	43	10.75
Nematus: a Toolkit for Neural Machine Translation	react-text: 452 Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than...  /react-text  react-text: 453   /react-text [Show full abstract]	2017	26	26
Tree-to-Sequence Attentional Neural Machine Translation	Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.	2016	25	12.5
Coverage Embedding Models for Neural Machine Translation	In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.	2016	27	13.5
Linguistic Input Features Improve Neural Machine Translation	Abstract Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to EnglishGerman neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3.	2016	26	13
Fully Character-Level Neural Machine Translation without Explicit Segmentation	Abstract:  Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.	2017	28	28
Incorporating Discrete Translation Lexicons into Neural Machine Translation	Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.	2016	24	12
Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation	Abstract:  Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.	2016	24	12
Encoding Source Language with Convolutional Neural Network for Machine Translation	Abstract:  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average	2015	32	10.6666666667
Machine translation instant messaging applications	An instant messaging translation plug-in interacts with an instant messaging program to intercept incoming messages and forward these messages to a language translation service. The plug-in then displays a translation received from the service along with the original message. This provides translation which can be used by instant messaging users to communicate across language barriers, and without local translation or knowledge of the internal workings of the translation services used. Additionally, the translation plug-in also provides for manual translation of messages, which allows communication with users who use a different language but do not use the translation plug-in. Messages are modified before translation in order to correct spelling, to prevent particular words or phrases from being translated, and to change instant messaging language into standard language form. The techniques can be performed on various messaging services, including instant messaging on computers or mobile devices, as well as SMS.	2014	34	8.5
Montreal Neural Machine Translation Systems for WMT’15	react-text: 555 We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine...  /react-text  react-text: 556   /react-text [Show full abstract]	2015	30	10
Syntactically Guided Neural Machine Translation	We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores.	2016	23	11.5
Vocabulary Manipulation for Neural Machine Translation	In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).	2016	20	10
Semi-Supervised Learning for Neural Machine Translation	While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.	2016	20	10
A Coverage Embedding Model for Neural Machine Translation	In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.	2016	23	11.5
Transfer Learning for Low-Resource Neural Machine Translation	Abstract:  The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.	2016	21	10.5
Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions	In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.	2016	20	10
Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation	Abstract Using machine translation output as a starting point for human translation has become an increasingly common application of MT.We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step. Individually, these techniques can substantially improve MT quality, even over strong baselines. Moreover, we see super-Additive improvements when all three techniques are used in tandem.	2014	30	7.5
Customizable machine translation service	Embodiments of the present invention provide a system and method for providing a translation service. The method comprises providing a translation interface accessible via a network. The translation interface receives specialized data associated with a domain from a member. A text string written in a source language is received from the member via the translation interface. A domain-based translation engine is selected. The domain-based translation engine may be associated with a source language, a target language, and a domain. The text string is translated into the target language using, at least in part, the selected domain-based translation engine. The translated text string is transmitted to the member via the Internet. In some embodiments, a translation memory is generated based on the specialized data.	2014	25	6.25
Optimizing parameters for machine translation	Methods, systems, and apparatus, including computer program products, for language translation are disclosed. In one implementation, a method is provided. The method includes accessing a hypothesis space, where the hypothesis space represents a plurality of candidate translations; performing decoding on the hypothesis space to obtain a translation hypothesis that minimizes an expected error in classification calculated relative to an evidence space; and providing the obtained translation hypothesis for use by a user as a suggested translation in a target translation.	2014	27	6.75
Supervised Attentions for Neural Machine Translation	"In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the ""true"" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system."	2016	17	8.5
Edinburgh's Phrase-based Machine Translation Systems for WMT-14	ABSTRACT  This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.	2014	26	6.5
A Shared Task on Multimodal Machine Translation and Crosslingual Image Description	This paper introduces and summarises the findings of a new shared task at the in- tersection of Natural Language Process- ing and Computer Vision: the generation of image descriptions in a target language, given an image and/or one or more de- scriptions in a different (source) language. This challenge was organised along with the Conference on Machine Translation (WMT16), and called for system submis- sions for two task variants: (i) a transla- tion task, in which a source language im- age description needs to be translated to a target language, (optionally) with addi- tional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image. In this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams.	2016	19	9.5
On the Elements of an Accurate Tree-to-String Machine Translation System	While tree-to-string (T2S) translation theoretically holds promise for efficient, accurate translation, in previous reports T2S systems have often proven inferior to other machine translation (MT) methods such as phrase-based or hierarchical phrase-based MT. In this paper, we attempt to clarify the reason for this performance gap by investigating a number of peripheral elements that affect the accuracy of T2S systems, including parsing, alignment, and search. Based on detailed experiments on the English-Japanese and Japanese- English pairs, we show how a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof- the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems.	2014	24	6
Zero-Resource Translation with Multi-Lingual Neural Machine Translation	In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.	2016	17	8.5
Mutual Information and Diverse Decoding Improve Neural Machine Translation	Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, both mechanisms offer a consistent performance boost on both standard LSTM and attention-based neural MT architectures. The result is the best published performance for a single (non-ensemble) neural MT system, as well as the potential application of our diverse decoding algorithm to other NLP re-ranking tasks.	2016	17	8.5
Agreement-based joint training for bidirectional attention-based neural machine translation	The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.	2016	19	9.5
SYSTRAN's Pure Neural Machine Translation Systems	"Abstract:  Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work. Our ultimate goal is to share our expertise to build competitive production systems for ""generic"" translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems."	2016	14	7
Semi-Supervised Learning for Neural Machine Translation	Abstract While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.	2016	14	7
Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions	react-text: 471 This paper presents a machine translation tool – based on Moses – developed for the International Maritime Organization (IMO) for the automatic translation of documents from Spanish, French, Rus-sian and Arabic to/from English. The main challenge lies in the insufficient size of in-house corpora (especially for Russian and Arabic). The United Nations (UN) granted IMO the right to use UN...  /react-text  react-text: 472   /react-text [Show full abstract]	2016	15	7.5
Hybrid Simplification using Deep Semantics and Machine Translation	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	2015	21	7
A Sense-Based Translation Model for Statistical Machine Translation	ABSTRACT  The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation.	2014	23	5.75
Zero-Resource Translation with Multi-Lingual Neural Machine Translation	"react-text: 452 Researchers in Computer Science discipline put a great effort for developing machine intelligence techniques, inspired from human intelligence and to cultivate elegant mathematical tools to design …""  /react-text  react-text: 453   /react-text [more]"	2016	14	7
Collecting and Using Comparable Corpora for Statistical Machine Translation	Lack of sufficient parallel data for many languages and domains is currently one of the major obstacles to further advancement of automated translation. The ACCURAT project is addressing this issue by researching methods how to improve machine translation systems by using comparable corpora. In this paper we present tools and techniques developed in the ACCURAT project that allow additional data needed for statistical machine translation to be extracted from comparable corpora. We present methods and tools for acquisition of comparable corpora from the Web and other sources, for evaluation of the comparability of collected corpora, for multi-level alignment of comparable corpora and for extraction of lexical and terminological data for machine translation. Finally, we present initial evaluation results on the utility of collected corpora in domain-adapted machine translation and real-life applications.	2015	20	6.6666666667
Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015	This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words.	2015	21	7
UM-Corpus: a large English-Chinese parallel corpus for statistical machine translation	Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems,especially for Statistical Machine Translation (SMT). However, most existing parallel corpora to Chinese are subject to in-house use,while others are domain specific and limited in size. To a certain degree, this limits the SMT research. This paper describes the acquisitionof a large scale and high quality parallel corpora for English and Chinese. The corpora constructed in this paper contain about 15 millionEnglish-Chinese (E-C) parallel sentences, and more than 2 million training data and 5,000 testing sentences are made publicly available.Different from previous work, the corpus is designed to embrace eight different domains. Some of them are further categorized intodifferent topics. The corpus will be released to the research community, which is available at the NLP 2 CT 1 website.	2014	22	5.5
Combination of Stochastic Understanding and Machine Translation Systems for Language Portability of Dialogue Systems	In this paper, several approaches for language portability of dialogue systems are investigated with a focus on the spoken language understanding (SLU) component. We show that the use of statistical machine translation (SMT) can greatly reduce the time and cost of porting an existing system from a source to a target language. Using automatically translated training data we study phrase-based machine translation as an alternative to conditional random fields for conceptual decoding to compensate for the loss of a precise concept-word alignment. Also two ways to increase SLU robustness to translation errors (smeared training data and translation post editing) are shown to improve performance when test data are translated then decoded in the source language. Overall the combination of all these approaches allows to reduce even further the concept error rate. Experiments were carried out on the French MEDIA dialogue corpus with a subset manually translated into Italian.	2014	22	5.5
Improved Neural Machine Translation with SMT Features	Neural machine translation (NMT) conducts end-to-end translation with a source language encoder and a target language decoder, making promising translation performance. However, as a newly emerged approach, the method has some limitation...	2016	14	7
Improving evaluation of machine translation quality estimation	ABSTRACT  Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during comparison of quality estimation prediction score distributions and gold label distributions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components ofWMT-13 andWMT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks.	2015	18	6
Attention-based Multimodal Neural Machine Translation	react-text: 467 In this work, synthesis of facial animation is done by modelling the mapping between facial motion and speech using the shared Gaussian process latent variable model. Both data are processed separately and subsequently coupled together to yield a shared latent space. This method allows coarticulation to be modelled by having a dynamical model on the latent space. Synthesis of novel animation...  /react-text  react-text: 468   /react-text [Show full abstract]	2016	13	6.5
Towards zero unknown word in neural machine translation	Neural Machine translation has shown promising results in recent years. In order to control the com- putational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vo- cabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In sub- stitution step, the rare words in a testing sen- tence are replaced with similar in-vocabulary words based on a similarity model learnt from monolin- gual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. Exper- iments on Chinese-to-English translation demon- strate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.	2016	14	7
Edinburgh’s Phrase-based Machine Translation Systems for WMT-14.	ABSTRACT This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.	2014	20	5
Hybrid Simplification using Deep Semantics and Machine Translation	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	2014	21	5.25
Guided Alignment Training for Topic-Aware Neural Machine Translation	In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.	2016	13	6.5
Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation	Abstract:  The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.	2014	21	5.25
Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)	Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.	2015	19	6.3333333333
Arabic machine translation: a survey	Although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. Thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. We discuss some of the linguistic characteristics of the Arabic language. Features of Arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. This paper summarizes the major techniques used in machine translation from Arabic into English, and discusses their strengths and weaknesses.	2014	19	4.75
Neural Machine Translation with Reconstruction	Abstract:  Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems.	2016	12	6
Fast Domain Adaptation for Neural Machine Translation	Neural Machine Translation (NMT) is a new approach for automatic translation of text from one human language into another. The basic concept in NMT is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is gaining popularity in the research community because it outperformed traditional SMT approaches in several translation tasks at WMT and other evaluation tasks/benchmarks at least for some language pairs. However, many of the enhancements in SMT over the years have not been incorporated into the NMT framework. In this paper, we focus on one such enhancement namely domain adaptation. We propose an approach for adapting a NMT system to a new domain. The main idea behind domain adaptation is that the availability of large out-of-domain training data and a small in-domain training data. We report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs. With our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly. In addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the NMT model on the combined data which usually takes several days/weeks depending on the volume of the data.	2016	13	6.5
Massive Exploration of Neural Machine Translation Architectures	Abstract:  Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.	2017	13	13
Vocabulary Selection Strategies for Neural Machine Translation	Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.	2016	12	6
Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2014	In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech - English, Vietnamese - English, French - English and German - English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality.	2015	17	5.6666666667
Proceedings of the 15th Annual Conference of the European Association for Machine Translation	The blood coagulation mechanism consists of a series of concatenated chemical reactions, governed by the coagulation factors present in the blood plasma, after the activation of the clot mechanism. The last reaction corresponds to the fibrinogen conversion into fibrin, followed by the fibrin polymerisation and production of a stable fibrin network. During the clotting process, there is a sol-gel transformation of the medium. The subject of the present paper is the measurement of the ultrasonic attenuation coefficient for human blood plasma during the coagulation process, in the frequency range of 8 to 22 MHz. The clot was obtained after the procedure to measure the prothrombin time (6512 s): mixing 150 μL of reconstituted lyophilised normal plasma with 300 μL of reconstituted lyophilised thromboplastin immersed in a water bath with the temperature controlled at 36.5°C. The attenuation coefficient for pure plasma remained constant within the measurement period of 10 s and at frequencies of 8, 9, 10, 15, 20, 21 and 22 MHz. On the other hand, there is a detectable time-decay of the attenuation coefficient for samples of plasma going through the coagulation process and at frequencies of 8, 9, 10 and 15 MHz. The time-decay becomes less and less detectable as the frequency increases and it becomes completely undetectable at 20, 21 and 22 MHz. (E-mail: jcm@peb.ufrj.br )	2014	19	4.75
Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing	Abstract This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model can be used to control for higher faithfulness with regard to the to-be-corrected machine translation input. Our submission outperforms the uncorrected baseline on the unseen test set by -3.2% TER and +5.5% BLEU.	2016	12	6
Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing	Abstract:  This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2\% TER and +5.5\% BLEU and outperforms any other system submitted to the shared-task by a large margin.	2016	12	6
Selection and use of nonstatistical translation components in a statistical machine translation framework	A system with a nonstatistical translation component integrated with a statistical translation component engine. The same corpus may be used for training the statistical engine and also for determining when to use the statistical engine and when to use the translation component. This training may use probabilistic techniques. Both the statistical engine and the translation components may be capable of translating the same information, however the system determines which component to use based on the training. Retraining can be carried out to add additional components, or when after additional translator training.	2014	19	4.75
Arabic machine translation: a survey	Although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. Thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. We discuss some of the linguistic characteristics of the Arabic language. Features of Arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. This paper summarizes the major techniques used in machine translation from Arabic into English, and discusses their strengths and weaknesses.	2014	18	4.5
Factored Neural Machine Translation	We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. The final translation is built using some \textit{a priori} linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.	2016	11	5.5
Variational Neural Machine Translation	Abstract:  Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.	2016	11	5.5
Jane: Open Source Machine Translation System Combination	Different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they yield. System combination is a method for combining the output.	2014	16	4
Using Discourse Structure Improves Machine Translation Evaluation	ABSTRACT  We present experiments in using dis-course structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment-and at the system-level. Rather than proposing a single new metric, we show that discourse information is com-plementary to the state-of-the-art evalu-ation metrics, and thus should be taken into account in the development of future richer evaluation metrics.	2014	16	4
Maximal Lattice Overlap in Example-Based Machine Translation	Example-Based Machine Translation (EBMT) retrieves pre-translated phrases from a sentence-aligned bilingual training corpus to translate new input sentences. EBMT uses long pre-translated phrases effectively but is subject to disfluencies at phrasal translation boundaries. We address this problem by introducing a novel method that exploits overlapping phrasal translations and the increased confidence in translation accuracy they imply. We specify an efficient algorithm for producing translations using overlap. Finally, our empirical analysis indicates that this approach produces higher quality translations than the standard method of EBMT in a peak-to-peak comparison.	2017	11	11
Exploiting Source-side Monolingual Data in Neural Machine Translation	Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the source- side monolingual data in NMT. The first ap- proach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach ap- plies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods ob- tain significant improvements over the strong attention-based NMT.	2016	11	5.5
Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation	Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to con- struct efficient large LM is an important topic in SMT. However, most of the ex- isting LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can im- prove both the perplexity score for LM e- valuation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.	2014	16	4
Don’t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation	react-text: 434 We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a...  /react-text  react-text: 435   /react-text [Show full abstract]	2014	17	4.25
Neural Machine Translation with Supervised Attention	The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.	2016	10	5
Coverage-based Neural Machine Translation	However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.	2016	10	5
Edinburgh's Statistical Machine Translation Systems for WMT16	This paper describes the University of Ed- inburgh's phrase-based and syntax-based submissions to the shared translation tasks of the ACL 2016 First Conference on Ma- chine Translation (WMT16). We sub- mitted five phrase-based and five syntax- based systems for the news task, plus one phrase-based system for the biomedical task.	2016	10	5
Pragmatic Neural Language Modelling in Machine Translation	This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.	2015	14	4.6666666667
Incorporating pronoun function into statistical machine translation	Pronouns are used frequently in language, and perform a range of functions. Some pronouns are used to express coreference, and others are not. Languages and genres differ in how and when they use pronouns and this poses a problem for Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Novák, 2011; Guillou, 2012; Weiner, 2014; Hardmeier, 2014). Attention to date has focussed on coreferential (anaphoric) pronouns with NP antecedents, which when translated from English into a language with grammatical gender, must agree with the translation of the head of the antecedent. Despite growing attention to this problem, little progress has been made, and little attention has been given to other pronouns. The central claim of this thesis is that pronouns performing different functions in text should be handled differently by SMT systems and when evaluating pronoun translation. This motivates the introduction of a new framework to categorise pronouns according to their function: Anaphoric/cataphoric reference, event reference, extra-textual reference, pleonastic, addressee reference, speaker reference, generic reference, or other function. Labelling pronouns according to their function also helps to resolve instances of functional ambiguity arising from the same pronoun in the source language having multiple functions, each with different translation requirements in the target language. The categorisation framework is used in corpus annotation, corpus analysis, SMT system development and evaluation. I have directed the annotation and conducted analyses of a parallel corpus of English-German texts called ParCor (Guillou et al., 2014), in which pronouns are manually annotated according to their function. This provides a first step toward understanding the problems that SMT systems face when translating pronouns. In the thesis, I show how analysis of manual translation can prove useful in identifying and understanding systematic differences in pronoun use between two languages and can help inform the design of SMT systems. In particular, the analysis revealed that the German translations in ParCor contain more anaphoric and pleonastic pronouns than their English originals, reflecting differences in pronoun use. This raises a particular problem for the evaluation of pronoun translation. Automatic evaluation methods that rely on reference translations to assess pronoun translation, will not be able to provide an adequate evaluation when the reference translation departs from the original source-language text. I also show how analysis of the output of state-of-the-art SMT systems can reveal how well current systems perform in translating different types of pronouns and indicate where future efforts would be best directed. The analysis revealed that biases in the training data, for example arising from the use of “it” and “es” as both anaphoric and pleonastic pronouns in both English and German, is a problem that SMT systems must overcome. SMT systems also need to disambiguate the function of those pronouns with ambiguous surface forms so that each pronoun may be translated in an appropriate way. To demonstrate the value of this work, I have developed an automated post-editing system in which automated tools are used to construct ParCor-style annotations over the source-language pronouns. The annotations are then used to resolve functional ambiguity for the pronoun “it” with separate rules applied to the output of a baseline SMT system for anaphoric vs. non-anaphoric instances. The system was submitted to the DiscoMT 2015 shared task on pronoun translation for English-French. As with all other participating systems, the automatic post-editing system failed to beat a simple phrase-based baseline. A detailed analysis, including an oracle experiment in which manual annotation replaces the automated tools, was conducted to discover the causes of poor system performance. The analysis revealed that the design of the rules and their strict application to the SMT output are the biggest factors in the failure of the system. The lack of automatic evaluation metrics for pronoun translation is a limiting factor in SMT system development. To alleviate this problem, Christian Hardmeier and I have developed a testing regimen called PROTEST comprising (1) a hand-selected set of pronoun tokens categorised according to the different problems that SMT systems face and (2) an automated evaluation script. Pronoun translations can then be automatically compared against a reference translation, with mismatches referred for manual evaluation. The automatic evaluation was applied to the output of systems submitted to the DiscoMT 2015 shared task on pronoun translation. This again highlighted the weakness of the post-editing system, which performs poorly due to its focus on producing gendered pronoun translations, and its inability to distinguish between pleonastic and event reference pronouns.	2016	10	5
Proceedings of the Second Workshop on Statistical Machine Translation	ABSTRACT  The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical ma-chine translation systems to a special do-main (here: news commentary), when most of the training data is from a dif-ferent domain (here: European Parliament speeches). This paper also gives a descrip-tion of the submission of the University of Edinburgh to the shared task.	2015	13	4.3333333333
Assessing the Discourse Factors that Influence the Quality of Machine Translation	Abstract We present a study of aspects of discourse structure - specifically discourse devices used to organize information in a sentence- that significantly impact the quality of machine translation. Our analysis is based on manual evaluations of translations of news from Chinese and Arabic to English. We find that there is a particularly strong mismatch in the notion of what constitutes a sentence in Chinese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to employ multiple explicit discourse connectives (because, but, etc.), as well as the presence of ambiguous discourse connectives in the English translation. Furthermore, the mismatches between discourse expressions across languages significantly impact translation quality.	2014	14	3.5
An empirical analysis of data selection techniques in statistical machine translation.	[EN] Domain adaptation has recently gained interest in statistical machine translation. One of the adaptation techniques is based in the selection data. Data selection aims to select the best subset of the bilingual sentences from an available pool of sentences, with which to train a SMT system. In this paper, we study how affect the bilingual corpora used for the data selection methods in the translation quality	2015	14	4.6666666667
Neural Machine Translation with External Phrase Memory	In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.	2016	9	4.5
Syntax-aware Neural Machine Translation Using CCG	Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.	2017	9	9
Embedding Word Similarity with Neural Machine Translation	Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.	2014	14	3.5
A Convolutional Encoder Model for Neural Machine Translation	Abstract:  The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.	2016	9	4.5
Standardizing Tweets with Character-Level Machine Translation	Summary: This paper presents the results of the standardization procedure of Slovene tweets that are full of colloquial, dialectal and foreign-language elements. With the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient out-of-vocabulary (OOV) tokens and used it to train a character-level statistical machine translation system (CSMT). Best results were obtained by combining the manually constructed lexicon and CSMT as fallback with an overall improvement of 9.9\% increase on all tokens and 31.3\% on OOV tokens. Manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. Finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1\% per-token accuracy with the former and 83.6\% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	2014	14	3.5
Standardizing Tweets with Character-Level Machine Translation	This paper presents the results of the standardization procedure of Slovene tweets that are full of colloquial, dialectal and foreign-language elements. With the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient out-of-vocabulary (OOV) tokens and used it to train a character-level statistical machine translation system (CSMT). Best results were obtained by combining the manually constructed lexicon and CSMT as fallback with an overall improvement of 9.9% increase on all tokens and 31.3% on OOV tokens. Manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. Finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1% per-token accuracy with the former and 83.6% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	2014	14	3.5
Adaptation of machine translation for multilingual information retrieval in the medical domain	We investigate machine translation (MT) of user search queries in the context of cross-lingual information retrieval (IR) in the medical domain. The main focus is on techniques to adapt MT to increase translation quality; however, we also explore MT adaptation to improve effectiveness of cross-lingual IR. Our MT system is Moses, a state-of-the-art phrase-based statistical machine translation system. The IR system is based on the BM25 retrieval model implemented in the Lucene search engine. The MT techniques employed in this work include in-domain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. The IR methods include morphological normalization and using multiple translation variants for query expansion. The experiments are performed and thoroughly evaluated on three language pairs: Czech–English, German–English, and French–English. MT quality is evaluated on data sets created within the Khresmoi project and IR effectiveness is tested on the CLEF eHealth 2013 data sets. The search query translation results achieved in our experiments are outstanding – our systems outperform not only our strong baselines, but also Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech–English, from 23.03 to 40.82 for German–English, and from 32.67 to 40.82 for French–English. This is a 55% improvement on average. In terms of the IR performance on this particular test collection, a significant improvement over the baseline is achieved only for French–English. For Czech–English and German–English, the increased MT quality does not lead to better IR results. Most of the MT techniques employed in our experiments improve MT of medical search queries. Especially the intelligent training data selection proves to be very successful for domain adaptation of MT. Certain improvements are also obtained from German compound splitting on the source language side. Translation quality, however, does not appear to correlate with the IR performance – better translation does not necessarily yield better retrieval. We discuss in detail the contribution of the individual techniques and state-of-the-art features and provide future research directions.	2014	14	3.5
Learning to Parse and Translate Improves Neural Machine Translation	Abstract:  There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.	2017	9	9
An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation	"In this paper, we propose a novel domain adaptation method named ""mixed fine tuning"" for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."	2017	9	9
Machine Translation: Mining Text for Social Theory	More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online trans.	2016	8	4
Neural Machine Translation with Recurrent Attention Modeling	Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.	2016	8	4
Neural Machine Translation Advised by Statistical Machine Translation	Abstract:  Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu et al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.	2016	9	4.5
Towards String-to-Tree Neural Machine Translation	We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.	2017	9	9
Doubly-Attentive Decoder for Multi-modal Neural Machine Translation	We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.	2017	8	8
NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems	In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017.	2017	8	8
Context-dependent word representation for neural machine translation ☆	We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En–Fr and En–De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.	2017	8	8
Systems and methods for tuning parameters in statistical machine translation	A method for tuning translation parameters in statistical machine translation based on ranking of the translation parameters is disclosed. According to one embodiment, the method includes sampling pairs of candidate translation units from a set of candidate translation units corresponding to a source unit, each candidate translation unit corresponding to numeric values assigned to one or more features, receiving an initial weighting value for each feature, comparing the pairs of candidate translation units to produce binary results, and using the binary results to adjust the initial weighting values to produce modified weighting values.	2014	13	3.25
Bilingual Continuous-Space Language Model Growing for Statistical Machine Translation	Larger n-gram language models (LMs) perform better in statistical machine translation (SMT). However, the existing approaches have two main drawbacks for constructing larger LMs: 1) it is not convenient to obtain larger corpora in the same domain as the bilingual parallel corpora in SMT; 2) most of the previous studies focus on monolingual information from the target corpora only, and redundant n-grams have not been fully utilized in SMT. Nowadays, continuous-space language model (CSLM), especially neural network language model (NNLM), has been shown great improvement in the estimation accuracies of the probabilities for predicting the target words. However, most of these CSLM and NNLM approaches still consider monolingual information only or require additional corpus. In this paper, we propose a novel neural network based bilingual LM growing method. Compared to the existing approaches, the proposed method enables us to use bilingual parallel corpus for LM growing in SMT. The results show that our new method outperforms the existing approaches on both SMT performance and computational efficiency significantly.	2015	11	3.6666666667
The Operation Sequence Model – Combining N-Gram-Based and Phrase-Based Statistical Machine Translation	In this article, we present a novel machine translation model, the Operation Sequence Model (OSM), which combines the benefits of phrase-based and N-gram-based statistical machine translation (SMT) and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption, and (iv) avoids the spurious phrasal segmentation problem. As in phrase-based SMT, themodel (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically, and (iii) decodes with large translation units during search. The unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM.	2015	12	4
Machine translation for query expansion	Methods, systems and apparatus, including computer program products, for expanding search queries. One method includes receiving a search query, selecting a synonym of a term in the search query based on a context of occurrence of the term in the received search query, the synonym having been derived from statistical machine translation of the term, and expanding the received search query with the synonym and using the expanded search query to search a collection of documents. Alternatively, another method includes receiving a request to search a corpus of documents, the request specifying a search query, using statistical machine translation to translate the specified search query into an expanded search query, the specified search query and the expanded search query being in the same natural language, and in response to the request, using the expanded search query to search a collection of documents.	2015	11	3.6666666667
EU-BRIDGE MT: Combined Machine Translation	Abstract This paper describes one of the col-laborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two Euro-pean language pairs, German→English and English→German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combina-tion. RWTH Aachen University, the Uni-versity of Edinburgh, and Karlsruhe In-stitute of Technology developed several individual systems which serve as sys-tem combination input. We devoted spe-cial attention to building syntax-based sys-tems and combining them with the phrase-based ones. The joint setups yield em-pirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT news-test2013 test set compared to the best sin-gle systems.	2014	12	3
Submodularity for Data Selection in Machine Translation	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT). By explicitly formulating data selection as a submodular program, we ob- tain fast scalable selection algorithms with mathematical performance guarantees, re- sulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. We present a new class of submodular functions designed specifically for SMT and evaluate them on two differ- ent translation tasks. Our results show that our best submodular method significantly outperforms several baseline methods, including the widely-used cross-entropy based data selection method. In addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing.	2014	12	3
Statistical machine translation enhancements through linguistic levels: A survey	Machine translation can be considered a highly interdisciplinary and multidisciplinary field because it is approached from the point of view of human trans.	2014	12	3
The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT~2015	This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year鈥檚 evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions.Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features.	2015	11	3.6666666667
The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015	This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared transla- tion task of the EMNLP 2015 Tenth Work- shop on Statistical Machine Translation (WMT 2015). We set up phrase-based sta- tistical machine translation systems for all ten language pairs of this year's evaluation campaign, which are English paired with Czech, Finnish, French, German, and Rus- sian in both translation directions.	2015	11	3.6666666667
Phrasal: A Toolkit for New Directions in Statistical Machine Translation	We present a new version of Phrasal, an open-source toolkit for statistical phrase-based machine translation. This revision includes features that support emerging re-search trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive ma-chine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1	2014	12	3
Using Joint Models for Domain Adaptation in Statistical Machine Translation	ABSTRACT  Joint models have recently shown to improve the state-of-the-art in machine translation (MT). We apply EM-based mixture modeling and data selection techniques using two joint models, namely the Operation Sequence Model or OSM — an ngram-based translation and reordering model, and the Neural Network Joint Model or NNJM — a continuous space translation model, to carry out domain adaptation for MT. The diversity of the two models, OSM with inherit reordering information and NNJM with continuous space modeling makes them interesting to be explored for this task. Our contribution in this paper is fusing the existing known techniques (linear interpolation, cross-entropy) with the state-of-the-art MT models (OSM, NNJM). On a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points.	2015	11	3.6666666667
An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation	Scalable discriminative training methods are now broadly available for estimating phrase-based, feature-rich translation mod-els. However, the sparse feature sets typi-cally appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities: they often overfit, do not gen-eralize, or require complex and slow fea-ture extractors. This paper introduces ex-tended features, which are more specific than dense features yet more general than lexicalized sparse features. Large-scale ex-periments show that extended features yield robust BLEU gains for both Arabic-English (+1.05) and Chinese-English (+0.67) rel-ative to a strong feature-rich baseline. We also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and language-independent tools for implementing the features. 1	2014	12	3
Towards String-To-Tree Neural Machine Translation	Goldberg. Towards string-to-tree neural machine translation. In ACL, 2017.Roee Aharoni and Yoav Goldberg. 2017. Towards String-to-Tree Neural Machine Translation. ArXiv e-prints.Aharoni and Goldberg 2017] Aharoni, R., and Goldberg, ...	2017	7	7
Machine Translation Post-editing and Effort. Empirical Studies on the Post-editing Process	Abstract This dissertation investigates the practice of machine translation post-editing and the various aspects of effort involved in post-editing work. Through analyses of edits made by post-editors, the work described here examines three main questions: 1) what types of machine translation errors or source text features cause particular effort in post-editing, 2) what types of errors can or cannot be corrected without the help of the source text, and 3) how different indicators of effort vary between different post-editors. The dissertation consists of six previously published articles, and an introductory summary. Five of the articles report original research, and involve analyses of post-editing data to examine questions related to post-editing effort as well as differences between post-editors. The sixth article is a survey presenting an overview of the research literature. The research reported is based on multiple datasets consisting of machine translations and their post-edited versions, as well as process and evaluation data related to post-editing effort. The dissertation presents a mixed methods study combining qualitative and quantitative approaches, as well as theoretical and analytical tools from the fields of language technology and translation studies. Data on edits performed by post-editors, post-editing time, keylogging data, and subjective evaluations of effort are combined with error analyses of the machine translations in question, and compared for various post-editors. The results of this dissertation provide evidence that, in addition to the number of edits performed, post-editing effort is affected by the type of edits as well as source text features. Secondly, the results show that while certain language errors can be corrected even without access to the source text, certain other types that more severely affect the meaning cannot. Thirdly, the results show that post-editors' speed and the amount of editing they perform differ, and that various profiles can be identified in terms of how the edits are planned and carried out by the post-editors. The results of this work may have both theoretical and practical implications for the measurement and estimation of post-editing effort.	2016	7	3.5
Latest trends in hybrid machine translation and its applications ☆	This survey provides a detailed overview of the modification of the standard rule-based architecture to include statistical knowledge, the introduction of rules in corpus-based approaches, and the hybridization of approaches within this last single category. The principal aim here is to cover the leading research and progress in this field of MT and in several related applications.	2015	10	3.3333333333
An evaluation of machine translation for multilingual sentence-level sentiment analysis	Sentiment analysis has become a key tool for several social media applications, including analysis of user's opinions about products and services, support to politics during campaigns and even for market trending. There are multiple existing sentiment analysis methods that explore different techniques, usually relying on lexical resources or learning approaches. Despite the large interest on this theme and amount of research efforts in the field, almost all existing methods are designed to work with only English content. Most existing strategies in specific languages consist of adapting existing lexical resources, without presenting proper validations and basic baseline comparisons. In this paper, we take a different step into this field. We focus on evaluating existing efforts proposed to do language specific sentiment analysis. To do it, we evaluated twenty-one methods for sentence-level sentiment analysis proposed for English, comparing them with two language-specific methods. Based on nine language-specific datasets, we provide an extensive quantitative analysis of existing multi-language approaches. Our main result suggests that simply translating the input text on a specific language to English and then using one of the existing English methods can be better than the existing language specific efforts evaluated. We also rank those implementations comparing their prediction performance and identifying the methods that acquired the best results using machine translation across different languages. As a final contribution to the research community, we release our codes and datasets. We hope our effort can help sentiment analysis to become English independent.	2016	7	3.5
An evaluation of machine translation for multilingual sentence-level sentiment analysis	Sentiment analysis has become a key tool for several social media applications, including analysis of user's opinions about products and services, support to politics during campaigns and even for market trending. There are multiple existing sentiment analysis methods that explore different techniques, usually relying on lexical resources or learning approaches. Despite the large interest on this theme and amount of research efforts in the field, almost all existing methods are designed to work with only English content. Most existing strategies in specific languages consist of adapting existing lexical resources, without presenting proper validations and basic baseline comparisons. In this paper, we take a different step into this field. We focus on evaluating existing efforts proposed to do language specific sentiment analysis. To do it, we evaluated twenty-one methods for sentence-level sentiment analysis proposed for English, comparing them with two language-specific methods. Based on nine language-specific datasets, we provide an extensive quantitative analysis of existing multi-language approaches. Our main result suggests that simply translating the input text on a specific language to English and then using one of the existing English methods can be better than the existing language specific efforts evaluated. We also rank those implementations comparing their prediction performance and identifying the methods that acquired the best results using machine translation across different languages. As a final contribution to the research community, we release our codes and datasets. We hope our effort can help sentiment analysis to become English independent.	2016	7	3.5
Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets	Abstract:  This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.	2017	7	7
Topic-Based Coherence Modeling for Statistical Machine Translation	Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	2015	10	3.3333333333
Learning to Parse and Translate Improves Neural Machine Translation	react-text: 343 In typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of...  /react-text  react-text: 344   /react-text [Show full abstract]	2017	7	7
A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena	Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials.To orient the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling.We then question why some approaches are more successful than others in different language pairs. We argue that besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.	2016	7	3.5
Can machine translation systems be evaluated by the crowd alone	Crowd-sourced assessments of machine translation quality allow evaluations to be carried out cheaply and on a large scale. It is essential, however, that the crowd's work be filtered to avoid contamination of results through the inclusion of false assessments. One method is to filter via agreement with experts, but even amongst experts agreement levels may not be high. In this paper, we present a new methodology for crowd-sourcing human assessments of translation quality, which allows individual workers to develop their own individual assessment strategy. Agreement with experts is no longer required, and a worker is deemed reliable if they are consistent relative to their own previous work. Individual translations are assessed in isolation from all others in the form of direct estimates of translation quality. This allows more meaningful statistics to be computed for systems and enables significance to be determined on smaller sets of assessments. We demonstrate the methodology's feasibility in large-scale human evaluation through replication of the human evaluation component of Workshop on Statistical Machine Translation shared translation task for two language pairs, Spanish-to-English and English-to-Spanish. Results for measurement based solely on crowd-sourced assessments show system rankings in line with those of the original evaluation. Comparison of results produced by the relative preference approach and the direct estimate method described here demonstrate that the direct estimate method has a substantially increased ability to identify significant differences between translation systems.	2015	9	3
Temporal Attention Model for Neural Machine Translation	Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles.	2016	7	3.5
Response-based Learning for Grounded Machine Translation	Abstract We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database. Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.	2014	11	2.75
Topic-based coherence modeling for statistical machine translation	Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	2015	9	3
OxLM: A Neural Language Modelling Framework for Machine Translation	This paper presents an open source implementation1 of a neural language model for machine translation. Neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. The language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. Neural language models are notoriously slow to train and test. Our framework is designed with scalability in mind and provides two optional techniques for reducing the computational cost: the so-called class decomposition trick and a training algorithm based on noise contrastive estimation. Our models may be extended to incorporate direct n-gram features to learn weights for every n-gram in the training data. Our framework comes with wrappers for the cdec and Moses translation toolkits, allowing our language models to be incorporated as normalized features in their decoders (inside the beam search).	2014	11	2.75
Sentence Level Dialect Identification for Machine Translation System Selection	react-text: 526 While Modern Standard Arabic (MSA) has many resources, Arabic Dialects, the primarily spoken local varieties of Arabic, are quite impoverished in this regard. In this article, we present ADAM (Analyzer for Dialectal Arabic Morphology). ADAM is a poor man’s solution to quickly develop morphological analyzers for dialectal Arabic. ADAM has roughly half the out-of-vocabulary rate of a...  /react-text  react-text: 527   /react-text [Show full abstract]	2014	11	2.75
Involving language professionals in the evaluation of machine translation	Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. TARA X U project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. In a consortium of research and industry partners, TARAXU project integrates human translators into the development process for rating and post-editing of machine translation outputs collecting feedback for possible improvements.	2014	11	2.75
Modelling and optimizing on syntactic n-grams for statistical machine translation	The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps. Syntactic language models have the potential to fill this modelling gap. We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order. It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models. We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian. We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.	2015	9	3
What’s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation	react-text: 371 As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains....  /react-text  react-text: 372   /react-text [Show full abstract]	2015	9	3
Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation	Abstract The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-To-English and Italian-To-English language pairs, showing significant improvements over a phrasebased Moses baseline. We also compare with state of the art German-To-English pre-reordering rules, showing that our method obtains similar or better results.	2015	9	3
Pushdown automata in statistical machine translation	2014 Association for Computational Linguistics.This article describes the use of pushdown automata (PDA) in the context of statistical machine translation and alignment under a synchronous context-free grammar. We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence. General-purpose PDA algorithms for replacement, composition, shortest path, and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the PDA representation and these algorithms.We contrast the complexity of this decoder with a decoder based on a finite state automata representation, showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context-free grammars and smaller language models. We assess this experimentally on a large-scale Chinese-to-English alignment and translation task. In translation, we propose a two-pass decoding strategy involving a weaker language model in the first-pass to address the results of PDA complexity analysis. We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art performance for large-scale SMT.	2014	10	2.5
Automatic spelling correction for machine translation	Methods, systems, and apparatus, including computer program products, for correcting spelling in text. A text input is received for translation. One or more suspect words in the text input are identified. For each suspect word, one or more candidate words are identified. A score for the text input and scores for each of one or more candidate inputs are determined, where each candidate input is the text input with one or more of the suspect words each replaced by a respective candidate word. If any, a candidate input whose score is highest among the scores for the candidate inputs and is greater than the text input score by at least a threshold is selected. Otherwise, the text input is selected. A translation of a selected candidate input or the selected text input is provided as the translation of the text input.	2014	10	2.5
Evaluation of machine translation systems at CLS Corporate Language Services AG	ABSTRACT  This paper describes the evaluation of Machine Translation (MT) System for use in a large company. To take into account the specific requirements of such an environment, a pragmatic approach for the evaluation was developed. It consists of five steps ranging from a specification of the evaluation process to the integration of the chosen MT system in a given infrastructure. The process includes a specification of MT evaluation criteria relevant to systems which have to be employed for a large customer base. The paper also shows the results of such an evaluation study which was recently carried out at CLS Corporate Language Services AG, where COMPRENDIUM is in the meantime being employed as corporate MT system.	2014	10	2.5
Controlling Politeness in Neural Machine Translation via Side Constraints	Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to...	2016	6	3
Models and Inference for Prefix-Constrained Machine Translation	react-text: 435 Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while com- puting alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi...  /react-text  react-text: 436   /react-text [Show full abstract]	2016	6	3
Statistical machine translation in the translation curriculum: overcoming obstacles and empowering translators	In this paper we argue that the time is ripe for translator educators to engage with Statistical Machine Translation (SMT) in more profound ways than they have done to date. We explain the basic principles of SMT and reflect on the role of humans in SMT workflows. Against a background of diverging opinions on the latter, we argue for a holistic approach to the integration of SMT into translator training programmes, one that empowers rather than marginalises translators. We discuss potential barriers to the use of SMT by translators generally and in translator training in particular, and propose some solutions to problems thus identified. More specifically, cloud-based services are proposed as a means of overcoming some of the technical and ethical challenges posed by more advanced uses of SMT in the classroom. Ultimately the paper aims to pave the way for the design and implementation of a new translator-oriented SMT syllabus at our own University and elsewhere.	2014	10	2.5
Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations	We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.	2017	6	6
Online adaptation to post-edits for phrase-based statistical machine translation	Recent research has shown that accuracy and speed of human translators can benefit from post-editing output of machine translation systems, with larger benefits for higher quality output.	2014	10	2.5
Incorporating Global Visual Features into Attention-Based Neural Machine Translation	We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.	2017	6	6
The New Thot Toolkit for Fully-Automatic and Interactive Statistical Machine Translation	We present the new THOT toolkit for fully- automatic and interactive statistical ma- chine translation (SMT). Initial public ver- sions of THOT date back to 2005 and did only include estimation of phrase-based models. By contrast, the new version of- fers several new features that had not been previously incorporated. The key innova- tions provided by the toolkit are computer- aided translation, including post-editing and interactive SMT, incremental learn- ing and robust generation of alignments at phrase level. In addition to this, the toolkit also provides standard SMT fea- tures such as fully-automatic translation, scalable and parallel algorithms for model training, client-server implementation of the translation functionality, etc. The toolkit can be compiled in Unix-like and Windows platforms and it is released un- der the GNU Lesser General Public Li- cense (LGPL).	2014	10	2.5
Bridging Neural Machine Translation and Bilingual Dictionaries	Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.	2016	6	3
Compression of Neural Machine Translation Models via Pruning	Abstract:  Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.	2016	6	3
Memory-enhanced Decoder for Neural Machine Translation	We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \textsc{MemDec}. At each time during decoding, \textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\cite{RNNsearch} to store the representation of source sentence, the memory in \textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set.	2016	6	3
HUME: Human UCCA-Based Evaluation of Machine Translation	Abstract:  Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.	2016	6	3
Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder	Abstract:  In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.	2016	6	3
Zero-resource machine translation by multimodal encoder–decoder network with multimedia pivot	"Abstract:  We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the ""pivot"", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best."	2016	6	3
An evaluation methodology for English to Sinhala machine translation	This paper presents evaluation methodology for English to Sinhala machine tran slation system. The English to Sinhala machine translation system has been developed by using Multi Agent Approach and powered through the concept of “Varanegeema”. Translation system works through the communication among nine agents namely English Morphological Analyzer Agent, English Parser Agent, English to Sinhala Base Word Translator Agent, Sinhala Morphological Generator Agent, Sinhala Parser agent, Transliteration agent, Intermediate Editor agent, Message Space Agent and Request agent. The evaluation was conducted through three steps. As the first step, evaluation was conducted through the white box testing approach and tested each module in the machine translation system through the developed testing tools. Then, evaluated the system performance and calculated the error rate through the result of the evaluation test bed. Finally, Intelligibility and the Accuracy test will be conducted through the human support. The experimental result shows 89% accuracy of the overall system and 7.2% word error rate and the 5.4% sentence error rate. Details of the evaluation and results are given in the paper	2016	6	3
A survey of word reordering in statistical machine translation: Computational models and language phenomena	2016. A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42.Arianna Bisazza and Marcello Federico. 2016. A Survey of Word Reordering in Statistical Machine ...	2016	6	3
How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs	Abstract:  Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English->German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.	2016	6	3
Learning local word reorderings for hierarchical phrase-based statistical machine translation	Statistical models for reordering source words have been used to enhance hierarchical phrase-based statistical machine translation. There are existing word-reordering models that learn reorderings for.	2016	6	3
Context Gates for Neural Machine Translation	In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.	2017	6	6
Six Challenges for Neural Machine Translation	We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.	2017	6	6
Interactive Attention for Neural Machine Translation	Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.	2016	6	3
Real Time Adaptive Machine Translation for Post-Editing with cdec and TransCenter	As sentences are translated, the models gain valuable context infor- mation, allowing them toadapt to the specific tar  On- line learning of log-linear weights in interactive ma- chine translation Online adaptation strategies for statistical machine translation in post- editing scenarios	2014	9	2.25
Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation	We present a parser for Abstract Meaning Representation (AMR). We treat English-to-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser improves upon state-of-the-art results by 7 Smatch points.	2015	8	2.6666666667
Enhancing Statistical Machine Translation with Bilingual Terminology in a CAT Environment	ABSTRACT  In this paper, we address the problem of extracting and integrating bilingual terminology into a Statistical Machine Translation (SMT) system for a Computer Aided Translation (CAT) tool scenario. We develop a framework that, taking as input a small amount of parallel in-domain data, gathers domain-specific bilingual terms and injects them in an SMT system to enhance the translation productivity. Therefore, we investigate several strategies to extract and align bilingual terminology, and to embed it into the SMT. We compare two embedding methods that can be easily used at run-time without altering the normal activity of an SMT system: XML markup and the cache-based model. We tested our framework on two different domains showing improvements up to 15% BLEU score points.	2014	9	2.25
Bilingual Sentiment Consistency for Statistical Machine Translation	Abstract In this paper, we explore bilingual sentiment knowledge for statistical machine translation (SMT). We propose to explicitly model the consistency of sentiment between the source and target side with a lexicon-based approach. The experiments show that the proposed model significantly improves Chinese-to-English NIST translation over a competitive baseline.	2014	9	2.25
Dependency-based Pre-ordering for Chinese-English Machine Translation	Abstract In statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders. This paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT. We present a set of dependency-based preordering rules which improved the BLEU score by 1.61 on the NIST 2006 evaluation data. We also investigate the accuracy of the rule set by conducting human evaluations.	2014	9	2.25
Optimizing instance selection for statistical machine translation with feature decay algorithms	We introduce FDA5 for efficient parameterization, optimization, and implementation of feature decay algorithms (FDA), a class of instance selection algorithms that use feature decay. FDA increase the diversity of the selected training set by devaluing features (i.e., n-grams) that have already been included. FDA5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with five parameters. We present optimization techniques that allow FDA5 to adapt these functions to in-domain and out-of-domain translation tasks for different language pairs. In a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. In machine translation experiments performed on the 2 million sentence English-German section of the Europarl corpus, we show that a subset of the training set selected by FDA5 can gain up to 3.22 BLEU points compared to a randomly selected subset of the same size, can gain up to 0.41 BLEU points compared to using all of the available training data using only 15% of it, and can reach within 0.5 BLEU points to the full training set result by using only 2.7% of the full training data. FDA5 peaks at around 8M words or 15% of the full training set. In an active learning setting, FDA5 minimizes the human effort by identifying the most informative sentences for translation and FDA gains up to 0.45 BLEU points using 3/5 of the available training data compared to using all of it and 1.12 BLEU points compared to random training set. In translation tasks involving English and Turkish, a morphologically rich language, FDA5 can gain up to 11.52 BLEU points compared to a randomly selected subset of the same size, can achieve the same BLEU score using as little as 4% of the data compared to random instance selection, and can exceed the full dataset result by 0.78 BLEU points. FDA5 is able to reduce the time to build a statistical machine translation system to about half with 1M words using only 3% of the space for the phrase table and 8% of the overall space when compared with a baseline system using all of the training data available yet still obtain only 0.58 BLEU points difference with the baseline system in out-of-domain translation.	2015	8	2.6666666667
