machine translation	a computer natural language translation system inputs source language text and outputs target language text. the target language text is generated from the source language text using stored translation data generated from examples of source and corresponding target language texts. the stored translation data includes a plurality of translation components, each having surface data representative of the order of occurrence of language units in the component; dependency data related to the semantic relationship between language units in the component; and link data linking dependency data of language components of the source language with corresponding dependency data of language components of the target language. the surface data of the source language is used in analyzing the source language text, and the surface date of the target language is used in generating the target language text. the dependency data and link data is used in transforming the analysis of the source text into an analysis for the target language.	-4	71
bleu: a method for automatic evaluation of machine translation	human evaluations of machine translation are extensive but expensive. human evaluations can take months to finish and involve human labor that can not be reused. we propose a method of automatic machine translation evaluation that is quick, inexpensive, and languageindependent, that correlates highly with human evaluation, and that has little marginal cost per run. we present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.	-11	6773
ibm research report bleu: a method for automatic evaluation of machine translation	human evaluations of machine translation are extensive but expensive. human evaluations can take months to finish and involve human labor that can not be reused.	-11	5442
minimum error rate training in statistical machine translation	often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. in this paper, we analyze various training criteria which directly optimize translation quality.	-10	2574
europarl: a parallel corpus for statistical machine translation	we collected a corpus of parallel text in 11 languages from the proceedings of the european parliament, which are published on the web 1 . this corpus has found widespread use in the nlp community. here, we focus on its acquisition and its application as training data for statistical machine translation (smt). we trained smt systems for 110 language pairs, which reveal interesting clues into the challenges ahead.	-8	2203
a parallel corpus for statistical machine translation	in this lecture we continue the study of machine translation in more details. in the previous lectrue, the translation model 3 was introduced. we will explain four other translation models and compare them with model 3. we also describe a computationally cheap learning algorithm for these models, given a set of bilingual texts. at the end, a hmmbased alignment model will be discussed briefly. 8.2 quick review as usual, we consider translation from source language f to the target language e, let’s say french to english. using base rule, we can rewrite p(e|f) as p(e)p(f|e). brown et al. [1] introduced wordbyword alignment between pairs of sentences f and e. one can think of alignment in different ways. we focus on manytoone alignment from f to e and vector a is used to represent this alignment. we assume e has length l and f has length m, so the size of a is also m. aj = i means that fj; the jth word of f is associated with ei; the ith word of e. to learn from pairs of translated sentences we should have some idea about the alignment of french and english words. we can consider all possible alignments and assign appropriate probabilities to them to accordingly compute other parameters of the model.	-8	2120
a statistical approach to machine translation	this chapter contains sections titled introduction, the language model, the translation model, searching, parameter estimation, two pilot experiments, plans, references	-23	2041
object recognition as machine translation: learning a lexicon for a fixed image vocabulary	we describe a model of object recognition as machine translation. in this model, recognition is a process of annotating image regions with words. firstly, images are segmented into regions, which are classified into region types using a variety of features. a mapping between region types and keywords supplied with the images, is then learned, using a method based around em. this process is analogous with learning a lexicon from an aligned bitext. for the implementation we describe, these words are nouns taken from a large vocabulary. on a large test set, the method can predict numerous words with high accuracy. simple methods identify words that cannot be predicted well. we show how to cluster words that individually are difficult to predict into clusters that can be predicted well 鈥 for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. the method is trained on a substantial collection of images. extensive experimental results illustrate the strengths and weaknesses of the approach.	-11	2184
object recognition as machine translation: learning a lexicon for a fixed image vocabulary	we describe a model of object recognition as machine translation. in this model, recognition is a process of annotating image regions with words. firstly, images are segmented into regions, which are classified into region types using a variety of features. a mapping between region types and keywords supplied with the images, is then learned, using a method based around em. this process is analogous with learning a lexicon from an aligned bitext. for the implementation we describe, these words are nouns taken from a large vocabulary. on a large test set, the method can predict numerous words with high accuracy. simple methods identify words that cannot be predicted well. we show how to cluster words that individually are difficult to predict into clusters that can be predicted well 鈥 for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. the method is trained on a substantial collection of images. extensive experimental results illustrate the strengths and weaknesses of the approach.	-11	2080
automatic evaluation of machine translation quality using n-gram co-occurrence statistics	evaluation is recognized as an extremely helpful forcing function in human language technology r&d. unfortunately, evaluation has not been a very powerful tool in machine translation (mt) research because it requires human judgments and is thus expensive and timeconsuming and not easily factored into the mt research agenda. however, at the july 2001 tides pi meeting in philadelphia, ibm described an automatic mt evaluation technique that can provide immediate feedback and guidance in mt research. their idea, which they call an evaluation understudy, compares mt output with expert reference translations in terms of the statistics of short sequences of words (word ngrams). the more of these ngrams that a translation shares with the reference translations, the better the translation is judged to be. the idea is elegant in its simplicity. but far more important, ibm showed a strong correlation between these automatically generated scores and human judgments of translation quality. as a result, darpa commissioned nist to develop an mt evaluation facility based on the ibm work. this utility is now available from nist and serves as the primary evaluation measure for tides mt research.	-11	1410
a hierarchical phrase-based model for statistical machine translation.	we present a statistical phrasebased transla tion model that uses hierarchical phrases— phrases that contain subphrases. the model is formally a synchronous contextfree gram mar but is learned from a bitext without any syntactic information. thus it can be seen as a shift to the formal machinery of syntax based translation systems without any lin guistic commitment. in our experiments us ing bleu as a metric, the hierarchical phrase based model achieves a relative improve ment of 7.5% over pharaoh, a stateoftheart phrasebased system.	-8	1394
automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics	in this paper we describe two new objective automatic evaluation methods for machine translation. the first method is based on longest common subsequence between a candidate translation and a set of reference translations. longest common subsequence takes into account sentence level structure similarity naturally and identifies longest cooccurring insequence ngrams automatically. the second method relaxes strict ngram matching to skipbigram matching. skipbigram is any pair of words in their sentence order. skipbigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set of reference translations. the empirical results show that both methods correlate with human judgments very well in both adequacy and fluency.	-9	1421
discriminative training and maximum entropy models for statistical machine translation	we present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source channel approach as a special case. all knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.	-11	1402
machine translation and translation theory /	ve interpreting. it consists in the mapping of a text onto a cognitive macrostructural scene that specifies the communicative intentions and strategies of the speaker. knowing the latter is of considerable help in human interpreting, and is therefore likely to be useful for machine interpreting as well. unfortunately, since speakers seldom make their intentions explicit, the macrostructural scenes have to be provided manually to the translation system, presumably in a training or preediting phase, and on the specifics of this training phase the author is very brief if i have understood the literature correctly, a connectionist, selfadaptive system will be most suitable for the necessary training, or at least a hybrid model (p. 33). equally brief are the comments on the feasibility of this enterprise, but with the insistence on the importance of macrostructural properties of texts the author strikes a chord that reverberates throughout the entire volume. monika doherty's textual g	-16	1315
statistical significance tests for machine translation evaluation	if two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? to answer this question, we describe bootstrap resampling methods to compute statis.	-9	934
findings of the 2011 workshop on statistical machine translation	this paper presents the results of the wmt11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. we conducted a largescale manual evaluation of 148 machine translation systems and 41 system combination entries. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. this year featured a haitian creole to english task translating sms messages sent to an emergency response service in the aftermath of the haitian earthquake. we also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.	-2	681
findings of the 2012 workshop on statistical machine translation	this paper presents the results of the wmt12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for runtime estimation of machine translation quality. we conducted a largescale manual evaluation of 103 machine translation systems submitted by 34 teams. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. we introduced a new quality estimation task this year, and evaluated submissions from 11 teams. 1	-1	611
findings of the 2009 workshop on statistical machine translation	this paper presents the results of the wmt09 shared tasks, which included a translation task, a system combination task, and an evaluation task. we conducted a largescale manual evaluation of 87 machine translation systems and 22 system combination entries. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. we present a new evaluation technique whereby system output is edited and judged for correctness.	-4	770
automatic evaluation of machine translation quality using n-gram co-occurrence statistics	reacttext 444 evaluation plays a crucial role in development of machine translation systems. in order to judge the quality of an existing mt system i.e. if the translated output is of human translation quality or not, various automatic metrics exist. we here present the implementation results of different metrics when used on hindi language along with their comparisons, illustrating how effective are these...  /reacttext  reacttext 445   /reacttext [show full ]	-11	1310
the alignment template approach to statistical machine translation	a phrasebased statistical machine translation approach 鈥 the alignment template approach 鈥 is described. this translation approach allows for general manytomany relations between words. thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. the model is described using a loglinear modeling approach, which is a generalization of the often used sourcechannel approach. thereby, the model is easier to extend than classical statistical machine translation systems. we describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. the evaluation of this approach is performed on three different tasks. for the germanenglish speech verbmobil task, we analyze the effect of various sys tem components. on the frenchenglish canadian hansards task, the alignment template system obtains significantly better results than a singlewordbased translation model. in the chineseenglish 2002 national institute of standards and technology (nist) machine transla tion evaluation it yields statistically significantly better nist scores than all competing research and commercial translation systems.	-9	1071
a comparative study on reordering constraints in statistical machine translation	in statistical machine translation, the generation of a translation hypothesis is computationally expensive. if arbitrary wordreorderings are permitted, the search problem is nphard. on the other hand, if we restrict the possible wordreorderings in an appropriate way, we obtain a polynomialtime search algorithm.	-10	965
findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation	this paper presents the results of the wmt10 and metricsmatr10 shared tasks, which included a translation task, a system combination task, and an evaluation task. we conducted a largescale manual evaluation of 104 machine translation.	-3	713
further meta-evaluation of machine translation	this paper analyzes the translation quality of machine translation systems for 10 language pairs translating between czech, english, french, german, hungarian, and spanish. we report the translation quality of over 30 diverse translation systems based on a largescale manual evaluation involv ing hundreds of hours of effort. we use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the cor relation with human judgments at both the systemlevel and at the sentencelevel. we validate our manual evaluation methodol ogy by measuring intra and interannotator agreement, and collecting timing information.	-5	436
clause restructuring for statistical machine translation	we describe a method for incorporating syntactic information in statistical machine translation systems. the first step of the method is to parse the source language string that is being translated. the second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. the goal of this step is to recover an underlying word order that is closer to the target language wordorder than the original string. the reordering approach is applied as a preprocessing step in both the training and decoding phases of a phrasebased statistical mt system. we describe experiments on translation from german to english, showing an improvement from 25.2 % bleu score for a baseline system to 26.8 % bleu score for the system with reordering, a statistically significant improvement.	-8	545
improved alignment models for statistical machine translation	in this paper, we describe improved alignment models for statistical machine translation. the statistical translation approach uses two types of information a translation model and a language model. the language model used is a bigram or general mgram model. the translation model is decomposed into a lexical and an alignment model. we describe two different approaches for statistical translation and present experimental results. the first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels a phrase level alignment between phrases and a word level alignment between single words. we present results using the verbmobil task (germanenglish, 6000word vocabulary) which is a limiteddomain spokenlanguage task. the experimental tests were performed on both the text transcription and the speech recognizer output. 1 statistical machine translation the goal of machine trans...	-14	683
exploiting similarities among languages for machine translation	dictionaries and phrase tables are the basis of modern statistical machine translation systems. this paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. it uses distributed representation of words and learns a linear mapping between vector spaces of languages. despite its simplicity, our method is surprisingly effective we can achieve almost 90% precision@5 for translation of words between english and spanish. this method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.	0	316
findings of the 2009 workshop on statistical machine translation	this paper presents the results of the wmt09 shared tasks, which included a translation task, a system combination task, and an evaluation task. we conducted a largescale manual evaluation of 87 machine translation systems and 22 system combination entries. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. we present a new evaluation technique whereby system output is edited and judged for correctness.	-4	711
tree-to-string alignment template for statistical machine translation	we present a novel translation model based on treetostring alignment template (tat) which describes the alignment be tween a source parse tree and a target string. a tat is capable of generating both terminals and nonterminals and per forming reordering at both low and high levels. the model is linguistically syntax based because tats are extracted auto matically from wordaligned, source side parsed parallel texts. to translate a source sentence, we first employ a parser to pro duce a source parse tree and then ap ply tats to transform the tree into a tar get string. our experiments show that the tatbased model significantly outper forms pharaoh, a stateoftheart decoder for phrasebased models.	-7	506
a phrase-based, joint probability model for statistical machine translation	a machine translation (mt) system utilizes a phrasebased joint probability model. the model is used to generate source and target language sentences simultaneously. in an embodiment, the model learns phrasetophrase alignments from wordtoword alignments generated by a wordtoword statistical mt system. the system utilizes the joint probability model for both sourcetotarget and targettosource translation applications.	-11	552
pharaoh: a beam search decoder for phrase-based statistical machine translation models	we describe pharaoh, a freely available decoder for phrasebased statistical machine translation models. the decoder is the implement at ion of an efficient dynamic programming search algorithm with lattice generation and xml markup for external components.	-9	802
(meta-) evaluation of machine translation	this paper evaluates the translation quality of machine translation systems for 8 language pairs translating french, german, spanish, and czech to english and back. we carried out an extensive human evaluation which allowed us not only to rank the different mt systems, but also to perform higherlevel analysis of the evaluation process. we measured timing and intra and interannotator agreement for three types of subjective evaluation. we measured the correlation of automatic evaluation metrics with human judgments. this metaevaluation reveals surprising facts about the most commonly used methodologies.	-6	438
large language models in machine translation	systems, methods, and computer program products for machine translation are provided. in some implementations a system is provided. the system includes a language model including a collection of ngrams from a corpus, each ngram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the ngram, each ngram corresponding to a backoff ngram having an order of n1 and a collection of backoff scores, each backoff score associated with an ngram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff ngram in the corpus.	-1	282
large language models in machine translation	this paper reports on the benefits of large scale statistical language modeling in ma chine translation. a distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion ngrams. it is capable of providing smoothed probabil ities for fast, singlepass decoding. we in troduce a new smoothing method, dubbed	-6	412
large language models in machine translation	this paper reports on the benefits of large scale statistical language modeling in ma chine translation. a distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion ngrams. it is capable of providing smoothed probabil ities for fast, singlepass decoding. we in troduce a new smoothing method, dubbed	-6	391
building a large-scale knowledge base for machine translation	knowledgebased machine translation (kbmt) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. the reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. one of the hypotheses being tested in the pangloss machine translation project is whether or not these resources can be semiautomatically acquired on a very large scale. this paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting kbmt. it contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. the ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semiautomatic methods. some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one kb to another. other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a kb in a second language, such as spanish or japanese.	-19	538
findings of the 2013 workshop on statistical machine translation	this paper presents the results of the wmt13 shared tasks, which included a translation task, a task for runtime estimation of machine translation quality, and an unofficial metrics task. this year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. an additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. the quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 1	0	246
findings of the 2013 workshop on statistical machine translation	we present the results of the wmt13 shared tasks, which included a translation task, a task for runtime estimation of machine translation quality, and an unofficial metrics task. this year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. an additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. the quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 1	0	246
findings of the 2009 workshop on statistical machine translation	j schroeder ed ac uk this paper presents the results of the wmt09 shared tasks, which included a translation task, a system combination task, and an evaluation task. we conducted a largescale manual evaluation of 87 machine translation systems and 22 system combination entries. we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. we present a new evaluation technique whereby system output is edited and judged for correctness.	0	246
discriminative training and maximum entropy models for statistical machine translation	we present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source channel approach as a special case. all knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.	-11	457
phrase-based statistical machine translation	this paper is based on the work carried out in the framework of the verbmobil project, which is a limiteddomain speech translation task (germanenglish).	-11	417
confidence estimation for machine translation	we present a detailed study of confidence estimation for machine translation. various methods for determining whether mt output is correct are investigated, for both whole sentences and words. since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. we present results on data from the nist 2003 chinesetoenglish mt evaluation.	-4	308
improving machine translation performance by exploiting non-parallel corpora	we present a novel method for discovering parallel sentences in comparable, nonparallel corpora. we train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. using this approach, we extract parallel data from large chinese, arabic, and english nonparallel newspaper corpora. we evaluate the quality of the extracted data by showing that it improves the performance of a stateoftheart statistical machine translation system. we also show that a goodquality mt system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large nonparallel corpus. thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.	-8	390
improving machine translation performance by exploiting non-parallel corpora	summary we present a novel method for discovering parallel sentences in comparable, nonparallel corpora. we train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. using this approach, we extract parallel data from large chinese, arabic, and english nonparallel newspaper corpora. we evaluate the quality of the extracted data by showing that it improves the performance of a stateoftheart statistical machine translation system. we also show that a goodquality mt system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large nonparallel corpus. thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.	-8	369
improving statistical machine translation using word sense disambiguation	we show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrasebased statistical machine translation (smt) model consistently improves translation quality across all three different iwslt chineseenglish test sets, as well as producing statistically significant improvements on the larger nist chineseenglish mt task鈥 and moreover never hurts performance on any test set, according not only to bleu but to all eight most commonly used automatic evaluation metrics. recent work has challenged the assumption that word sense disambiguation (wsd) systems are useful for smt. yet smt translation quality still obviously suffers from inaccurate lexical choice. in this paper, we address this problem by investigating a new strategy for integrating wsd into an smt system, that performs fully phrasal multiword disambiguation. instead of directly incorporating a sensevalstyle wsd system, we redefine the wsd task to match the exact same phrasal translation disambiguation task faced by phrasebased smt systems. our results provide the first known empirical evidence that lexical semantics are indeed useful for smt, despite claims to the contrary.	-6	346
batch tuning strategies for statistical machine translation	there has been a proliferation of recent work on smt tuning algorithms capable of handling larger feature sets than the traditional mert approach. we analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a structured svm. we perform empirical comparisons of eight different tuning strategies, including mert, in a variety of settings. among other results, we find that a simple and efficient batch version of mira performs at least as well as training online, and consistently outperforms other options.	-1	224
minimum bayes-risk decoding for statistical machine translation	we present minimum bayesrisk (mbr) decoding for statistical machine translation. this statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, wordtoword alignments from an mt system, and syntactic structure from parsetrees of source and target language sentences. we report the performance of the mbr decoders on a chinesetoenglish translation task. our results show that mbr decoding can be used to tune statistical mt performance for specific loss functions.	-9	386
lattice minimum bayes-risk decoding for statistical machine translation	we present minimum bayesrisk (mbr) de coding over translation lattices that compactly encode a huge number of translation hypothe ses. we describe conditions on the loss func tion that will enable efficient implementation of mbr decoders on lattices. we introduce an approximation to the bleu score (pap ineni et al., 2001) that satisfies these condi tions. the mbr decoding under this approx imate bleu is realized using weighted fi nite state automata. our experiments show that the lattice mbr decoder yields mod erate, consistent gains in translation perfor mance over nbest mbr decoding on arabic toenglish, chinesetoenglish and english tochinese translation tasks. we conduct a range of experiments to understand why lat tice mbr improves upon nbest mbr and study the impact of various parameters on mbr performance.	-5	312
maximum entropy based phrase reordering model for statistical machine translation	citeseerx  document details (isaac councill, lee giles, pradeep teregowda) we propose a novel reordering model for phrasebased statistical machine translation (smt) that uses a maximum entropy (maxent) model to predicate reorderings of neighbor blocks (phrase pairs). the model provides contentdependent, hierarchical phrasal reordering with generalization based on features automatically learned from a realworld bitext. we present an algorithm to extract all reordering events of neighbor blocks from bilingual data. in our experiments on chinesetoenglish translation, this maxentbased reordering model obtains significant improvements in bleu score on the nist mt05 and iwslt04 tasks. 1	-7	351
maximum entropy based phrase reordering model for statistical machine translation	we propose a novel reordering model for phrasebased statistical machine transla tion (smt) that uses a maximum entropy (maxent) model to predicate reorderings of neighbor blocks (phrase pairs). the model provides contentdependent, hier archical phrasal reordering with general ization based on features automatically learned from a realworld bitext. we present an algorithm to extract all reorder ing events of neighbor blocks from bilin gual data. in our experiments on chinese toenglish translation, this maxentbased reordering model obtains significant im provements in bleu score on the nist mt05 and iwslt04 tasks.	-7	347
the mathematics of machine translation: parameter estimation	we describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. we define a concept of wordbyword alignment between such pairs of sentences. for any given pair of such sentences each of our models assigns a probability to each of the possible wordbyword alignments. we give an algorithm for seeking the most probable of these alignments. although the algorithm is suboptimal, the alignment thus obtained accounts well for the wordbyword relationships in the pair of sentences. we have a great deal of data in french and english from the proceedings of the canadian parliament. accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. we also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that wordbyword alignments are inherent in any sufficiently large bilingual corpus.	-20	422
fast and optimal decoding for machine translation ☆	a good decoding algorithm is critical to the success of any statistical machine translation system. the decoder's job is to find the translation that is most likely according to a set of previously learned parameters (and a formula for combining them). since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. unfortunately, examining more of the space leads to unacceptably slow decodings. in this paper, we compare the speed and output quality of a traditional stackbased decoding algorithm with two new decoders a fast but nonoptimal greedy decoder and a slow but optimal decoder that treats decoding as an integerprogramming optimization problem.	-9	334
improving statistical machine translation using word sense disambiguation	citeseerx  document details (isaac councill, lee giles, pradeep teregowda) we show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrasebased statistical machine translation (smt) model consistently improves translation quality across all three different iwslt chineseenglish test sets, as well as producing statistically significant improvements on the larger nist chineseenglish mt task鈥 and moreover never hurts performance on any test set, according not only to bleu but to all eight most commonly used automatic evaluation metrics. recent work has challenged the assumption that word sense disambiguation (wsd) systems are useful for smt. yet smt translation quality still obviously suffers from inaccurate lexical choice. in this paper, we address this problem by investigating a new strategy for integrating wsd into an smt system, that performs fully phrasal multiword disambiguation. instead of directly incorporating a sensevalstyle wsd system, we redefine the wsd task to match the exact same phrasal translation disambiguation task faced by phrasebased smt systems.	-6	323
spmt: statistical machine translation with syntactified target language phrases	we introduce spmt, a new class of statistical translation models that use syntactified target language phrases. the spmt models outperform a state of the art phrasebased baseline model by 2.64 bleu points on the nist 2003 chineseenglish test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1	-7	332
batch tuning strategies for statistical machine translation	there has been a proliferation of recent work on smt tuning algorithms capable of handling larger feature sets than the traditional mert approach. we analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a structured svm. we perform empirical comparisons of eight different tuning strategies, including mert, in a variety of settings. among other results, we find that a simple and efficient batch version of mira performs at least as well as training online, and consistently outperforms other options.	-1	220
a smorgasbord of features for statistical machine translation	we describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation.	-9	340
a new string-to-dependency machine translation algorithm with a target dependency language model	in this paper, we propose a novel stringto dependency algorithm for statistical machine translation. with this new framework, we em ploy a target dependency language model dur ing decoding to exploit long distance word relations, which are unavailable with a tra ditional ngram language model. our ex periments show that the stringtodependency decoder achieves 1.48 point improvement in bleu and 2.53 point improvement in ter compared to a standard hierarchical stringto string system on the nist 04 chineseenglish evaluation set.	-5	294
better hypothesis testing for statistical machine translation: controlling for optimizer instability	in statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. to answer this question, he runs an experiment to evaluate the behavior of the two systems on heldout data. in this paper, we consider how to make such experiments more statistically reliable. we provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately	-2	223
syntax augmented machine translation via chart parsing	we present translation results on the shared task exploiting parallel texts for statistical machine translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. we use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. we present results on the frenchtoenglish task for this workshop, representing significant improvements over the workshop's baseline system. our translation system is available opensource under the gnu general public license.	-7	301
syntax augmented machine translation via chart parsing	we present translation results on the shared task exploiting parallel texts for statistical machine translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. we use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. we present results on the frenchtoenglish task for this workshop, representing significant improvements over the workshop's baseline system. our translation system is available opensource under the gnu general public license.	-7	298
the mathematics of machine translation: parameter estimation	this paper, we focus on the translation modeling problem. before we turn to this problem, however, we should address an issue that may be a concern to some readers why do we estimate pr(e) and pr(fle) rather than estimate pr(elf ) directly? we are really interested in this latter probability. wouldn't we reduce our problems from three to two by this direct approach? if we can estimate pr(fle) adequately, why can't we just turn the whole process around to estimate pr(elf)? to understand this, imagine that we divide french and english strings into those that are wellformed and those that are illformed. this is not a precise notion. we have in mind that strings like il va la bibliothque, or i live in a house, or even colorless green ideas sleep furiously are wellformed, but that strings like lava i1 bibliothque or a i in live house are not. when we translate a french string into english, we can think of ourselves as springing from a wellformed french string into the sea of wellformed english strings with the hope of landing on a good one. it is important, therefore, that our model for pr(elf ) concentrate its probability as much as possible on wellformed english strings. but it is not important that our model for pr(fle ) concentrate its probability on wellformed french strings.	-20	392
improved statistical machine translation using paraphrases	parallel corpora are crucial for training smt systems. however, for many lan guage pairs they are available only in very limited quantities. for these lan guage pairs a huge portion of phrases en countered at runtime will be unknown. we show how techniques from paraphras ing can be used to deal with these oth erwise unknown source language phrases. our results show that augmenting a state oftheart smt system with paraphrases leads to significantly improved coverage and translation quality. for a training corpus with 10,000 sentence pairs we in crease the coverage of unique test set un igrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.	-7	301
learning non-isomorphic tree mappings for machine translation	often one may wish to learn a treetotree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. unlike previous statistical formalisms (limited to isomorphic trees), allows local distortion of the tree topology. we reformulate it to permit dependency trees, and sketch em/viterbi algorithms for alignment, training, and decoding.	-10	337
word sense disambiguation improves statistical machine translation	recent research presents conflicting evidence on whether word sense disambiguation (wsd) systems can help to improve the performance of statistical machine translation (mt) systems. in this paper, we successfully integrate a stateoftheart wsd system into a stateoftheart hierarchical phrasebased mt system, hiero. we show for the first time that integrating a wsd system improves the performance of a stateoftheart statistical mt system on an actual translation task. furthermore, the improvement is statistically significant. 漏 2007 association for computational linguistics.	-6	288
optimizing chinese word segmentation for machine translation performance	previous work has shown that chinese word segmentation is useful for machine translation to english, yet the way different segmentation strategies affect mt is still poorly understood. in this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance. we find that other factors such as segmentation consistency and granularity of chinese words can be more important for machine translation. based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task, providing an improvement of 0.73 bleu. we also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 bleu increase. 1	-5	264
experiments in domain adaptation for statistical machine translation	the special challenge of the wmt 2007 shared task was domain adaptation. we took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here news commentary), when most of the training data is from a different domain (here european parliament speeches). this paper also gives a description of the submission of the university of edinburgh to the shared task.	-6	282
automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.	in this paper we describe two new objective automatic evaluation methods for machine translation. the first method is based on long est common subsequence between a candidate translation and a set of reference translations. longest common subsequence takes into ac count sentence level structure similarity natu rally and identifies longest cooccurring in sequence ngrams automatically. the second method relaxes strict ngram matching to skip bigram matching. skipbigram is any pair of words in their sentence order. skipbigram co occurrence statistics measure the overlap of skipbigrams between a candidate translation and a set of reference translations. the empiri cal results show that both methods correlate with human judgments very well in both ade quacy and fluency.	-9	332
better hypothesis testing for statistical machine translation: controlling for optimizer instability	in statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. to answer this question, he runs an experiment to evaluate the behavior of the two systems on heldout data. in this paper, we consider how to make such experiments more statistically reliable. we provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately	-2	212
statistical machine translation by parsing	designers of statistical machine translation (smt) systems have begun trying to exploit treestructured syntactic information. this article offers a coherent algorithmic framework to facilitate such efforts. our main contribution is a generalization of the common notion of parsing. in an ordinary parser, the input is a single string, and the grammar ranges over strings. in order to use syntactic information, an smt system requires generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. three particular generalizations, connected by some trivial glue, are all that is necessary for syntaxaware smt a synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the orrespondence relation between these structures. when a parser's input can have fewer dimensions than the parser's grammar, it is a translator. when a parser's grammar can have fewer dimensions than the parser's input, it is a synchronizer. this article offers a guided tour of these generalized parsing algorithms. it culminates with a recipe for using generalized parsing algorithms to train and apply a syntaxaware smt system.	-9	183
discriminative reranking for machine translation	this paper describes the application of discriminative reranking techniques to the problem of machine translation. for each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n best list of candidate translations in the target language. we introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the bleu metric. we provide experimental results on the nist 2003 chineseenglish large data track evaluation. we also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide stateoftheart performance in machine translation.	-9	183
synchronous binarization for machine translation	systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla tion output, but are often very computa tionally intensive.	-7	169
word-sense disambiguation for machine translation	in word sense disambiguation, a system attempts to determine the sense of a word from contextual fea tures. major barriers to building a highperforming word sense disambiguation system include the dif ficulty of labeling data for this task and of pre dicting finegrained sense distinctions. these is sues stem partly from the fact that the task is be ing treated in isolation from possible uses of au tomatically disambiguated data. in this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. we can use parallel language corpora as a large supply of partially labeled data for this task. we present algorithms for solving the word translation problem and demonstrate a signif icant improvement over a baseline system. we then show that the wordtranslation system can be used to improve performance on a simplified machine translation task and can effectively and accurately prune the set of candidate translations for a word.	-8	169
machine translation and telecommunications system using user id data to select dictionaries	a machine translation and telecommunications system includes a machine translation engine for translation of input text from a source language to a target language, a dictionary database including a core dictionary and a plurality of sublanguage (domain) dictionaries usable for translation from a source to a target language, a receiving interface for receiving text input from any of a plurality of users, each text input being accompanied by control information including user id data indicative of one or more sublanguages preferred by a particular user, an output interface, and a dictionary control module coupled to the receiving interface responsive to the user id data indicative of a sublanguage preference of a particular user for selecting a corresponding sublanguage dictionary of the dictionary database to be used by the machine translation engine along with the core dictionary for performing translation of the particular user's text input. user dictionaries can be maintained and selected to enhance translation accuracy in the same manner. the dictionary database encompassing core, sublanguage (domain), and user dictionaries is cumulated for greater capability over time through the use of dictionary maintenance utilities for updating the dictionaries.	-17	214
active learning and crowd-sourcing for machine translation.	in recent years, corpus based approaches to machine translation have become predominant, with statistical machine translation (smt) being the most actively progressing area. success of these approaches depends on the availability of parallel corpora. in this paper we propose active crowd translation (act), a new paradigm where active learning and crowdsourcing come together to enable automatic translation for lowresource language pairs. active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowdsourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. we experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. similarly, our experiments with crowdsourcing on mechanical turk have shown that it is possible to create parallel corpora using nonexperts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	-3	133
hmm word and phrase alignment for statistical machine translation	estimation and alignment procedures for word and phrase alignment hidden markov models (hmms) are developed for the alignment of parallel text. the development of these models is motivated by an analysis of the desirable features of ibm model 4, one of the original and most effective models for word alignment. these models are formulated to capture the desirable aspects of model 4 in an hmm alignment formalism. alignment behavior is analyzed and compared to humangenerated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. in analyzing alignment performance, chineseenglish word alignments are shown to be comparable to those of ibm model 4 even when models are trained over large parallel texts. in translation performance, phrasebased statistical machine translation systems based on these hmm alignments can equal and exceed systems based on model 4 alignments, and this is shown in arabicenglish and chineseenglish translation. these alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.	-5	154
large-scale dictionary construction for foreign language tutoring and interlingual machine translation	this paper describes techniques for automatic construction of dictionaries for use in largescale foreign language tutoring (flt) and interlingual machine translation (mt) systems. the dictionaries are based on a languageindependent representation called “lexical conceptual structure” (lcs). a primary goal of the lcs research is to demonstrate that synonymous verb senses share distributional patterns. we show how the syntax–semantics relation can be used to develop a lexical acquisition approach that contributes both toward the enrichment of existing online resources and toward the development of lexicons containing more complete information than is provided in any of these resources alone. we start by describing the structure of the lcs and showing how this representation is used in flt and mt. we then focus on the problem of building lcs dictionaries for largescale flt and mt. first, we describe authoring tools for manual and semiautomatic construction of lcs dictionaries; we then present a more sophisticated approach that uses linguistic techniques for building word definitions automatically. these techniques have been implemented as part of a set of lexicondevelopment tools used in the milt flt project.	-16	202
machine translation: an introductory guide	the article provides insights for dental hygienists on using machine translation (mt). tips on using mt are enumerated, one of which is avoiding complex sentence to avoid confusing translation software. the methods through which mt operates include those based on dictionary entries and based statistical rules. also cited are popular and free mt sites such as yahoo! babel fish at http//translation2.paralink.com/ and http//www.systranet.com.	-19	315
machine translation and telecommunications system	a machine translation and telecommunications system automatically translates input text in a source language to output text in a target language using a dictionary database (22) containing core language dictionaries for general words, a plurality of sublanguage dictionaries for specialized words of different domains or user groups, and a plurality of user dictionaries for individualized words used by different users. the system includes a receiving interface (11) for receiving input from a sender, in the form of electronic text, facsimile (graphics) input, or page image data, and an output module (30) for sending translated output text to any designated recipient(s). the input text is accompanied by a cover page or header (50) identifying the sender, one or more recipients, their addresses, the source/target languages of the text, any sublanguage(s) applicable to the input text, and any formatting requirements for the output text. the system uses the cover page or header data to select the core language, sublanguage, and/or user dictionaries to be used for translation processing, to format the translated output text, and to send the output to the recipient(s) at the designated address(es).	-17	309
statistical machine translation for query expansion in answer retrieval	we present an approach to query expan sion in answer retrieval that uses statisti cal machine translation (smt) techniques to bridge the lexical gap between ques tions and answers. smtbased query ex pansion is done by i) using a fullsentence paraphraser to introduce synonyms in con text of the entire query, and ii) by trans lating query terms into answer terms us ing a fullsentence smt model trained on questionanswer pairs. we evaluate these global, contextaware query expansion tech niques on tfidf retrieval from 10 million questionanswer pairs extracted from faq pages. experimental results show that smt based expansion improves retrieval perfor mance over local expansion and over re trieval without expansion.	-5	235
loosely tree-based alignment for machine translation	we augment a model of translation based on reordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. this is done by adding a new subtree cloning operation to either treetostring or treetotree alignment algorithms.	-10	279
chinese syntactic reordering for statistical machine translation	syntactic reordering approaches are an ef fective method for handling wordorder dif ferences between source and target lan guages in statistical machine translation (smt) systems. this paper introduces a re ordering approach for translation from chi nese to english. we describe a set of syntac tic reordering rules that exploit systematic differences between chinese and english word order. the resulting system is used as a preprocessor for both training and test sentences, transforming chinese sentences to be much closer to english in terms of their word order. we evaluated the reordering approach within the moses phrasebased smt system (koehn et al., 2007). the reordering approach improved the bleu score for the moses system from 28.52 to 30.86 on the nist 2006 evaluation data. we also conducted a series of experiments to an alyze the accuracy and impact of different types of reordering rules.	-6	241
manual and automatic evaluation of machine translation between european languages	we evaluated machine translation performance for six european language pairs that participated in a shared task translating french, german, spanish texts to english and back. evaluation was done automatically using the bleu score and manually on fluency and adequacy.	-7	246
a comparison of alignment models for statistical machine translation	in this paper, we present and compare various alignment models for statistical machine translation. we propose to measure the quality of an alignment model using the quality of the viterbi alignment compared to a manuallyproduced alignment and describe a rened annotation scheme to produce suitable reference alignments. the presented alignment models are then compared according to this error criterion. we also compare the impact of dierent alignment models on the translation quality of the alignment template system. 1 introduction in statistical machine translation (smt) it is necessary to model the translation probability p r(f j 1 je i 1 ). here f j 1 = f denotes the (french) source and e i 1 = e denotes the (english) target string. most smt models (brown et al., 1993; vogel et al., 1996) try to model wordtoword correspondences between source and target words using an alignment mapping from source position j to target position i = a j . formally, we can rewrite the pro...	-13	306
phrase-based joint probability model for statistical machine translation	a machine translation (mt) system utilizes a phrasebased joint probability model. the model is used to generate source and target language sentences simultaneously. in an embodiment, the model learns phrasetophrase alignments from wordtoword alignments generated by a wordtoword statistical mt system. the system utilizes the joint probability model for both sourcetotarget and targettosource translation applications.	-2	175
re-examining machine translation metrics for paraphrase identification	we propose to reexamine the hypothesis that automated metrics developed for mt evaluation can prove useful for paraphrase identification in light of the significant work on the development of new mt metrics over the last 4 years. we show that a metaclassifier trained using nothing but recent mt metrics outperforms all previous paraphrase identification approaches on the microsoft research paraphrase corpus. in addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on mt metrics. we release both the new dataset and the error analysis annotations for use by the community.	-1	91
recent advances in example-based machine translation	book review recent advances in examplebased machine translation michael carl and andy way (editors) (universitat des saarlandes and dublin city university) dordrecht  kluwer academic publishers (text, speech and language technology series, edited by nancy ide and jean veronis, volume 21), 2003 , xxxi+482 pp; hardbound, isbn 1402014007 , $173.00, f115.00, 180.00	-9	158
deeper sentiment analysis using machine translation technology	this paper proposes a new paradigm for sentiment analysis translation from text documents to a set of sentiment units. the techniques of deep language analysis for machine translation are applicable also to this kind of text mining task. we developed a highprecision sentiment analysis system at a low development cost, by making use of an existing transferbased machine translation engine.	-9	158
following directions using statistical machine translation	mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. in this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and a map of an environment built by a robot. our approach uses training data to learn to translate from natural language instructions to an automaticallylabeled map. the complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map. as a result, our technique can efficiently handle uncertainty in both map labeling and parsing. our experiments demonstrate the promising capabilities achieved by our approach.	-3	108
generation of word graphs in statistical machine translation	statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. we describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. the advantage is that these hypotheses can be rescored using a refined language or translation model.	-11	164
active learning and crowdsourcing for machine translation in low resource scenarios	corpus based approaches to automatic translation such as example based and statistical machine translation systems use large amounts of parallel data created by humans to train mathematical models for automatic language translation. large scale parallel data generation for new language pairs requires intensive human effort and availability of fluent bilinguals or expert translators. therefore it becomes immensely difficult and expensive to provide stateoftheart machine translation (mt) systems for rare languages.	-1	98
a comparative study on reordering constraints in statistical machine translation	in statistical machine translation, the generation of a translation hypothesis is computationally expensive. if arbitrary wordreorderings are permitted, the search problem is nphard. on the other hand, if we restrict the possible wordreorderings in an appropriate way, we obtain a polynomialtime search algorithm.in this paper, we compare two different reordering constraints, namely the itg constraints and the ibm constraints. this comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. we show a connection between the itg constraints and the since 1870 known schröder numbers.we evaluate these constraints on two tasks the verbmobil task and the canadian hansards task. the evaluation consists of two parts first, we check how many of the viterbi alignments of the training corpus satisfy each of these constraints. second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.the experiments will show that the baseline itg constraints are not sufficient on the canadian hansards task. therefore, we present an extension to the itg constraints. these extended itg constraints increase the alignment coverage from about 87% to 96%.	-10	157
a novel string-to-string distance measure with applications to machine translation evaluation	we introduce a stringtostring distance measure which extends the edit distance by block transpositions as constant cost edit operation. an algorithm for the calculation of this distance measure in polynomial time is presented. we then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. the correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. in general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level.	-10	161
the candide system for machine translation	we present an overview of candide, a system for automatic translation of french text to english text. candide uses methods of information theory and statistics to develop a probability model of the translation process. this model, which is made to accord as closely as possible with a large body of french and english sentence pairs, is then used to generate english translations of previously unseen french sentences. this paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 1. introduction candide is an experimental computer program, now in its fifth year of development at ibm, for translation of french text to english text. our goal is to perform fullyautomatic, highquality texttotext translation. however, because we are still far from achieving this goal, the program can be used in both fullyautomatic and translator'sassistant modes. our approach is founded upon the statistical analysis of language....	-19	179
word-sense disambiguation for machine translation	in word sense disambiguation, a system attempts to determine the sense of a word from contextual fea tures. major barriers to building a highperforming word sense disambiguation system include the dif ficulty of labeling data for this task and of pre dicting finegrained sense distinctions. these is sues stem partly from the fact that the task is be ing treated in isolation from possible uses of au tomatically disambiguated data. in this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. we can use parallel language corpora as a large supply of partially labeled data for this task. we present algorithms for solving the word translation problem and demonstrate a signif icant improvement over a baseline system. we then show that the wordtranslation system can be used to improve performance on a simplified machine translation task and can effectively and accurately prune the set of candidate translations for a word.	-8	144
domain adaptation for machine translation by mining unseen words	we show that unseen words account for a large part of the translation error when moving to new domains. using an extension of a recent approach to mining translations from comparable corpora (haghighi et al., 2008), we are able to find translations for otherwise oov terms. we show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 bleu points) on four domains and two language pairs. 1	-2	98
improving machine translation quality with automatic named entity recognition	named entities create serious problems for stateoftheart commercial machine translation (mt) systems and often cause translation failures beyond the local context, affecting both the overall morphosyntactic wellformedness of sentences and word sense disambiguation in the source text. we report on the results of an experiment in which mt input was processed using output from the named entity recognition module of sheffield's gate information extraction (ie) system. the gain in mt quality indicates that specific components of ie technology could boost the performance of current mt systems.	-10	147
translation engines: techniques for machine translation	machine translation (mt) is the area of computer science and applied linguistics dealing with the translation of human languages such as english and german. mt on the internet has become an important tool by providing fast, economical and useful translations. with globalisation and expanding trade, demand for translation is set to grow. translation engines covers theoretical and practical aspects of mt, both classic and new, including  character sets and formatting languages  translation memory  linguistic and computational foundations  basic computational linguistic techniques  transfer and interlingua mt  evaluation software accompanies the text, providing readers with hands on experience of the main algorithms.	-14	167
tailoring word alignments to syntactic machine translation	domain adaptationjohn blitzer and hal daumé iiiclassical “singledomain” learningpredicthorrible book, horrible. this book was horrible. i read half, suffering from a headache the entire time, and eventually i lit it on fire. 1 less copy in the world. don't waste your money. i wish i had the time spent reading this book back. it wasted my lifeso the topic of ah the talk today is online learningdomain adaptationso the topic of ah the talk today is online learningeverything is happening online. even the	-6	125
reordering constraints for phrase-based statistical machine translation	in statistical machine translation, the generation of a translation hypothesis is computationally expensive. if arbitrary reorderings are permitted, the search problem is nphard. on the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomialtime search algorithm. we investigate different reordering constraints for phrasebased statistical machine translation, namely the ibm constraints and the itg constraints. we present efficient dynamic programming algorithms for both constraints. we evaluate the constraints with respect to translation quality on two japanese鈥揈nglish tasks. we show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. the itg constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search.	-9	149
consensus network decoding for statistical machine translation system combination	this paper presents a simple and robust consensus decoding approach for combining multiple machine translation (mt) system outputs. a consensus network is constructed from an nbest list by aligning the hypotheses against an alignment reference, where the alignment is based on minimising the translation edit rate (ter). the minimum bayes risk (mbr) decoding technique is investigated for the selection of an appropriate alignment reference. several alternative decoding strategies proposed to retain coherent phrases in the original translations. experimental results are presented primarily based on threeway combination of chineseenglish translation outputs, and also presents results for sixway system combination. it is shown that worthwhile improvements in translation performance can be obtained using the methods discussed.	-6	128
adaptation of the translation model for statistical machine translation based on information retrieval	. in this paper we present experiments concerning translation model adaptation for statistical machine translation. we develop a method to adapt translation models using information retrieval. the approach selects sentences similar to the test set.	-8	138
perplexity minimization for translation model domain adaptation in statistical machine translation	we investigate the problem of domain adaptation for parallel data in statistical machine translation (smt). while techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. we also explore adapting multiple (4鈥10) data sets with no a priori distinction between indomain and outofdomain data except for an indomain development set.	-1	88
n-gram-based machine translation	this article describes in detail an ngram approach to statistical machine translation. this approach consists of a loglinear combination of a translation model based on ngrams of bilingual units, which are referred to as tuples, along with four specific feature functions. translation performance, which happens to be in the state of the art, is demonstrated with spanishtoenglish and englishtospanish translations of the european parliament plenary sessions (epps).	-7	200
machine translation with inferred stochastic finite-state transducers	summary finitestate transducers are models that are being used in different areas of pattern recognition and computational linguistics. one of these areas is machine translation, in which the approaches that are based on building models automatically from training examples are becoming more and more attractive. finitestate transducers are very adequate for use in constrained tasks in which training samples of pairs of sentences are available. a technique for inferring finitestate transducers is proposed in this article. this technique is based on formal relations between finitestate transducers and rational grammars. given a training corpus of sourcetarget pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an $n$gram) is inferred. this grammar is finally converted into a finitestate transducer. the proposed methods are assessed through a series of machine translation experiments within the framework of the eutrans project.	-7	199
machine translation divergences: a formal description and proposed solution	this paper demonstrates that a systematic solution to the divergence problem can be derived from the forrealization of two types of information (1) the linguistically grounded classes upon which lexicalsemantic divergences are based; and (2) the techniques by which lexicalsemantic divergences are resolved. this forrealization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system	-19	269
improvements in phrase-based statistical machine translation	in statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. we describe the baseline phrasebased translation system and various refinements. we describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. we present translation results for three tasks verbmobil, xerox and the canadian hansards. for the xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10k words. the translation results for the xerox and canadian hansards task are very promising. the system even outperforms the alignment template system.	-9	225
joshua: an open source toolkit for parsing-based machine translation	we describe joshua, an open source toolkit for statistical machine translation. joshua implements all of the algorithms required for synchronous context free grammars (scfgs) chartparsing, ngram language model integration, beamand cubepruning, and kbest extraction. the toolkit also implements suffixarray grammar extraction and minimum error rate training. it uses parallel and distributed computing techniques for scalability. we demonstrate that the toolkit achieves state of the art translation performance on the wmt09 frenchenglish translation task. 1	-4	164
europarl: a multilingual corpus for evaluation of machine translation	this paper reports results of the 1992 evaluation of machine translation (mt) systems in the darpa mt initiative and results of a pretest to the 1993 evaluation. the darpa initiative is unique in that the evaluated systems differ radically in languages translated, theoretical approach to system design, and intended enduser application. in the 1992 suite, a comprehension test compared the accuracy and interpretability of system and control outputs; a quality panel for each language pair judged the fidelity of translations from each source version. the 1993 suite evaluated adequacy and fluency and investigated three scoring methods.	-11	230
minimum error-rate training in statistical machine translation using structural svms	different works on training of loglinear interpolation models for statistical machine translation reported performance improvements by optimizing parameters with respect to translation quality.	-4	174
lattice-based minimum error rate training for statistical machine translation	minimum error rate training (mert) is an effective means to estimate the feature func tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. to accomplish this, the training procedure determines for each feature func tion its exact error surface on a given set of candidate translations. the feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. typically, candidates in mert are represented as n  best lists which contain the n most probable translation hypotheses produced by a decoder. in this paper, we present a novel algorithm that allows for efficiently constructing and repre senting the exact error surface of all trans lations that are encoded in a phrase lattice. compared to n best mert, the number of candidate translations thus taken into account increases by several orders of magnitudes. the proposed method is used to train the feature function weights of a phrasebased statistical machine translation system. experi ments conducted on the nist 2008 translation tasks show significant runtime improvements and moderate bleu score gains over n best mert.	-5	178
orange: a method for evaluating automatic evaluation metrics for machine translation	comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as pearson's product moment correlation coefficient or spearman's rank order correlation coefficient between human scores and automatic scores. however, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. unfortunately, these judgments are often inconsistent and very expensive to acquire. in this paper, we introduce a new evaluation method, orange, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. we also show the results of comparing several existing automatic metrics and three new automatic metrics using orange.	-9	222
a non-contiguous tree sequence alignment-based model for statistical machine translation	the tree sequence based translation model al lows the violation of syntactic boundaries in a rule to capture nonsyntactic phrases, where a tree sequence is a contiguous sequence of sub trees. this paper goes further to present a trans lation model based on noncontiguous tree se quence alignment, where a noncontiguous tree sequence is a sequence of subtrees and gaps. compared with the contiguous tree sequence based model, the proposed model can well han dle noncontiguous phrases with any large gaps by means of noncontiguous tree sequence alignment. an algorithm targeting the non contiguous constituent decoding is also proposed. experimental results on the nist mt05 chi neseenglish translation task show that the pro posed model statistically significantly outper forms the baseline systems.	-4	176
synchronous binarization for machine translation	systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla tion output, but are often very computa tionally intensive. the complexity is ex ponential in the size of individual gram mar rules due to arbitrary reorderings be tween the two languages, and rules ex tracted from parallel corpora can be quite large. we devise a lineartime algorithm for factoring syntactic reorderings by bi narizing synchronous rules when possible and show that the resulting rule set signif icantly improves the speed and accuracy of a stateoftheart syntaxbased machine translation system.	-7	186
review article: example-based machine translation	in the last ten years there has been a significant amount ofresearch in machine translation within a ``new'' paradigm ofempirical approaches, often labelled collectively as``examplebased'' approaches. the first manifestation of thisapproach caused some surprise and hostility among observers moreused to different ways of working, but the techniques were quicklyadopted and adapted by many researchers, often creating hybridsystems. this paper reviews the various research efforts withinthis paradigm reported to date, and attempts a categorisation ofdifferent manifestations of the general approach.	-14	232
combining outputs from multiple machine translation systems	currently there are several approaches to machine translation (mt) based on differ ent paradigms; e.g., phrasal, hierarchical and syntaxbased. these three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. the availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple sys tems. this paper describes three differ ent approaches to mt system combina tion. these combination methods oper ate on sentence, phrase and word level exploiting information from best lists, system scores and targettosource phrase alignments. the wordlevel combination provides the most robust gains but the best results on the development test sets (nist mt05 and the newsgroup portion of gale 2006 dryrun) were achieved by combining all three methods.	-6	183
statistical machine translation : from single word models to alignment templates /	in diesear arbeit werden neue ans01tze zur sprachübersetzung basierend auf statistischen verfahren vorgestellt. als verallgemeinerung zu dem üblicherweise verwendeten sourcechannel modell wird ein allgemeineres modell basierend auf dem maximumentropieprinzip vorgeschlagen. es werden verschiedene verfahren zur bestimmung von wortalignments unter nutzung von statistischen und heuristischen modellen beschrieben. dabei werden insbesondere verschiedene gl01ttungsverfahren, methoden zur integration zus01tzlicher lexika und trainingsverfahren verglichen. eine detaillierte bewertung der alignmentqualit01t wird durchgeführt indem die automatisch erstellten wortalignments mit manuell erstellten alignments verglichen werden. aufbauend auf diesen grundlegenden einzelwortbasierten alignmentmodellen wird dann ein phrasenbasiertes statistisches 05bersetzungsmodell, das alignment template modell, vorgeschlagen. für dieses modell wird ein trainingsverfahren und ein effizienter suchalgorithmus basierend auf dem prinzip der dynamischer programmierung und strahlsuche entwickelt.	-11	207
a polynomial-time algorithm for statistical machine translation	we introduce a polynomialtime algorithm for statistical machine translation. this algorithm can be used in place of the expensive, slow bestfirst search strategies in current statistical translation architectures. the approach employs the stochastic bracketing transduction grammar (sbtg) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. the new algorithm in our experience yields major speed improvement with no significant loss of accuracy.	-17	226
online large-margin training for statistical machine translation	we achieved a state of the art performance in statistical machine translation by using a large number of features with an online largemargin training algorithm. the millions of parameters were tuned only on a small development set consisting of less than 1k sentences. experiments on arabictoenglish translation indicated that a model trained with sparse binary features outperformed a conventional smt system with a small number of features. 1	-6	182
learning for semantic parsing with statistical machine translation	we present a novel statistical approach to semantic parsing, wasp, for construct ing a complete, formal meaning represen tation of a sentence. a semantic parser is learned given a set of sentences anno tated with their correct meaning represen tations. the main innovation of wasp is its use of stateoftheart statistical ma chine translation techniques. a word alignment model is used for lexical acqui sition, and the parsing model itself can be seen as a syntaxbased translation model. we show that wasp performs favorably in terms of both accuracy and coverage compared to existing learning methods re quiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.	-7	192
a tree-to-tree alignment-based model for statistical machine translation	this paper presents a novel statistical machine translation (smt) model that uses treetotree alignment between a source parse tree and a target parse tree. the model is formally a probabilistic synchronous treesubstitution grammar (stsg) that is a collection of aligned elementary tree pairs with mapping probabilities (which are automatically learned from wordaligned biparsed parallel texts). unlike previous syntaxbased smt models, this new model supports multilevel global structure distortion of the tree typology and can fully utilize the source and target parse tree structure features, which gives our system more expressive power and flexibility. the experimental results on the hit biparsed text show that our method performs significantly better than pharaoh, a stateoftheart phrasebased smt system, and other syntaxbased methods, such as the synchronous cfgbased method on the small dataset.	-6	184
computing consensus translation for multiple machine translation systems using enhanced hypothesis alignment	this paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (mt) systems. the outputs are combined and a possibly new translation hypothesis can be generated. similarly to the wellestablished rover approach of (fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. to create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. the context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	-7	190
discriminative instance weighting for domain adaptation in statistical machine translation	we describe a new approach to smt adaptation that weights outofdomain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. this extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. we incorporate instance weighting into a mixturemodel framework, and find that it yields consistent improvements over a wide range of baselines.	-3	140
machine translation: past, present, future	most computational linguists are probably aware that machine translation (mt) has provided the impetus for a number of important advances in linguistics and computing over the past 40 years. but even those who have worked on mt could not have fully appreciated.	-27	237
machine translation and telecommunications system using user id data to select dictionaries	a machine translation and telecommunications system includes a machine translation engine for translation of input text from a source language to a target language, a dictionary database including a core dictionary and a plurality of sublanguage (domain) dictionaries usable for translation from a source to a target language, a receiving interface for receiving text input from any of a plurality of users, each text input being accompanied by control information including user id data indicative of one or more sublanguages preferred by a particular user, an output interface, and a dictionary control module coupled to the receiving interface responsive to the user id data indicative of a sublanguage preference of a particular user for selecting a corresponding sublanguage dictionary of the dictionary database to be used by the machine translation engine along with the core dictionary for performing translation of the particular user's text input. user dictionaries can be maintained and selected to enhance translation accuracy in the same manner. the dictionary database encompassing core, sublanguage (domain), and user dictionaries is cumulated for greater capability over time through the use of dictionary maintenance utilities for updating the dictionaries.	-17	214
active learning and crowd-sourcing for machine translation	in recent years, corpus based approaches to machine translation have become predominant, with statistical machine translation (smt) being the most actively progressing area. success of these approaches depends on the availability of parallel corpora. in this paper we propose active crowd translation (act), a new paradigm where active learning and crowdsourcing come together to enable automatic translation for lowresource language pairs. active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowdsourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. we experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. similarly, our experiments with crowdsourcing on mechanical turk have shown that it is possible to create parallel corpora using nonexperts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	-3	135
predicting the sentence-level quality of machine translation systems	we investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available. the problem is addressed as a regression task and a method that takes into account t...	-4	146
the meteor metric for automatic evaluation of machine translation	the meteor  automatic metric for machine translation evaluation, originally developed and released in 2004, was designed with the explicit goal of producing sentencelevel scores which correlate well with human judgments of translation quality. several key design decisions were incorporated into meteor  in support of this goal. in contrast with ibm鈥檚 bleu , which uses only precisionbased features, meteor  uses and emphasizes recall in addition to precision, a property that has been confirmed by several metrics as being critical for high correlation with human judgments. meteor  also addresses the problem of reference translation variability by utilizing flexible word matching, allowing for morphological variants and synonyms to be taken into account as legitimate correspondences. furthermore, the feature ingredients within meteor  are parameterized, allowing for the tuning of the metric鈥檚 free parameters in search of values that result in optimal correlation with human judgments. optimal parameters can be separately tuned for different types of human judgments and for different languages. we discuss the initial design of the meteor  metric, subsequent improvements, and performance in several independent evaluations in recent years.	-4	150
hmm word and phrase alignment for statistical machine translation	estimation and alignment procedures for word and phrase alignment hidden markov models (hmms) are developed for the alignment of parallel text. the development of these models is motivated by an analysis of the desirable features of ibm model 4, one of the original and most effective models for word alignment. these models are formulated to capture the desirable aspects of model 4 in an hmm alignment formalism. alignment behavior is analyzed and compared to humangenerated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. in analyzing alignment performance, chineseenglish word alignments are shown to be comparable to those of ibm model 4 even when models are trained over large parallel texts. in translation performance, phrasebased statistical machine translation systems based on these hmm alignments can equal and exceed systems based on model 4 alignments, and this is shown in arabicenglish and chineseenglish translation. these alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.	-5	154
apertium: a free/open-source platform for rule-based machine translation	apertium is a free/opensource platform for rulebased machine translation. it is being widely used to build machine translation systems for a variety of l...	-2	125
demonstration of joshua: an open source toolkit for parsing-based machine translation	we describe joshua, an open source toolkit for statistical machine transla tion. joshua implements all of the algo rithms required for synchronous context free grammars (scfgs) chartparsing, n gram language model integration, beam and cubepruning, and kbest extraction. the toolkit also implements suffixarray grammar extraction and minimum error rate training. it uses parallel and dis tributed computing techniques for scala bility. we demonstrate that the toolkit achieves state of the art translation per formance on the wmt09 frenchenglish translation task.	-4	147
statistical machine translation by parsing	designers of statistical machine translation (smt) systems have begun trying to exploit treestructured syntactic information. this article offers a coherent algorithmic framework to facilitate such efforts. our main contribution is a generalization of the common notion of parsing. in an ordinary parser, the input is a single string, and the grammar ranges over strings. in order to use syntactic information, an smt system requires generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. three particular generalizations, connected by some trivial glue, are all that is necessary for syntaxaware smt a synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the orrespondence relation between these structures. when a parser's input can have fewer dimensions than the parser's grammar, it is a translator. when a parser's grammar can have fewer dimensions than the parser's input, it is a synchronizer. this article offers a guided tour of these generalized parsing algorithms. it culminates with a recipe for using generalized parsing algorithms to train and apply a syntaxaware smt system.	-9	183
discriminative reranking for machine translation	this paper describes the application of discriminative reranking techniques to the problem of machine translation. for each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n best list of candidate translations in the target language. we introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the bleu metric. we provide experimental results on the nist 2003 chineseenglish large data track evaluation. we also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide stateoftheart performance in machine translation.	-9	183
synchronous binarization for machine translation	systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine transla tion output, but are often very computa tionally intensive. the complexity is ex ponential in the size ...	-7	169
word-sense disambiguation for machine translation	in word sense disambiguation, a system attempts to determine the sense of a word from contextual fea tures. major barriers to building a highperforming word sense disambiguation system include the dif ficulty of labeling data for this task and of pre dicting finegrained sense distinctions. these is sues stem partly from the fact that the task is be ing treated in isolation from possible uses of au tomatically disambiguated data. in this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. we can use parallel language corpora as a large supply of partially labeled data for this task. we present algorithms for solving the word translation problem and demonstrate a signif icant improvement over a baseline system. we then show that the wordtranslation system can be used to improve performance on a simplified machine translation task and can effectively and accurately prune the set of candidate translations for a word.	-8	169
morphological analysis for statistical machine translation	we present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities. the technique presupposes finegrained segmentation of a word in the morphologically rich language into the sequence of prefix(es)stemsuffix(es) and partofspeech tagging of the parallel corpus. the algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry. the technique improves arabictoenglish translation qualities significantly when applied to ibm model 1 and phrase translation models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs.	-9	176
active learning and crowd-sourcing for machine translation.	in recent years, corpus based approaches to machine translation have become predominant, with statistical machine translation (smt) being the most actively progressing area. success of these approaches depends on the availability of parallel corpora. in this paper we propose active crowd translation (act), a new paradigm where active learning and crowdsourcing come together to enable automatic translation for lowresource language pairs. active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowdsourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. we experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. similarly, our experiments with crowdsourcing on mechanical turk have shown that it is possible to create parallel corpora using nonexperts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.	-3	133
experiments and prospects of example-based machine translation	ebmt (examplebased machine translation) is proposed. ebmt retrieves similar examples (pairs of source phrases, sentences, or texts and their translations) from a tahase of examples, adapting the examples to franslate a new input. ebmt has the following features (1) it is easily upgr, zled simply by inputting appropriate examples to the database; (2) it assigns a reliability factor to the translation result; (3) it is accelerated effectively by both indexing axi parallel computing; (4) it is robust because of bestmatch reasoning; (5) it well utilizes translator expertise. a prototype system has been implemented to deal with a difficult iranslation problem fee conventional rulebased machine translation (rbmt), i.e., translating japanese noun phrases of the form 'lq a no n2 into english. the system has achieved about a 78% success rate on average. this paper explains the basic idea of ebmt, illustrates the experiment in detail, explains the broad applicability of ebmt to several difficult translation problems fee rbmt discusses the advantages of integrating ebmt with rbmt.	-22	211
word reordering and a dynamic programming beam search algorithm for statistical machine translation	summary in this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (dp). the search algorithm uses the translation model presented in [{\it p. f. brown}, {\it s. a. della pietra}, {\it v. j. della pietra} and {\it r. l. mercer}, 鈥淭he mathematics of statistical machine translation parameter estimation, ibid. 19, no. 2, 263鈥311 (1993)]. starting from a dpbased solution to the travelingsalesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. word reordering restrictions especially useful for the translation direction german to english are presented. the restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. the beam search procedure has been successfully tested on the verbmobil task (german to english, 8,000word vocabulary) and on the canadian hansards task (french to english, 100,000word vocabulary). for the mediumsized verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur	-10	178
large-scale dictionary construction for foreign language tutoring and interlingual machine translation	this paper describes techniques for automatic construction of dictionaries for use in largescale foreign language tutoring (flt) and interlingual machine translation (mt) systems. the dictionaries are based on a languageindependent representation called “lexical conceptual structure” (lcs). a primary goal of the lcs research is to demonstrate that synonymous verb senses share distributional patterns. we show how the syntax–semantics relation can be used to develop a lexical acquisition approach that contributes both toward the enrichment of existing online resources and toward the development of lexicons containing more complete information than is provided in any of these resources alone. we start by describing the structure of the lcs and showing how this representation is used in flt and mt. we then focus on the problem of building lcs dictionaries for largescale flt and mt. first, we describe authoring tools for manual and semiautomatic construction of lcs dictionaries; we then present a more sophisticated approach that uses linguistic techniques for building word definitions automatically. these techniques have been implemented as part of a set of lexicondevelopment tools used in the milt flt project.	-16	202
machine translation of languages	, machine translation of languages (pp. 1523). new york  john wiley & sons.w. weaver.translation (1949). machine translation of languages . 1955weaver w. (1955). translation (1949). in machine translation of languages, mit ...	-58	227
statistical machine translation by parsing	in an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. this paper explores generalizations of ordinary parsing algo rithms that allow the input to consist of string tu ples and/or the grammar to range over string tu ples. such algorithms can infer the synchronous structures hidden in parallel texts. it turns out that these generalized parsers can do most of the work required to train and apply a syntaxaware statisti cal machine translation system.	-9	167
the cmu statistical machine translation system	in this paper we describe the components of our statistical machine translation system.	-10	169
distortion models for statistical machine translation.	in this paper, we argue that ngram lan guage models are not sufficient to address word reordering required for machine trans lation. we propose a new distortion model that can be used with existing phrasebased smt decoders ...	-7	157
word-level confidence estimation for machine translation	this article introduces and evaluates several different wordlevel confidence measures for ma chine translation. these measures provide a method for labeling each word in an automatically generated translation as correct or incorrect. all approaches to confidence estimation presented here are based on word posterior probabilities. different concepts of word posterior probabilities as well as different ways of calculating them will be introduced and compared. they can be divided into two categories systembased methods that explore knowledge provided by the translation system that generated the translations, and direct methods that are independent of the translation system. the systembased techniques make use of system output, such as word graphs or nbest lists. the word posterior probability is determined by summing the probabilities of the sentences in the translation hypothesis space that contains the target word. the direct confidence measures take other knowledge sources, such as word or phrase lexica, into account. they can be applied to output from nonstatistical machine translation systems as well. experimental assessment of the different confidence measures on various translation tasks and in several language pairs will be presented.	-6	150
computing consensus translation from multiple machine translation systems	we address the problem of computing a consensus translation given the outputs from a set of machine translation (mt) systems. the translations from the mt systems are aligned with a multiple string alignment algorithm and the consensus translation is then computed. we describe the multiple string alignment algorithm and the consensus mt hypothesis computation. we report on the subjective and objective performance of the multilingual acquisition approach on a limited domain spoken language application. we evaluate five domainindependent offtheshelf mt systems and show that the consensusbased translation performance is equal to or better than any of the given mt systems, in terms of both objective and subjective measures.	-12	179
a maximum entropy word aligner for arabic-english machine translation.	this paper presents a maximum entropy word alignment algorithm for arabic english based on supervised training data. we demonstrate that it is feasible to cre ate training material for problems in ma chine translation and that a mixture of su pervised and unsupervised methods yields superior performance. the probabilistic model used in the alignment directly mod els the link decisions. significant improve ment over traditional word alignment tech niques is shown as well as improvement on several machine translation tests. perfor mance of the algorithm is contrasted with human annotation performance.	-8	161
active learning and crowdsourcing for machine translation in low resource scenarios	ambati, v., vogel, s., carbonell, j. active learning and crowdsourcing for machine translation. language resources and evaluation (lrec) 7, 2169–2174 (2010)ambati, v., vogel, s., & carbonell j. 2010. active learning ...	-1	98
the world wide web as a resource for example-based machine translation tasks	the www is two orders of magnitude larger than the largest corpora. although noisy, web text presents language as it is used, and statistics derived from the web can have practical uses in many nlp applications. for this reason, the www should be seen and studied as any other computationally available linguistic resource. in this article, we illustrate this by showing that an examplebased approach to lexical choice for machine translation can use the web as an adequate and free resource.	-14	187
recent advances in example-based machine translation	book review recent advances in examplebased machine translation michael carl and andy way (editors) (universitat des saarlandes and dublin city university) dordrecht  kluwer academic publishers (text, speech and language technology series, edited by nancy ide and jean veronis, volume 21), 2003 , xxxi+482 pp; hardbound, isbn 1402014007 , $173.00, f115.00, 180.00	-9	158
deeper sentiment analysis using machine translation technology	this paper proposes a new paradigm for sentiment analysis translation from text documents to a set of sentiment units. the techniques of deep language analysis for machine translation are applicable also to this kind of text mining task. we developed a highprecision sentiment analysis system at a low development cost, by making use of an existing transferbased machine translation engine.	-9	158
generation of word graphs in statistical machine translation	statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. we describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. the advantage is that these hypotheses can be rescored using a refined language or translation model.	-11	164
two approaches to matching in example-based machine translation	this paper describes two approaches to matching input strings with strings from a translation archive in the examplebased machine translation paradigm  the more canonical chunking + matching + recombination method and an alternative method of matching at the level of complete sentences. the latter produces less exact matches while the former suffers from (often serious) translation quality lapses at the boundaries of recombined chunks. a set of text matching criteria was selected to reflect the tradeoff between utility and computational price of each criterion. a metric for comparing text passages was devised and calibrated with the help of a specially constructed diagnostic example set. a partitioning algorithm was developed for finding an optimum cover of an input string by a set of bestmatching shorter chunks. the results were evaluated in a monolingual setting using an existing mt postediting tool the distance between the input and its best match in the archive was calculated in terms of the number of keystrokes necessary to reduce the latter to the former. as a result, the metric was adjusted and an experiment was run to test the two ebmt methods, both on the training corpus and on the working corpus (or archive) of some 6,500 sentences.	-20	189
proceedings of the fourth workshop on statistical machine translation	we describe two systems for englishtoczech machine translation that took part in the wmt09 translation task. one of the systems is a tuned phrasebased system and the other one is based on a linguistically motivated analysistransfers...	-4	124
active learning and crowdsourcing for machine translation in low resource scenarios	corpus based approaches to automatic translation such as example based and statistical machine translation systems use large amounts of parallel data created by humans to train mathematical models for automatic language translation. large scale parallel data generation for new language pairs requires intensive human effort and availability of fluent bilinguals or expert translators. therefore it becomes immensely difficult and expensive to provide stateoftheart machine translation (mt) systems for rare languages.	-1	98
a productivity test of statistical machine translation post-editing in a typical localisation context	we evaluated the productivity increase of statistical mt postediting as compared to traditional translation in a twoday test involving twelve participants translating from english to french, italian, german, and spanish. the test setup followed an empirical methodology. a random subset of the entire new content produced in our company during a given year was translated with statistical mt engines trained on data from the previous year. the translation environment recorded translation and postediting times for each sentence. the results show a productivity increase for each participant, with significant variance across inviduals.	-3	118
a novel string-to-string distance measure with applications to machine translation evaluation	we introduce a stringtostring distance measure which extends the edit distance by block transpositions as constant cost edit operation. an algorithm for the calculation of this distance measure in polynomial time is presented. we then demonstrate how this distance measure can be used as an evaluation criterion in machine translation. the correlation between this evaluation criterion and human judgment is systematically compared with that of other automatic evaluation measures on two translation tasks. in general, like other automatic evaluation measures, the criterion shows low correlation at sentence level, but good correlation at system level.	-10	161
word reordering and a dynamic programming beam search algorithm for statistical machine translation	summary in this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (dp). the search algorithm uses the translation model presented in [{\it p. f. brown}, {\it s. a. della pietra}, {\it v. j. della pietra} and {\it r. l. mercer}, 鈥淭he mathematics of statistical machine translation parameter estimation, ibid. 19, no. 2, 263鈥311 (1993)]. starting from a dpbased solution to the travelingsalesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. word reordering restrictions especially useful for the translation direction german to english are presented. the restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. the beam search procedure has been successfully tested on the verbmobil task (german to english, 8,000word vocabulary) and on the canadian hansards task (french to english, 100,000word vocabulary).	-10	164
the candide system for machine translation	we present an overview of candide, a system for automatic translation of french text to english text. candide uses methods of information theory and statistics to develop a probability model of the translation process. this model, which is made to accord as closely as possible with a large body of french and english sentence pairs, is then used to generate english translations of previously unseen french sentences. this paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 1. introduction candide is an experimental computer program, now in its fifth year of development at ibm, for translation of french text to english text. our goal is to perform fullyautomatic, highquality texttotext translation. however, because we are still far from achieving this goal, the program can be used in both fullyautomatic and translator'sassistant modes. our approach is founded upon the statistical analysis of language....	-19	179
word-sense disambiguation for machine translation	in word sense disambiguation, a system attempts to determine the sense of a word from contextual fea tures. major barriers to building a highperforming word sense disambiguation system include the dif ficulty of labeling dat...	-8	144
re-examining machine translation metrics for paraphrase identification	we propose to reexamine the hypothesis that automated metrics developed for mt evaluation can prove useful for paraphrase identification in light of the significant work on the development of new mt metrics over the last 4 years. we show that a metaclassifier trained using nothing but recent mt metrics outperforms all previous paraphrase identification approaches on the microsoft research paraphrase corpus. in addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on mt metrics. we release both the new dataset and the error analysis annotations for use by the community.	-1	91
translation engines: techniques for machine translation	machine translation (mt) is the area of computer science and applied linguistics dealing with the translation of human languages such as english and german. mt on the internet has become an important tool by providing fast, economical and useful translations. with globalisation and expanding trade, demand for translation is set to grow. translation engines covers theoretical and practical aspects of mt, both classic and new, including  character sets and formatting languages  translation memory  linguistic and computational foundations  basic computational linguistic techniques  transfer and interlingua mt  evaluation software accompanies the text, providing readers with hands on experience of the main algorithms.	-14	167
following directions using statistical machine translation	mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language. in this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and a map of an environment built by a robot. our approach uses training data to learn to translate from natural language instructions to an automaticallylabeled map. the complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map. as a result, our technique can efficiently handle uncertainty in both map labeling and parsing. our experiments demonstrate the promising capabilities achieved by our approach.	-3	108
word sense disambiguation vs. statistical machine translation	we directly investigate a subject of much recent debate do word sense disambiga tion models help statistical machine trans lation quality? we present empirical re sults casting doubt on this common, but unproved, assumption. using a stateof theart chinese word sense disambigua tion model to choose translation candi dates for a typical ibm statistical mt system, we find that word sense disam biguation does not yield significantly bet ter translation quality than the statistical machine translation system alone. error analysis suggests several key factors be hind this surprising finding, including in herent limitations of current statistical mt architectures.	-8	149
lattice minimum bayes-risk decoding for statistical machine translation	we present minimum bayesrisk (mbr) decoding for statistical machine translation. this statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, wordtoword alignments from an mt system, and syntactic structure from parsetrees of source and target language sentences. we report the performance of the mbr decoders on a chinesetoenglish translation task. our results show that mbr decoding can be used to tune statistical mt performance for specific loss functions.	-5	127
reordering constraints for phrase-based statistical machine translation	in statistical machine translation, the generation of a translation hypothesis is computationally expensive. if arbitrary reorderings are permitted, the search problem is nphard. on the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomialtime search algorithm. we investigate different reordering constraints for phrasebased statistical machine translation, namely the ibm constraints and the itg constraints. we present efficient dynamic programming algorithms for both constraints. we evaluate the constraints with respect to translation quality on two japanese鈥揈nglish tasks. we show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. the itg constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search.	-9	149
a comparative study on reordering constraints in statistical machine translation	in statistical machine translation, the generation of a translation hypothesis is computationally expensive. if arbitrary wordreorderings are permitted, the search problem is nphard. on the other hand, if we restrict the possible wordreorderings in an appropriate way, we obtain a polynomialtime search algorithm.in this paper, we compare two different reordering constraints, namely the itg constraints and the ibm constraints. this comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. we show a connection between the itg constraints and the since 1870 known schröder numbers.we evaluate these constraints on two tasks the verbmobil task and the canadian hansards task. the evaluation consists of two parts first, we check how many of the viterbi alignments of the training corpus satisfy each of these constraints. second, we restrict the search to each of these constraints and compare the resulting translation hypotheses.the experiments will show that the baseline itg constraints are not sufficient on the canadian hansards task. therefore, we present an extension to the itg constraints. these extended itg constraints increase the alignment coverage from about 87% to 96%.	-10	157
machine translation evaluation versus quality estimation	most evaluation metrics for machine translation (mt) require reference translations for each sentence in order to produce a score reflecting certain aspects of its quality. the de facto metrics, bleu and nist, are known to have good correlation with human evaluation at the corpus level, but this is not the case at the segment level. as an attempt to overcome these two limitations, we address the problem of evaluating the quality of mt as a prediction task, where referenceindependent features are extracted from the input sentences and their translation, and a quality score is obtained based on models produced from training data. we show that this approach yields better correlation with human evaluation as compared to commonly used metrics, even with models trained on different mt systems, languagepairs and text domains.	-3	84
machine translation with a stochastic grammatical channel	we introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation. as with the pure statistical translation model described by wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a languageindependent inversiontransduction model. however, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. the model employs only (1) a translation lexicon, (2) a contextfree grammar for the target language, and (3) a bigram language model. the fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. initial experiments show that it also achieves significant speed gains over our earlier model.	-15	131
multi-engine machine translation guided by explicit word matching	we describe a new approach for synthetically combining the output of several different machine translation (mt) engines operating on the same input. the goal is to pro duce a synthetic combination that surpasses all of the original systems in translation quality. our approach uses the individual mt engines as black boxes and does not require any explicit cooperation from the original mt systems. an explicit word matcher is first used in order to identify the words that are common between the mt engine outputs. a decoding algorithm then uses this information, in conjunction with confidence estimates for the vari ous engines and a trigram language model in order to score and rank a collection of sen tence hypotheses that are synthetic combinations of words from the various original en gines. the highest scoring sentence hypothesis is selected as the final output of our system. experiments conducted using three chinesetoenglish online translation systems demon strate that our multiengine combination system provides an improvement of about 6% over the best original system, and is about equal in translation quality to an oracle capable of selecting the best of the original systems on a sentencebysentence basis.	-8	108
machine translation by triangulation: making effective use of multi-parallel corpora	current phrasebased smt systems perform poorly when using small training sets. this is a consequence of unreliable translation es timates and low coverage over source and target phrases. this paper presents a method which alleviates this problem by exploit ing multiple translations of the same source phrase. central to our approach is triangula tion, the process of translating from a source to a target language via an intermediate third language. this allows the use of a much wider range of parallel corpora for train ing, and can be combined with a standard phrasetable using conventional smoothing methods. experimental results demonstrate bleu improvements for triangulated mod els over a standard phrasebased system.	-6	103
bilingual framenet dictionaries for machine translation	this paper describes issues surrounding the planning and design of germanframenet (gfn), a counterpart to the englishbased framenet project. the goals of gfn are (a) to create lexical entries for german nouns, verbs, and adjectives that correspond to existing framenet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. gfn will take a finegrained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. the parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation.	-11	119
statistical machine translation with word- and sentence-aligned parallel corpora.	the parameters of statistical translation models are typically estimated from sentencealigned parallel corpora. we show that significant improvements in the alignment and translation quality of such models can be achieved by additionall...	-9	115
applying morphology generation models to machine translation	we improve the quality of statistical machine translation (smt) by applying models that predict word forms from their stems using extensive morphological and syntactic infor mation from both the source and target lan guages. our inflection generation models are trained independently of the smt system. we investigate different ways of combining the in flection prediction component with the smt system by training the base mt system on fully inflected forms or on word stems. we applied our inflection generation models in translating english into two morphologically complex languages, russian and arabic, and show that our model improves the quality of smt over both phrasal and syntaxbased smt systems according to bleu and human judge ments.	-5	94
a probabilistic approach to syntax-based reordering for statistical machine translation	inspired by previous preprocessing ap proaches to smt, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrasebased smt. given a source sentence and its parse tree, our method generates, by tree operations, an nbest list of re ordered inputs, which are then fed to stan dard phrasebased decoder to produce the optimal translation. experiments show that, for the nist mt05 task of chineseto english translation, the proposal leads to bleu improvement of 1.56%.	-6	103
word-dependent transition models in hmm based word alignment for statistical machine translation	a word alignment modeler uses probabilistic learning techniques to train “worddependent transition models” for use in constructing phrase level hidden markov model (hmm) based word alignment models. as defined herein, “worddependent transition models” provide a probabilistic model wherein for each source word in training data, a selftransition probability is modeled in combination with a probability of jumping from that particular word to a different word, thereby providing a full transition model for each word in a source phrase. hmm based word alignment models are then used for various word alignment and machine translation tasks. in additional embodiments sparse data problems (i.e., rarely used words) are addressed by using probabilistic learning techniques to estimate worddependent transition model parameters by maximum a posteriori (map) training.	-2	75
machine translation system	a machine translation system includes a database for storing various information, database management section for performing database management, a bilingual correspondence data record subsystem for performing recording/learning processing of translation examples, a translation subsystem for performing translation processing, and dictionary management utilities for performing dictionary management and database transmission/reception processing. the bilingual correspondence data recording section records english and japanese bilingual correspondences by using english and japanese sentences stored in the same file or different files. the recorded bilingual correspondences are linked in units of parts by a bilingual correspondence learning section. in performing translation, an englishtojapanese translation section and the like generate a translation of an original sentence as a translation target by using parts and the like which have undergone learning/recording processing. the dictionary management utilities perform database transmission/reception processing to/from another machine translation system.	-16	99
robust machine translation evaluation with entailment features	existing evaluation metrics for machine translation lack crucial robustness their correlations with hu man quality judgments vary considerably across lan guages and genres. we believe that the main reason is their inability to properly capturemeaning a good translation candidate means the same thing as the reference translation, regardless of formulation. we propose a metric that evaluates mt output based on a rich set of features motivated by textual entailment, such as lexicalsemantic (in)compatibility and ar gument structure overlap. we compare this metric against a combination metric of four stateofthe art scores (bleu, nist, ter, and meteor) in two different settings. the combination metric out performs the individual scores, but is bested by the entailmentbased metric. combining the entailment and traditional features yields further improvements.	-4	66
platform-independent automated machine translation system	a client computer program designed to run in operating systems with various primary character sets is provided. the client computer program further allows the user to choose a preferred destination for translated documents. for example, the translation can be appended to the original document, written into a new document, sent to the clipboard, or turned out to any other supported destination according to the specifications of the user. in addition, the client computer program provides a translation job queue window that lists documents awaiting analysis and transmission for translation. through the translation job queue window, the user can dynamically add to and delete from the translation job queue, as well as suspend or reorder jobs, and open completed files directly from the list. furthermore, the user can choose to display either the requested translation alone or the requested translation together with the untranslated email message in the same window. finally, the client computer program interacts with an email program so that the user can initiate translation directly from the email program's user interface.	-10	88
syntactic preprocessing for statistical machine translation	we describe an approach to automatic sourcelanguage syntactic preprocessing in the context of arabicenglish phrasebased machine translation. sourcelanguage labeled dependencies, that are word aligned with target language words in a parallel corpus, are used to automatically extract syntactic reordering rules in the same spirit of xia and mccord (2004) and zhang et al. (2007). the extracted rules are used to reorder the sourcelanguage side of the training and test data. our results show that when using monotonic decoding and translations for unigram sourcelanguage phrases only, sourcelanguage reordering gives very significant gains over no reordering (25% relative increase in bleu score). with decoder distortion turned on and with access to all phrase translations, the differences in bleu scores are diminished. however, an analysis of sentencelevel bleu scores shows reordering outperforms noreordering in over 40% of the sentences. these results suggest that the approach holds big promise but much more work on arabic parsing may be needed.	-6	75
phrase-based backoff models for machine translation of highly inflected languages	we propose a backoff model for phrase based machine translation that translates unseen word forms in foreignlanguage text by hierarchical morphological ab stractions at the word and the phrase level. the model is evaluated on the europarl corpus for germanenglish and finnish english translation and shows improve ments over stateoftheart phrasebased models.	-7	79
using multiple edit distances to automatically rank machine translation output	this paper addresses the challenging problem of automatically evaluating output from machine translation (mt) systems in order to support the developers of these systems. conventional approaches to the problem include methods that automatically assign a rank such as a, b, c, or d to mt output according to a single edit distance between this output and a correct translation example. the single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. this inhibits improving accuracy of rank assignment. to overcome this obstacle, this paper proposes an automatic ranking method that, by using multiple edit distances, encodes machinetranslated sentences with a rank assigned by humans into multidimensional vectors from which a classifier of ranks is learned in the form of a decision tree (dt). the proposed method assigns a rank to mt output through the learned dt. the proposed method is evaluated using transcribed texts of real conversations in the travel arrangement domain. experimental results show that the proposed method is more accurate than the singleeditdistancebased ranking methods, in both closed and open tests. moreover, the proposed method could estimate mt quality within 3% error in some cases.	-12	92
towards the use of word stems and suffixes for statistical machine translation	in this paper we present methods for improving the quality of translation from an inflected language into english by making use of partofspeech tags and word stems and suffixes in the source language. results for translations from spanish and catalan into english are presented on the lcstar trilingual corpus which consists of spontaneously spoken dialogues in the domain of travelling and appointment scheduling. results for translation from serbian into english are presented on the assimil language course, the bilingual corpus from unrestricted domain. we achieve up to 5% relative reduction of error rates for spanish and catalan and about 8% for serbian.	-6	75
cross-lingual ontology mapping --- an investigation of the impact of machine translation	ontologies are at the heart of knowledge management and make use of information that is not only written in english but also in many other natural languages. in order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. this paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in crosslingual ontology mapping scenarios. in particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in english and chinese are presented. based on findings derived from these studies, limitations of this generic approach are discussed. it is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in crosslingual ontology mapping. finally, to address the identified challenges, a semanticoriented crosslingual ontology mapping (socom) framework is proposed and discussed.	-4	66
translation system, translation communication system, machine translation method, and medium embodying program	a translation means determination section of a translation system determines whether translation processing of language data input from an input section is to be performed in an internal translation section or on an external translation server connected by a communication line through a communication control section. in the latter case, it is also determined what processing the translation server is requested to perform. the translation means determination section flexibly switches between internal processing and processing on the translation server by using information such as a language pair to which translation is applied, the communication line state, and a comparison of abilities between the translation system and the translation server. a translation result of the translation section is output from an output section. this configuration allows easy implementation of function extension viewed from a user while minimizing communication cost and overhead.	-7	79
462 machine translation systems for europe	we built 462 machine translation systems for all language pairs of the acquis communautaire corpus. we report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multipivot, multisource) that are possible due to the available systems. 1	-4	65
statistical machine translation. final report	search all the public and authenticated articles in citeulike. include unauthenticated resultstoo (may include spam) enter a search phrase. you can also specify a citeulike article id(123456),. a doi (doi10.1234/12345678). or a pubmed id (pmid12345678).	-14	93
using lexicalized tags for machine translation	lexicalized tree adjoining grammar (ltag) is an attractive formalism for linguistic description mainly because of its extended domain of locality and its factoring recursion out from the domain of local dependencies (joshi, 1984, kroch and joshi, 1985, abeill茅, 1988). ltag's extended domain of locality enables one to localize syntactic dependencies (such as fillergap), as well as semantic dependencies (such as predicatearguments). the aim of this paper is to show that these properties combined with the lexicalized property of ltag are especially attractive for machine translation. the transfer between two languages, such as french and english, can be done by putting directly into correspondence large elementary universe without going through some interlingual representation and without major changes to the source and target grammars. the underlying formalism from the transfer is synchronous tree adjoining grammars (sheiber and schabes [1990]). transfer rules are stated as correspondences between nodes of trees of large domain of locality which are associated with words. we can thus define lexical transfer rules that avoid the defects of a mere wordtoword approach but still benefit from the simplicity and elegance of a lexical approach.	-23	102
target-text mediated interactive machine translation	the use of machine translation as a tool for professional or other highly skilled translators is for the most part currently limited to postediting arrangements in which the translator invokes mt when desired and then manually cleans up the results. a theoretically promising but hitherto largely unsuccessful alternative to postediting for this application is interactive machine translation (imt), in which the translator and mt system work in tandem. we argue that past failures to make imt viable as a tool for skilled translators have been the result of an infelicitous mode of interaction rather than any inherent flaw in the idea. as a solution, we propose a new style of imt in which the target text under construction serves as the medium of communication between an mt system and its user. we describe the design, implementation, and performance of an automatic word completion system for translators which is intended to demonstrate the feasibility of the proposed approach, albeit in a very rudimentary form.	-16	96
a matching technique in example-based machine translation	this paper addresses an important problem in examplebased machine translation (ebmt), namely how to measure similarity between a sentence fragment and a set of stored examples. a new method is proposed that measures similarity according to both surface structure and content. a second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient. results on a large number of test cases from the celex database are presented.cranias, l; papageorgiou, h; piperidis, s	-19	100
an example-based method for transfer-driven machine translation	this paper presents a method called transferdriven machine translation (tdmt), which utilizes an examplebased framework for various process and combines multilevel knowledge. an examplebased framework can achieve quick processing and consistently describe knowledge. it is useful for spokenlanguage translation, which needs robust and efficient translation. tdmt strengthens the examplebased framework by integrating it with other frameworks. the feasibility of tdmt and the advantages of the examplebased framework have been confirmed with a prototype system, which translates spoken dialog sentences from japanese to english.	-21	102
the use of lexical semantics in interlingual machine translation	this paper describes the lexicalsemantic basis for unitran, an implemented scheme for translating spanish, english, and german bidirectionally. two claims made here are that the current representation handles many distinctions (or divergences ) across languages without recourse to languagespecific rules and that the lexicalsemantic framework provides the basis for a systematic mapping between the interlingua and the syntactic structure. the representation adopted is an extended version of lexical conceptual structure  which is suitable to the task of translating between divergent structures for two reasons (1) it provides an ion  of languageindependent properties from structural idiosyncrasies; and (2) it is compositional  in nature. the lexicalsemantic approach addresses the divergence problem by using a linguistically grounded mapping that has access to parameter settings in the lexicon. we will examine a number of relevant issues including the problem of defining primitives, the issue of interlinguality, the crosslinguistic coverage of the system, and the mapping between the syntactic structure and the interlingua. a detailed example of lexicalsemantic composition will be presented.	-21	102
maxsim: a maximum similarity metric for machine translation evaluation	we propose an automatic machine translation (mt) evaluation metric that calculates a sim ilarity score (based on precision and recall) of a pair of sentences. unlike most metrics, we compute a similarity score between items across the two sentences. we then find a maxi mum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. this general framework allows us to use arbitrary similarity functions between items, and to in corporate different information in our com parison, such as ngrams, dependency rela tions, etc. when evaluated on data from the acl07 mt workshop, our proposed metric achieves higher correlation with human judge ments than all 11 automatic mt evaluation metrics that were evaluated during the work shop.	-5	70
segmentation for english-to-arabic statistical machine translation	in this paper, we report on a set of ini tial results for englishtoarabic statistical machine translation (smt). we show that morphological decomposition of the arabic source is beneficial, especially for smallersize corpora, and investigate different recombina tion techniques. we also report on the use of factored translation models for english toarabic translation.	-5	70
novel reordering approaches in phrase-based statistical machine translation	this paper presents novel approaches to reordering in phrasebased statistical machine translation. we perform consistent reordering of source sentences in training and estimate a statistical translation model. using this model, we follow a phrasebased monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. in translation, we apply source sentence reordering on word level and use a reordering automaton as input. we show how to compute reordering automata ondemand using ibm or itg constraints, and also introduce two new types of reordering constraints. we further add weights to the reordering automata. we present detailed experimental results and show that reordering significantly improves translation quality.	-8	80
machine translation system	methods and apparatus for machine translation are disclosed. in one embodiment of the invention, information is stored in a memory which is contained in a computer, or some other device. the stored information includes a set of eigens for a number of languages, a crosslanguage eigen dictionary, a pattern dictionary, and a crosslanguage pattern dictionary. the first step of the translation is the conversion of a sentence in a first language to an instantiated pattern form. a corresponding pattern is then found in the crosslanguage pattern dictionary. eigens are then found using the crosslanguage eigen dictionary, and a translation in a second language is assembled.	-10	85
the lrc machine translation system: linguistics research center	sourcelanguage text is analyzed according to phrasestructure grammar rules augmented with procedures incorporating syntactic and semantic restrictions, surface and deep case analysis, and transformations; transformed to targetlanguage structures and lexical items representing an equivalent utterance; then synthesized into targetlanguage text.	-31	106
example-based machine translation using dp-matching between word sequences	summary we propose a new approach to the examplebased machine translation paradigm. first, the proposed approach retrieves the most similar example by carrying out dpmatching of the input sentence and source sentences in an example database while measuring the semantic distances of the words. second, the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary. we demonstrate its high coverage and accuracy through a computational experiment for a limited domain.	-12	88
machine translation system incorporating syntactic dependency treelets into a statistical framework	in one embodiment of the present invention, a decoder receives a dependency tree as a source language input and accesses a set of statistical models that produce outputs combined in a log linear framework. the decoder also accesses a table of treelet translation pairs and returns a target dependency tree based on the source dependency tree, based on access to the table of treelet translation pairs, and based on the application of the statistical models.	-3	59
shallow parsing for portuguese--spanish machine translation	to produce fast, reasonably intelligible and easily correctable translations between related languages, it suffices to use a machine translation strategy which uses shallow parsing techniques to refine what would usually be called wordforword machine translation. this paper describes the application of shallow parsing techniques (morphological analysis, lexical disambiguation, and flat, local parsing) in a portuguese–spanish, spanish–portuguese machine translation system which is currently being developed by our group and is publicly and freely available at http//copacabana.dlsi.ua.es.	-10	84
can crowds build parallel corpora for machine translation systems?	corpus based approaches to machine translation (mt) rely on the availability of parallel corpora. in this paper we explore the effectiveness of mechanical turk for creating parallel corpora. we explore the task of sentence translation, both into and out of a language. we also perform preliminary experiments for the task of phrase translation, where ambiguous phrases are provided to the turker for translation in isolation and in the context of the sentence it originated from.	-3	59
multiple-parts-of-speech disambiguating method and apparatus for machine translation system	a machine translation system comprises input means for inputting a sentence written in a natural language, processor for parsing the input sentence, a word dictionary memory referred to by the processor, and a memory for storing multiplepartsofspeech disambiguating rules in the form of a table. the parts of speech of words capable of functioning as multiple parts of speech should be in the inputted sentence are determined in consideration of an array of the parts of speech by applying the multiplepartsofspeech disambiguating rules. additionally, rate of appearance of each part of speech which the word of the input sentence can function as is previously calculated, and the part of speech which can not be determined by consulting the disambiguating rule table is determined in dependence on whether the rate of appearance exceeds a predetermined threshold value.	-26	102
cross-lingual ontology mapping – an investigation of the impact of machine translation	ontologies are at the heart of knowledge management and make use of information that is not only written in english but also in many other natural languages. in order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. this paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in crosslingual ontology mapping scenarios. in particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in english and chinese are presented. based on findings derived from these studies, limitations of this generic approach are discussed. it is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in crosslingual ontology mapping. finally, to address the identified challenges, a semanticoriented crosslingual ontology mapping (socom) framework is proposed and discussed.	-4	64
dialectal to standard arabic paraphrasing to improve arabic-english statistical machine translation	this paper is interested in improving the quality of arabicenglish statistical machine translation (smt) on highly dialectal arabic text using morphological knowledge. we present a lightweight rulebased approach to producing modern standard arabic (msa) paraphrases of dialectal arabic outofvocabulary words and low frequency words. our approach extends an existing msa analyzer with a small number of morphological clitics and transfer rules. the generated paraphrase lattices are input to a stateoftheart phrasebased smt system resulting in improved bleu scores on a blind test set by 0.56 absolute bleu (or 1.5% relative).	-2	54
incremental hypothesis alignment for building confusion networks with application to machine translation system combination	confusion network decoding has been the most successful approach in combining out puts from multiple machine translation (mt) systems in the recent darpa gale and nist open mt evaluations. due to the vary ing word order between outputs from differ ent mt systems, the hypothesis alignment presents the biggest challenge in confusion network decoding. this paper describes an incremental alignment method to build confu sion networks based on the translation edit rate (ter) algorithm. this new algorithm yields significant bleu score improvements over other recent alignment methods on the gale test sets and was used in bbn's submission to the wmt08 shared translation task.	-5	68
syntax-based language models for machine translation	reacttext 435 we propose and evaluate computational techniques for deciphering unknown scripts. we focus on the case in which an unfamiliar script encodes a known language. the decipherment of a brief document or inscription is driven by data about the spoken language. we consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language...  /reacttext  reacttext 436   /reacttext [show full ]	-10	83
displaying and correcting method for machine translation system	in a system wherein a first text in a first natural language is translated into a second text in a second natural language; a text displaying and correcting system comprising a first memory area for storing parts of the first text divided in predetermined units, with identifications assigned to the respective parts, and a second memory area for storing predetermined units of the second text corresponding to the aforementioned units of the first text, with the same identifications assigned thereto, so that the first and second texts are simultaneously displayed on a screen of a display unit, and that the text is revised in each unit with identification assigned.	-27	99
online learning for interactive statistical machine translation	stateoftheart machine translation (mt) systems are still far from being perfect. an alternative is the socalled interactive machine translation (imt) framework. in this framework, the knowledge of a human translator is combined with a mt system. the vast majority of the existing work on imt makes use of the wellknown batch learning paradigm. in the batch learning paradigm, the training of the imt system and the interactive translation process are carried out in separate stages. this paradigm is not able to take advantage of the new knowledge produced by the user of the imt system. in this paper, we present an application of the online learning paradigm to the imt framework. in the online learning paradigm, the training and prediction stages are no longer separated. this feature is particularly useful in imt since it allows the user feedback to be taken into account. the online learning techniques proposed here incrementally update the statistical models involved in the translation process. empirical results show the great potential of online learning in the imt framework.	-3	57
e-services translation utilizing machine translation and translation memory	a system and method for translating data from a source language to a target language is provided wherein machine generated target translation of a source sentence is compared to a database of human generated target sentences. if a matching human generated target sentence is found, the human generated target sentence may be used instead of the machine generated sentence, since the human generated target sentence is more likely to be a wellformed sentence than the machine generated sentence. the system and method does not rely on a translation memory containing pairs of sentences in both source and target languages, and minimizes the reliance on a human translator to correct a translation generated by machine translation.	-2	52
translating with examples: a new approach to machine translation	atr interpreting telephony research laboratories sanpeidani, inuidani seikacho, sorakugun kyoto 61902, japan email sumita%atrla.atr.co.jp@uunet.uu.net    this paper proposes examplebased machine translation (ebmt). ebmt retrieves similar examples	-23	100
error detection for statistical machine translation using linguistic features	automatic error detection is desired in the postprocessing to improve machine translation quality. the previous work is largely based on confidence estimation using systembased features, such as word posterior probabilities calculated from nbest lists or word lattices. we propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection lexical and syntactic features. we use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. the experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the f measure by 16.37%.	-3	58
maxsim: a maximum similarity metric for machine translation evaluation	we propose an automatic machine translation (mt) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. unlike most metrics, we compute a similarity score between items across the two sentences. we then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. this general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as ngrams, dependency relations, etc. when evaluated on data from the acl07 mt workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic mt evaluation metrics that were evaluated during the workshop. 漏 2008 association for computational linguistics.	-5	65
stochastic finite-state models for spoken language machine translation	the problem of machine translation can be viewed as consisting of twosubproblems (a) lexical selection and (b) lexical reordering. in thispaper, we propose stochastic finitestate models for these two subproblems. stochastic finitestate models are efficiently learnablefrom data, effective for decoding and are associated with a calculusfor composing models which allows for tight integration of constraintsfrom various levels of language processing. we present a method forlearning stochastic finitestate models for lexical selection andlexical reordering that are trained automatically from pairs of sourceand target utterances. we use this method to develop models forenglish–japanese and english–spanish translation and present the performance of these models for translation on speech and text. we also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.	-11	85
language model adaptation for statistical machine translation based on information retrieval	language modeling is an important part for both speech recognition and machine translation systems. adaptation has been successfully applied to language models for speech recognition. in this paper we present experiments concerning language model adaptation for statistical machine translation. we develop a method to adapt language models using information retrieval methods. the adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.	-9	79
handbook of natural language processing and machine translation: darpa global autonomous language exploitation	this comprehensive handbook, written by leading experts in the field, details the groundbreaking research conducted under the breakthrough gale program  the global autonomous language exploitation within the defense advanced research projects agency (darpa), while placing it in the context of previous research in the fields of natural language and signal processing, artificial intelligence and machine translation. the most fundamental contrast between gale and its predecessor programs was its holistic integration of previously separate or sequential processes. in earlier language research proolive, joseph p; christianson, caitlin; mccary, john	-2	53
machine translation using vector space representations	an embodiment of the present invention provides a method for automatically translating text. first, a conceptual representation space is generated based on sourcelanguage documents and targetlanguage documents, wherein respective terms from the sourcelanguage and targetlanguage documents have a representation in the conceptual representation space. second, a new sourcelanguage document is represented in the conceptual representation space, wherein a subset of terms in the new sourcelanguage document is represented in the conceptual representation space, such that each term in the subset has a representation in the conceptual representation space. then, a term in the new sourcelanguage document is automatically translated into a corresponding targetlanguage term based on a similarity between the representation of the term and the representation of the corresponding targetlanguage term.	-3	56
statistical machine translation gains respect	as business, finance, education, and the internet become increasingly international and multilingual, google and other organizations are investing more time, money, and talent into researching the effectiveness of machinetranslation technologies.	-8	75
generation-heavy hybrid machine translation	this paper describes generationheavy hybrid machine translation (ghmt), a novel approach for trans lating between structurallydivergent language pairs with asymmetrical resources. the approach depends on the existence of rich target language resources such as word lexical semantics, categorial variations and subcategorization frames. these resources are used to overgenerate multiple lexicostructural variations from a targetglossed syntactic dependency representation of the source language sentence. this symbolic overgeneration, which accounts for a wide range of possible variations, is constrained by a statistical targetlanguage model. the exploitation of target language resources (symbolic and statistical) to handle a problem usually reserved for transfer and interlingual mt is useful for translation from source languages with scarce linguistic resources. a preliminary evaluation on the application of this approach to spanishenglish mt is conducted with promising results.	-11	81
a finite-state approach to machine translation	the problem of machine translation can be viewed as consisting of two subproblems (a) lexical selection; (b) lexical reordering. we propose stochastic finitestate models for these two subproblems. stochastic finitestate models are efficiently able to learn from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. we present a method for learning stochastic finitestate models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances. we use this method to develop models for englishjapanese translation and present the performance of these models for translation of speech and text. we also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.	-12	84
the lrc machine translation system: an overview of the linguistic component of metal	the lrc machine translation system an overview of the linguistic component of metaldoi10.3115/990100.990105winfield s. bennettthe university of texas at austinacademia prahaconference on computational linguisticsbennett, w. and slocum, j., the lrc machine translation system, computational linguistics, vol. 11, no. 23, pp. 111121, 1985....	-31	100
cache-based document-level statistical machine translation	statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring documentlevel information. in this paper, we propose a cachebased approach to documentlevel translation. since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highlyrelevant data of a reasonable size. in this paper, we present three kinds of caches to store relevant documentlevel information 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the targetside topic words related with the test document in the sourceside. in particular, three new features are designed to explore various kinds of documentlevel information in above three kinds of caches.	-2	51
purest ever example-based machine translation: detailed presentation and assessment	we have designed, implemented and assessed an ebmt system that can be dubbed the 'purest ever built' it strictly does not make any use of variables, templates or patterns, does not have any explicit transfer component, and does not require any preprocessing or training of the aligned examples. it only uses a specific operation, proportional analogy, that implicitly neutralises divergences between languages and captures lexical and syntactical variations along the paradigmatic and syntagmatic axes without explicitly decomposing sentences into fragments. exactly the same genuine implementation of such a core engine was evaluated on different tasks and language pairs. to begin with, we compared our system on two tasks of a previous mt evaluation campaign to rank it among other current stateoftheart systems. then, we illustrated the 'universality' of our system by participating in a recent mt evaluation campaign, with exactly the same core engine, for a wide variety of language pairs. finally, we studied the in uence of extra data like dictionaries and paraphrases on the system performance.	-6	68
cohesive phrase-based decoding for statistical machine translation	phrasebased decoding produces stateofthe art translations with no regard for syntax. we add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. the constraint allows the decoder to employ arbitrary, nonsyntactic phrases, but ensures that those phrases are translated in an order that respects the source tree's structure. in this way, we target the phrasal decoder's weakness in order model ing, without affecting its strengths. to fur ther increase flexibility, we incorporate cohe sion as a decoder feature, creating a soft con straint. the resulting cohesive, phrasebased decoder is shown to produce translations that are preferred over noncohesive output in both automatic and human evaluations.	-5	64
system and method for machine learning a confidence metric for machine translation	a machine translation system is trained to generate confidence scores indicative of a quality of a translation result. a source string is translated with a machine translator to generate a target string. features indicative of translation operations performed are extracted from the machine translator. a trusted entityassigned translation score is obtained and is indicative of a trusted entityassigned translation quality of the translated string. a relationship between a subset of the extracted features and the trusted entityassigned translation score is identified.	-4	60
neural machine translation of rare words with subword units	neural machine translation (nmt) models typically operate with a fixed vocabulary, but translation is an openvocabulary problem. previous work addresses the translation of outofvocabulary words by backing off to a dictionary. in this paper, we introduce a simpler and more effective approach, making the nmt model capable of openvocabulary translation by encoding rare and unknown words as sequences of subword units. this is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). we discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a backoff dictionary baseline for the wmt 15 translation tasks englishgerman and englishrussian by 1.1 and 1.3 bleu, respectively.	3	169
neural machine translation by jointly learning to align and translate	neural machine translation is a recently proposed approach to machine translation. unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. the models proposed recently for neural machine translation often belong to a family of encoderdecoders and consists of an encoder that encodes a source sentence into a fixedlength vector from which a decoder generates a translation. in this paper, we conjecture that the use of a fixedlength vector is a bottleneck in improving the performance of this basic encoderdecoder architecture, and propose to extend this by allowing a model to automatically (soft)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. with this new approach, we achieve a translation performance comparable to the existing stateoftheart phrasebased system on the task of englishtofrench translation. furthermore, qualitative analysis reveals that the (soft)alignments found by the model agree well with our intuition.	1	1890
on the properties of neural machine translation: encoder-decoder approaches	neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. the neural machine translation models often consist of an encoder and a decoder. the encoder extracts a fixedlength representation from a variablelength input sentence, and the decoder generates a correct translation from this representation. in this paper, we focus on analyzing the properties of the neural machine translation using two models; rnn encoderdecoder and a newly proposed gated recursive convolutional neural network. we show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.	1	467
findings of the 2014 workshop on statistical machine translation	this paper presents the results of the wmt14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for runtime estimation of machine translation quality, and a metrics task. this year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. an additional 6 anonymized systems were included, and were then evaluated both automatically and manually. the quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries	1	391
effective approaches to attention-based neural machine translation	an attentional mechanism has lately been used to improve neural machine translation (nmt) by selectively focusing on parts of the source sentence during translation. however, there has been little work exploring useful architectures for attentionbased nmt. this paper examines two simple and effective classes of attentional mechanism a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. we demonstrate the effectiveness of both approaches over the wmt translation tasks between english and german in both directions. with local attention, we achieve a significant gain of 5.0 bleu points over nonattentional systems which already incorporate known techniques such as dropout. our ensemble model using different attention architectures has established a new stateoftheart result in the wmt'15 english to german translation task with 25.9 bleu points, an improvement of 1.0 bleu points over the existing best system backed by nmt and an ngram reranker.	2	301
google's neural machine translation system: bridging the gap between human and machine translation	neural machine translation (nmt) is an endtoend learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrasebased translation systems. unfortunately, nmt systems are known to be computationally expensive both in training and in translation inference. also, most nmt systems have difficulty with rare words. these issues have hindered nmt's use in practical deployments and services, where both accuracy and speed are essential. in this work, we present gnmt, google's neural machine translation system, which attempts to address many of these issues. our model consists of a deep lstm network with 8 encoder and 8 decoder layers using attention and residual connections. to improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. to accelerate the final translation speed, we employ lowprecision arithmetic during inference computations. to improve handling of rare words, we divide words into a limited set of common subword units (wordpieces) for both input and output. this method provides a good balance between the flexibility of characterdelimited models and the efficiency of worddelimited models.	3	223
on using very large target vocabulary for neural machine translation	neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. in this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. we show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. the models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the lstmbased neural machine translation models. furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the stateoftheart translation performance (measured by bleu) on the english>german translation and almost as high performance as stateoftheart english>french translation system.	1	188
fast and robust neural network joint models for statistical machine translation	proceedings of the 52nd annual meeting of the association for computational linguistics, pages 1370–1380, baltimore, maryland, usa, june 2325 2014. co2014 association for computational linguisticsfast and robust neural network joint models for statistical machine translationjacob devlin, rabih zbib, zhongqiang huang, thomas lamar, richard schwartz, and john makhoul raytheon bbn technologies, 10 moulton st, cambridge, ma 02138, usa {jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com recent work	1	234
learning phrase representations using rnn encoder-decoder for statistical machine translation	in this paper, we propose a novel neural network model called rnn encoderdecoder that consists of two recurrent neural networks (rnn). one rnn encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. the encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. the performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the rnn encoderdecoder as an additional feature in the existing loglinear model. qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.	1	1174
modeling coverage for neural machine translation	attention mechanism advanced stateoftheart neural machine translation (nmt) by jointly learning to align and translate. however, attentionbased nmt ignores past alignment information, which often leads to overtranslation and undertranslation. in response to this problem, we maintain a coverage vector to keep track of the attention history. the coverage vector is fed to the attention model to help adjust future attention, which guides nmt to consider more about the untranslated source words. experiments show that the proposed approach significantly improves both translation quality and alignment quality over traditional attentionbased nmt.	3	53
minimum risk training for neural machine translation	we propose minimum risk training for endtoend neural machine translation. unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. experiments show that our approach achieves significant improvements over maximum likelihood estimation on a stateoftheart neural machine translation system across various languages pairs. transparent to architectures, our approach can be applied to more neural networks and potentially benefit more nlp tasks.	2	70
findings of the 2016 conference on machine translation	this paper presents the results of the wmt16 shared tasks, which included five machine translation (mt) tasks (standard news, itdomain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, runtime estimation of mt qual ity), and an automatic postediting task and bilingual document alignment task. this year, 102 mt systems from 24 in stitutions (plus 36 anonymized online sys tems) were submitted to the 12 translation directions in the news translation task. the itdomain task received 31 submissions from 12 institutions in 7 directions and the biomedical task received 15 submissions systems from 5 institutions. evaluation was both automatic and manual (relative ranking and 100point scale assessments).	3	60
addressing the rare word problem in neural machine translation	neural machine translation (nmt) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. a significant weakness in conventional nmt systems is their inability to correctly translate very rare words endtoend nmts tend to have relatively small vocabularies with a single unk symbol that represents every possible outofvocabulary (oov) word. in this paper, we propose and implement an effective technique to address this problem. we train an nmt system on data that is augmented by the output of a word alignment algorithm, allowing the nmt system to emit, for each oov word in the target sentence, the position of its corresponding word in the source sentence. this information is later utilized in a postprocessing step that translates every oov word using a dictionary. our experiments on the wmt14 english to french translation task show that this method provides a substantial improvement of up to 2.8 bleu points over an equivalent nmt system that does not use this technique. with 37.5 bleu points, our nmt system is the first to surpass the best result achieved on a wmt14 contest task.	1	156
findings of the 2015 workshop on statistical machine translation	this paper presents the results of the wmt15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for runtime estimation of machine translation quality, and an automatic postediting task. this year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. an additional 7 anonymized systems were included, and were then evaluated both automatically and manually. the quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. the pilot automatic post editing task had a total of 4 teams, submitting 7 entries.	2	133
optimizing chinese word segmentation for machine translation performance	previous work has shown that chinese word segmentation is useful for machine translation to english, yet the way different segmentation strategies affect mt is still poorly understood. in this paper, we demonstrate that optimizing segment.	3	56
multi-way, multilingual neural machine translation with a shared attention mechanism	we propose multiway, multilingual neural machine translation. the proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. this is made possible by having a single attention mechanism that is shared across all language pairs. we train the proposed multiway, multilingual model on ten language pairs from wmt'15 simultaneously and observe clear performance improvements over models trained on only one language pair. in particular, we observe that the proposed model significantly improves the translation quality of lowresource language pairs.	3	60
improved statistical machine translation for resource-poor languages using related resource-rich languages	we propose a novel languageindependent approach for improving machine translation for resourcepoor languages by exploiting their similarity to resourcerich ones. more precisely, we improve the translation from a resourcepoor source language x_1 into a resourcerich language y given a bitext containing a limited number of parallel sentences for x_1y and a larger bitext for x_2y for some resourcerich language x_2 that is closely related to x_1. this is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages x_1 and x_2 in spelling, word order, and syntax offer (1) we improve the word alignments for the resourcepoor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. the evaluation for indonesian >english using malay and for spanish > english using portuguese and pretending spanish is resourcepoor shows an absolute gain of up to 1.35 and 3.37 bleu points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. overall, our method cuts the amount of necessary real training data by a factor of 25.	1	82
a character-level decoder without explicit segmentation for neural machine translation	the existing machine translation systems, whether phrasebased or neural, have relied almost exclusively on wordlevel modelling with explicit segmentation. in this paper, we ask a fundamental question can neural machine translation generate a character sequence without any explicit segmentation? to answer this question, we evaluate an attentionbased encoderdecoder with a subwordlevel encoder and a characterlevel decoder on four language pairsencs, ende, enru and enfi using the parallel corpora from wmt'15. our experiments show that the models with a characterlevel decoder outperform the ones with a subwordlevel decoder on all of the four language pairs. furthermore, the ensembles of neural models with a characterlevel decoder outperform the stateoftheart nonneural machine translation systems on encs, ende and enfi and perform comparably on enru.	3	86
a character-level decoder without explicit segmentation for neural machine translation	the existing machine translation systems, whether phrasebased or neural, have relied almost exclusively on wordlevel modelling with explicit segmentation. in this paper, we ask a fundamental question can neural machine translation generate a character sequence without any explicit segmentation? to answer this question, we evaluate an attentionbased encoderdecoder with a subwordlevel encoder and a characterlevel decoder on four language pairsencs, ende, enru and enfi using the parallel corpora from wmt'15. our experiments show that the models with a characterlevel decoder outperform the ones with a subwordlevel decoder on all of the four language pairs. furthermore, the ensembles of neural models with a characterlevel decoder outperform the stateoftheart nonneural machine translation systems on encs, ende and enfi and perform comparably on enru.	3	55
is machine translation ready yet?	the default option of the google translator toolkit (gtt), released in june 2009, is to prefill with machine translation all segments for which a 'no match' has been returned by the memories, while the window clearly advises that most users should not modify this. to confirm whether this approach indeed benefits translators and translation quality, we designed and performed tests whereby trainee translators used the gtt to translate passages from english into chinese either entirely from the source text, or after seeding of empty segments by the google translate engine as recommended. the translations were timed, and their quality assessed by independent experienced markers following australian naati test criteria. our results show that, while time differences were not significant, the machine translation seeded passages were more favourably assessed by the markers in thirty three of fifty six cases. this indicates that, at least for certain tasks and language combinations — and against the received wisdom of translation professionals and translator trainers — translating by proofreading machine translation may be advantageous.	3	43
neural machine translation in linear time	we present a novel neural network for processing sequences. the bytenet is a onedimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. the two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. to address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. the bytenet uses dilation in the convolutional layers to increase its receptive field. the resulting network has two core properties it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. the bytenet decoder attains stateoftheart performance on characterlevel language modelling and outperforms the previous best results obtained with recurrent networks. the bytenet also achieves stateoftheart performance on charactertocharacter machine translation on the englishtogerman wmt translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. we find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.	3	41
character-based neural machine translation	we introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. since wordlevel information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. in the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. as the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. a secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. we show that our model can achieve translation results that are on par with conventional wordbased models.	2	70
character-based neural machine translation	neural machine translation (mt) has reached stateoftheart results. however, one of the main challenges that neural mt still faces is dealing with very large vocabularies and morphologically rich languages. in this paper, we propose a neural mt system using characterbased embeddings in combination with convolutional and highway layers to replace the standard lookupbased word representations. the resulting unlimitedvocabulary and affixaware source word embeddings are tested in a stateoftheart neural mt based on an attentionbased bidirectional recurrent neural network. the proposed mt scheme provides improved results even when the source language is not morphologically rich. improvements up to 3 bleu points are obtained in the germanenglish wmt task.	3	47
edinburgh neural machine translation systems for wmt 16	we participated in the wmt 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions english2czech, english2german, english2romanian and english2russian. our systems are based on an attentional encoderdecoder, using bpe subword segmentation for openvocabulary translation with a fixed vocabulary. we experimented with using automatic backtranslations of the monolingual news corpus as additional training data, pervasive dropout, and targetbidirectional models. all reported methods give substantial improvements, and we see improvements of 4.311.2 bleu over our baseline systems. in the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.	3	40
on using monolingual corpora in neural machine translation	recent work on endtoend neural networkbased architectures for machine translation has shown promising results for enfr and ende translation. arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. in this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. compared to a phrasebased and hierarchical baseline, we obtain up to $1.96$ bleu improvement on the lowresource language pair turkishenglish, and $1.59$ bleu on the focused domain task of chineseenglish chat messages. while our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as csen and deen where we obtain an improvement of $0.39$ and $0.47$ bleu scores over the neural machine translation baselines, respectively.	2	58
google's multilingual neural machine translation system: enabling zero-shot translation	we propose a simple solution to use a single neural machine translation (nmt) model to translate between multiple languages. our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. the rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. using a shared wordpiece vocabulary, our approach enables multilingual nmt using a single model without any increase in parameters, which is significantly simpler than previous proposals for multilingual nmt. our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. on the wmt'14 benchmarks, a single multilingual model achieves comparable performance for english$\rightarrow$french and surpasses stateoftheart results for english$\rightarrow$german. similarly, a single multilingual model surpasses stateoftheart results for french$\rightarrow$english and german$\rightarrow$english on wmt'14 and wmt'15 benchmarks respectively.	3	49
achieving open vocabulary neural machine translation with hybrid word-character models	nearly all previous work in neural machine translation (nmt) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. this paper presents a novel wordcharacter solution to achieving open vocabulary nmt. we build hybrid systems that translate mostly at the word level and consult the character components for rare words. our characterlevel recurrent neural networks compute source word representations and recover unknown target words when needed. the twofold advantage of such a hybrid approach is that it is much faster and easier to train than characterbased ones; at the same time, it never produces unknown words as in the case of wordbased models. on the wmt'15 english to czech translation task, this hybrid approach offers a boost of up to +7.9 bleu points over models that do not handle unknown words. our best hybrid system has established a new stateoftheart result with 19.9 bleu score. we demonstrate that our character models can successfully learn to not only generate wellformed words for czech, a highlyinflected language with a very complex vocabulary, but also build correct representations for english source words.	3	51
character-based neural machine translation	neural machine translation (mt) has reached stateoftheart results. however, one of the main challenges that neural mt still faces is dealing with very large vocabularies and morphologically rich languages. in this paper, we propose a neural mt system using characterbased embeddings in combination with convolutional and highway layers to replace the standard lookupbased word representations. the resulting unlimitedvocabulary and affix aware source word embeddings are tested in a stateoftheart neural mt based on an attentionbased bidirectional recurrent neural network. the proposed mt scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. the number of target words is still limited by the standard wordbased softmax output layer. however the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrasebased baselines. improvements up to 3 bleu points are obtained in the germanenglish wmt task.	3	33
improving neural machine translation models with monolingual data	neural machine translation (nmt) has obtained stateofthe art performance for several language pairs, while only using parallel data for training. targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation, and we investigate the use of monolingual data for nmt. in contrast to previous work, which combines nmt models with separately trained language models, we note that encoderdecoder nmt architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. by pairing monolingual training data with an automatic backtranslation, we can treat it as additional parallel training data, and we obtain substantial improvements on the wmt 15 task englishgerman (+2.83.7 bleu), and for the lowresourced iwslt 14 task turkish>english (+2.13.4 bleu), obtaining new stateoftheart results. we also show that finetuning on indomain monolingual and parallel data gives substantial improvements for the iwslt 15 task english>german.	2	53
minimum risk training for neural machine translation	we propose minimum risk training for endtoend neural machine translation. unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. experiments on chineseenglish and englishfrench translation show that our approach achieves significant improvements over maximum likelihood estimation on a stateoftheart neural machine translation system.	3	39
computerized statistical machine translation with phrasal decoder	a computerized system for performing statistical machine translation with a phrasal decoder is provided. the system may include a phrasal decoder trained prior to runtime on a monolingual parallel corpus, the monolingual parallel corpus including a machine translation output of source language documents of a bilingual parallel corpus and a corresponding target human translation output of the source language documents, to thereby learn mappings between the machine translation output and the target human translation output. the system may further include a statistical machine translation engine configured to receive a translation input and to produce a raw machine translation output, at runtime. the phrasal decoder may be configured to process the raw machine translation output, and to produce a corrected translation output based on the learned mappings for display on a display associated with the system.	2	44
neural versus phrase-based machine translation quality: a case study	within the field of statistical machine translation (smt), the neural approach (nmt) has recently emerged as the first technology able to challenge the longstanding dominance of phrasebased approaches (pbmt). in particular, at the iwslt 2015 evaluation campaign, nmt outperformed well established stateoftheart pbmt systems on englishgerman, a language pair known to be particularly hard because of morphology and syntactic differences. to understand in what respects nmt provides better translation quality than pbmt, we perform a detailed analysis of neural versus phrasebased smt outputs, leveraging high quality postedits performed by professional translators on the iwslt data. for the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models  such as the reordering of verbs  while pointing out other aspects that remain to be improved.	3	37
neural versus phrase-based machine translation quality: a case study	reacttext 270 word reordering is one of the most difficult aspects of statistical machine translation (smt), and an important factor of its quality and efficiency. despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. instead, the choice of the optimal approach for a...  /reacttext  reacttext 271   /reacttext [show full ]	3	34
knowledge-based question answering as machine translation	a typical knowledgebased question answering (kbqa) system faces two challenges one is to transform natural language questions into their meaning representations (mrs); the other is to retrieve answers from knowledge bases (kbs) using generated mrs. unlike previous methods which treat them in a cascaded manner, we present a translationbased approach to solve these two tasks in one unified framework. we translate questions to answers based on cyk parsing. answers as translations of the span covered by each cyk cell are obtained by a question translation method, which first generates formal triple queries as mrs for the span based on question patterns and relation expressions, and then retrieves answers from a given kb based on triple queries generated. a linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of questionanswer pairs. compared to a kbqa system using a stateoftheart semantic parser, our method achieves better results.	1	49
bilingually-constrained phrase embeddings for machine translation	we propose bilinguallyconstrained recursive autoencoders (brae) to learn phrase embeddings (compact vector representations for phrases), which can distinguish the phrases in different semantic meanings. the brae is trained with the objective to minimize the semantic distance of translation equivalents and maximize the semantic distance of nontranslation pairs. the learned model can embed any phrase semantically in two languages and can transform semantic space in one language to the other. we evaluate the brae on two endtoend smt tasks (phrase table pruning and translation hypotheses reranking) which need to measure semantic similarity between a source phrase and its translation candidates. extensive experiments show that the brae is spectacularly successful in these two tasks.	1	58
comparative experiments using supervised learning and machine translation for multilingual sentiment analysis	sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts. in recent years, due to the growth in the quantity and fast spreading of usergenerated contents online and the impact such information has on events, people and companies worldwide, this task has been approached in an important body of research in the field. despite different methods having been proposed for distinct types of text, the research community has concentrated less on developing methods for languages other than english. in the abovementioned context, the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less/no resources are available for this task when compared to english, stressing upon the impact of translation quality on the sentiment classification performance. our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can, in combination to appropriate machine learning algorithms and carefully chosen features, be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for english.	1	48
dual learning for machine translation	while neural machine translation (nmt) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. however, human labeling is very costly. to tackle this training data bottleneck, we develop a duallearning mechanism, which can enable an nmt system to automatically learn from unlabeled data through a duallearning game. this mechanism is inspired by the following observation any machine translation task has a dual task, e.g., englishtofrench translation (primal) versus frenchtoenglish translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. in the duallearning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). we call the corresponding approach to neural machine translation \emph{dualnmt}.	3	31
lium smt machine translation system for wmt 2010	this paper describes the development of frenchenglish and englishfrench machine translation systems for the 2010 wmt shared task evaluation. these systems were standard phrasebased statistical systems based on the moses decoder, trained on the provided data only. most of our efforts were devoted to the choice and extraction of bilingual data used for training. we filtered out some bilingual corpora and pruned the phrase table. we also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data. we first collected bilingual data by performing automatic translations of monolingual texts. the second type of bilingual text was harvested from comparable corpora with information retrieval techniques.	4	29
nematus: a toolkit for neural machine translation	we present nematus, a toolkit for neural machine translation. the toolkit prioritizes high translation accuracy, usability, and extensibility. nematus has been used to build topperforming submissions to shared translation tasks at wmt and iwslt, and has been used to train systems for production environments.	4	29
opennmt: open-source toolkit for neural machine translation	we describe an opensource toolkit for neural machine translation (nmt). the toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting nmt research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. the toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.	4	31
linguistic input features improve neural machine translation	neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. in this paper we show that the strong learning capability of neural mt models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. we generalize the embedding layer of the encoder in the attentional encoderdecoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. we add morphological features, partofspeech tags, and syntactic dependency labels as input features to englishgerman, and english>romanian neural machine translation systems. in experiments on wmt16 training and test sets, we find that linguistic input features improve model quality according to three metrics perplexity, bleu and chrf3. an opensource implementation of our neural mt system is available, as are sample files and configurations.	3	31
achieving open vocabulary neural machine translation with hybrid word-character models	reacttext 428 in this paper, we describe fbk's neural machine translation (nmt) systems submitted at the international workshop on spoken language translation (iwslt) 2016. the systems are based on the stateoftheart nmt architecture that is equipped with a bidirectional encoder and an attention mechanism in the decoder. they leverage linguistic information such as lemmas and partofspeech tags of the...  /reacttext  reacttext 429   /reacttext [show full ]	3	29
a recursive recurrent neural network for statistical machine translation	in this paper, we propose a novel recursive recurrent neural network (r2nn) to model the endtoend decoding process for statistical machine translation. r2nn is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. a semisupervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. experiments on a chinese to english translation task show that our proposed r2nn can outperform the stateof theart baseline by about 1.5 points in bleu.	1	46
integrating an unsupervised transliteration model into statistical machine translation	we investigate three methods for integrating an unsupervised transliteration model into an endtoend smt system. we induce a transliteration model from parallel data and use it to translate oov words. our approach is fully unsupervised and language independent. in the methods to integrate transliterations, we observed improvements from 0.230.75 (62 0.41) bleu points across 7 language pairs. we also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora.	1	43
integrating an unsupervised transliteration model into statistical machine translation	reduced mechanical stress to bone in bedridden patients and astronauts leads to bone loss and increase in fracture risk which is one of the major medical and health issues in modern aging society and space medicine. however, no molecule involved in the mechanisms underlying this phenomenon has been identified to date. osteopontin (opn) is one of the major noncollagenous proteins in bone matrix, but its function in mediating physicalforce effects on bone in vivo has not been known. to investigate the possible requirement for opn in the transduction of mechanical signaling in bone metabolism in vivo, we examined the effect of unloading on the bones of opn 61/ 61 mice using a tail suspension model. in contrast to the tail suspension–induced bone loss in wildtype mice, opn 61/ 61 mice did not lose bone. elevation of urinary deoxypyridinoline levels due to unloading was observed in wildtype but not in opn 61/ 61 mice. analysis of the mechanisms of opn deficiency–dependent reduction in bone on the cellular basis resulted in two unexpected findings. first, osteoclasts, which were increased by unloading in wildtype mice, were not increased by tail suspension in opn 61/ 61 mice. second, measures of osteoblastic bone formation, which were decreased in wildtype mice by unloading, were not altered in opn 61/ 61 mice.	1	43
nematus: a toolkit for neural machine translation	reacttext 452 neural machine translation (nmt) models are able to partially learn syntactic information from sequential lexical information. still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. this work aims to answer two questions 1) does explicitly modeling source or target language syntax help nmt? 2) is tight integration of words and syntax better than...  /reacttext  reacttext 453   /reacttext [show full ]	4	26
tree-to-sequence attentional neural machine translation	most of the existing neural machine translation (nmt) models focus on the conversion of sequential data and do not directly take syntax into consideration. we propose a novel endtoend syntactic nmt model, extending a sequencetosequence model with the sourceside phrase structure. our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. experimental results on the wat'15 englishtojapanese dataset demonstrate that our proposed model outperforms sequencetosequence attentional nmt models and compares favorably with the stateoftheart treetostring smt system.	3	25
coverage embedding models for neural machine translation	in this paper, we enhance the attentionbased neural machine translation (nmt) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in nmt. for each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. experiments on the largescale chinesetoenglish task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system.	3	27
linguistic input features improve neural machine translation	neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. in this paper we show that the strong learning capability of neural mt models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. we generalize the embedding layer of the encoder in the attentional encoderdecoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. we add morphological features, partofspeech tags, and syntactic dependency labels as input features to englishgerman neural machine translation systems. in experiments on wmt16 training and test sets, we find that linguistic input features improve model quality according to three metrics perplexity, bleu and chrf3.	3	26
fully character-level neural machine translation without explicit segmentation	most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. we introduce a neural machine translation (nmt) model that maps a source character sequence to a target character sequence without any segmentation. we employ a characterlevel convolutional network with maxpooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subwordlevel models while capturing local regularities. our charactertocharacter model outperforms a recently proposed baseline with a subwordlevel encoder on wmt'15 deen and csen, and gives comparable performance on fien and ruen. we then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a manytoone translation task. in this multilingual setting, the characterlevel encoder significantly outperforms the subwordlevel encoder on all the language pairs. we observe that on csen, fien and ruen, the quality of the multilingual characterlevel translation even surpasses the models specifically trained on that language pair alone, both in terms of bleu score and human judgment.	4	28
incorporating discrete translation lexicons into neural machine translation	neural machine translation (nmt) often makes mistakes in translating lowfrequency content words that are essential to understanding the meaning of the sentence. we propose a method to alleviate this problem by augmenting nmt systems with discrete translation lexicons that efficiently encode translations of these lowfrequency words. we describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the nmt model to select which source word lexical probabilities the model should focus on. we test two methods to combine this probability with the standard nmt probability (1) using it as a bias, and (2) linear interpolation. experiments on two corpora show an improvement of 2.02.3 bleu and 0.130.44 nist score, and faster convergence time.	3	24
deep recurrent models with fast-forward connections for neural machine translation	neural machine translation (nmt) aims at solving machine translation (mt) problems using neural networks and has exhibited promising results in recent years. however, most of the existing nmt models are shallow and there is still a performance gap between a single nmt model and the best conventional mt system. in this work, we introduce a new type of linear connections, named fastforward connections, based on deep long shortterm memory (lstm) networks, and an interleaved bidirectional architecture for stacking the lstm layers. fastforward connections play an essential role in propagating the gradients and building a deep topology of depth 16. on the wmt'14 englishtofrench task, we achieve bleu=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 bleu points. this is the first time that a single nmt model achieves stateoftheart performance and outperforms the best conventional model by 0.7 bleu points. we can still achieve bleu=36.3 even without using an attention mechanism. after special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with bleu=40.4. our models are also validated on the more difficult wmt'14 englishtogerman task.	3	24
encoding source language with convolutional neural network for machine translation	the recently proposed neural network joint model (nnjm) (devlin et al., 2014) augments the ngram target language model with a heuristically chosen source context window, achieving stateoftheart performance in smt. in this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. with different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. this representation, together with target language words, are fed to a deep neural network (dnn) to form a stronger nnjm. experiments on two nist chineseenglish translation tasks show that the proposed model can achieve significant improvements over the previous nnjm by up to +1.08 bleu points on average	2	32
machine translation instant messaging applications	an instant messaging translation plugin interacts with an instant messaging program to intercept incoming messages and forward these messages to a language translation service. the plugin then displays a translation received from the service along with the original message. this provides translation which can be used by instant messaging users to communicate across language barriers, and without local translation or knowledge of the internal workings of the translation services used. additionally, the translation plugin also provides for manual translation of messages, which allows communication with users who use a different language but do not use the translation plugin. messages are modified before translation in order to correct spelling, to prevent particular words or phrases from being translated, and to change instant messaging language into standard language form. the techniques can be performed on various messaging services, including instant messaging on computers or mobile devices, as well as sms.	1	34
montreal neural machine translation systems for wmt’15	reacttext 555 we propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. these models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. we also discover that attentionbased neural machine...  /reacttext  reacttext 556   /reacttext [show full ]	2	30
syntactically guided neural machine translation	we investigate the use of hierarchical phrasebased smt lattices in endtoend neural machine translation (nmt). weight pushing transforms the hiero scores.	3	23
vocabulary manipulation for neural machine translation	in order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. in this paper, we alleviate this issue by introducing a sentencelevel or batchlevel vocabulary, which is only a very small subset of the full output vocabulary. for each sentence or batch, we only predict the target words in its sentencelevel or batchlevel vocabulary. thus, we reduce both the computing time and the memory usage. our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordtoword translation model or a bilingual phrase library learned from a traditional machine translation model. experimental results on the largescale englishtofrench task show that our method achieves better translation performance by 1 bleu point over the large vocabulary neural machine translation system of jean et al. (2015).	3	20
semi-supervised learning for neural machine translation	while endtoend neural machine translation (nmt) has made remarkable progress recently, nmt systems only rely on parallel corpora for parameter estimation. since parallel corpora are usually limited in quantity, quality, and coverage, especially for lowresource languages, it is appealing to exploit monolingual corpora to improve nmt. we propose a semisupervised approach for training nmt models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. the central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourcetotarget and targettosource translation models serve as the encoder and decoder, respectively. our approach can not only exploit the monolingual corpora of the target language, but also of the source language. experiments on the chineseenglish dataset show that our approach achieves significant improvements over stateoftheart smt and nmt systems.	3	20
a coverage embedding model for neural machine translation	in this paper, we enhance the attentionbased neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in nmt. for each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. all the initialized coverage embeddings and updating matrix are learned in the training procedure. experiments on the largescale chinesetoenglish task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary nmt system.	3	23
transfer learning for low-resource neural machine translation	the encoderdecoder framework for neural machine translation (nmt) has been shown effective in large data scenarios, but is much less effective for lowresource languages. we present a transfer learning method that significantly improves bleu scores across a range of lowresource languages. our key idea is to first train a highresource language pair (the parent model), then transfer some of the learned parameters to the lowresource pair (the child model) to initialize and constrain training. using our transfer learning method we improve baseline nmt models by an average of 5.6 bleu on four lowresource language pairs. ensembling and unknown word replacement add another 2 bleu which brings the nmt performance on lowresource machine translation close to a strong syntax based machine translation (sbmt) system, exceeding its performance on one language pair. additionally, using the transfer learning model for rescoring, we can improve the sbmt system by an average of 1.3 bleu, improving the stateoftheart on lowresource machine translation.	3	21
is neural machine translation ready for deployment? a case study on 30 translation directions	in this paper we provide the largest published comparison of translation quality for phrasebased smt and neural machine translation across 30 translation directions. for ten directions we also include hierarchical phrasebased mt. experiments are performed for the recently published united nations parallel corpus v1.0 and its large sixway sentencealigned subcorpus. in the second part of the paper we investigate aspects of translation speed, introducing amunmt, our efficient neural machine translation decoder. we demonstrate that current neural machine translation could already be used for inproduction systems when comparing wordspersecond ratios.	3	20
learning from post-editing: online model adaptation for statistical machine translation	using machine translation output as a starting point for human translation has become an increasingly common application of mt.we propose and evaluate three computationally efficient online methods for updating statistical mt systems in a scenario where postedited mt output is constantly being returned to the system (1) adding new rules to the translation model from the postedited content, (2) updating a bayesian language model of the target language that is used by the mt system, and (3) updating the mt system's discriminative parameters with a mira step. individually, these techniques can substantially improve mt quality, even over strong baselines. moreover, we see superadditive improvements when all three techniques are used in tandem.	1	30
customizable machine translation service	embodiments of the present invention provide a system and method for providing a translation service. the method comprises providing a translation interface accessible via a network. the translation interface receives specialized data associated with a domain from a member. a text string written in a source language is received from the member via the translation interface. a domainbased translation engine is selected. the domainbased translation engine may be associated with a source language, a target language, and a domain. the text string is translated into the target language using, at least in part, the selected domainbased translation engine. the translated text string is transmitted to the member via the internet. in some embodiments, a translation memory is generated based on the specialized data.	1	25
optimizing parameters for machine translation	methods, systems, and apparatus, including computer program products, for language translation are disclosed. in one implementation, a method is provided. the method includes accessing a hypothesis space, where the hypothesis space represents a plurality of candidate translations; performing decoding on the hypothesis space to obtain a translation hypothesis that minimizes an expected error in classification calculated relative to an evidence space; and providing the obtained translation hypothesis for use by a user as a suggested translation in a target translation.	1	27
supervised attentions for neural machine translation	in this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. we simply compute the distance between the machine attentions and the true alignments, and minimize this cost in the training procedure. our experiments on largescale chinesetoenglish task show that our model improves both translation and alignment qualities significantly over the largevocabulary neural machine translation system, and even beats a stateoftheart traditional syntaxbased system.	3	17
edinburgh's phrase-based machine translation systems for wmt-14	this paper describes the university of edinburgh's (uedin) phrasebased submissions to the translation and medical translation shared tasks of the 2014 workshop on statistical machine translation (wmt). we participated in all language pairs. we have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of english, ii) using unsupervised characterbased models to translate unknown words in russianenglish and hindienglish pairs, iii) synthesizing hindi data from closelyrelated urdu data, and iv) building huge language models on the common crawl corpus.	1	26
a shared task on multimodal machine translation and crosslingual image description	this paper introduces and summarises the findings of a new shared task at the in tersection of natural language process ing and computer vision the generation of image descriptions in a target language, given an image and/or one or more de scriptions in a different (source) language. this challenge was organised along with the conference on machine translation (wmt16), and called for system submis sions for two task variants (i) a transla tion task, in which a source language im age description needs to be translated to a target language, (optionally) with addi tional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image. in this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams.	3	19
on the elements of an accurate tree-to-string machine translation system	while treetostring (t2s) translation theoretically holds promise for efficient, accurate translation, in previous reports t2s systems have often proven inferior to other machine translation (mt) methods such as phrasebased or hierarchical phrasebased mt. in this paper, we attempt to clarify the reason for this performance gap by investigating a number of peripheral elements that affect the accuracy of t2s systems, including parsing, alignment, and search. based on detailed experiments on the englishjapanese and japanese english pairs, we show how a basic t2s system that performs on par with phrasebased systems can be improved by 2.64.6 bleu, greatly exceeding existing stateof theart methods. these results indicate that t2s systems indeed hold much promise, but the abovementioned elements must be taken seriously in construction of these systems.	1	24
zero-resource translation with multi-lingual neural machine translation	in this paper, we propose a novel finetuning algorithm for the recently introduced multiway, mulitlingual neural machine translate that enables zeroresource machine translation. when used together with novel manytoone translation strategies, we empirically show that this finetuning algorithm allows the multiway, multilingual model to translate a zeroresource language pair (1) as well as a singlepair neural translation model trained with up to 1m direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attentionrelated parameters.	3	17
mutual information and diverse decoding improve neural machine translation	sequencetosequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. we introduce an alternative objective function for neural mt that maximizes the mutual information between the source and target sentences, modeling the bidirectional dependency of sources and targets. we implement the model with a simple reranking method, and also introduce a decoding algorithm that increases diversity in the nbest list produced by the first pass. applied to the wmt german/english and french/english tasks, both mechanisms offer a consistent performance boost on both standard lstm and attentionbased neural mt architectures. the result is the best published performance for a single (nonensemble) neural mt system, as well as the potential application of our diverse decoding algorithm to other nlp reranking tasks.	3	17
agreement-based joint training for bidirectional attention-based neural machine translation	the attentional mechanism has proven to be effective in improving endtoend neural machine translation. however, due to the intricate structural divergence between natural languages, unidirectional attentionbased models might only capture partial aspects of attentional regularities. we propose agreementbased joint training for bidirectional attentionbased endtoend neural machine translation. instead of training sourcetotarget and targettosource translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. experiments on chineseenglish and englishfrench translation tasks show that agreementbased joint training significantly improves both alignment and translation quality over independent training.	3	19
systran's pure neural machine translation systems	since the first online demonstration of neural machine translation (nmt) by lisa, nmt development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of nmt engines to replace their existing technologies. nmt systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. in this work, we present our approach to productionready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). we explore different practical choices an efficient and evolutive opensource framework; data preparation; network architecture; additional implemented features; tuning for production; etc. we discuss about evaluation methodology, present our first findings and we finally outline further work. our ultimate goal is to share our expertise to build competitive production systems for generic translation. we aim at contributing to set up a collaborative framework to speedup adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of usecase specific engines integrated in real production workflows. mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.	3	14
semi-supervised learning for neural machine translation	while endtoend neural machine translation (nmt) has made remarkable progress recently, nmt systems only rely on parallel corpora for parameter estimation. since parallel corpora are usually limited in quantity, quality, and coverage, especially for lowresource languages, it is appealing to exploit monolingual corpora to improve nmt. we propose a semisupervised approach for training nmt models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. the central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourcetotarget and targettosource translation models serve as the encoder and decoder, respectively. our approach can not only exploit the monolingual corpora of the target language, but also of the source language. experiments on the chineseenglish dataset show that our approach achieves significant improvements over stateoftheart smt and nmt systems.	3	14
is neural machine translation ready for deployment? a case study on 30 translation directions	reacttext 471 this paper presents a machine translation tool – based on moses – developed for the international maritime organization (imo) for the automatic translation of documents from spanish, french, russian and arabic to/from english. the main challenge lies in the insufficient size of inhouse corpora (especially for russian and arabic). the united nations (un) granted imo the right to use un...  /reacttext  reacttext 472   /reacttext [show full ]	3	15
hybrid simplification using deep semantics and machine translation	we present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. the approach differs from previous work in two main ways. first, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. when compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	2	21
a sense-based translation model for statistical machine translation	the sense in which a word is used determines the translation of the word. in this paper, we propose a sensebased translation model to integrate word senses into statistical machine translation. we build a broadcoverage sense tagger based on a nonparametric bayesian topic model that automatically learns sense clusters for words in the source language. the proposed sensebased translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. we test the effectiveness of the proposed sensebased translation model on a largescale chinesetoenglish translation task. results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation.	1	23
zero-resource translation with multi-lingual neural machine translation	reacttext 452 researchers in computer science discipline put a great effort for developing machine intelligence techniques, inspired from human intelligence and to cultivate elegant mathematical tools to design …  /reacttext  reacttext 453   /reacttext [more]	3	14
collecting and using comparable corpora for statistical machine translation	lack of sufficient parallel data for many languages and domains is currently one of the major obstacles to further advancement of automated translation. the accurat project is addressing this issue by researching methods how to improve machine translation systems by using comparable corpora. in this paper we present tools and techniques developed in the accurat project that allow additional data needed for statistical machine translation to be extracted from comparable corpora. we present methods and tools for acquisition of comparable corpora from the web and other sources, for evaluation of the comparability of collected corpora, for multilevel alignment of comparable corpora and for extraction of lexical and terminological data for machine translation. finally, we present initial evaluation results on the utility of collected corpora in domainadapted machine translation and reallife applications.	2	20
neural reranking improves subjective quality of machine translation: naist at wat2015	this year, the nara institute of science and technology (naist)'s submission to the 2015 workshop on asian translation was based on syntaxbased statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. experiments reconfirmed results from previous work stating that neural mt reranking provides a large gain in objective evaluation measures such as bleu, and also confirmed for the first time that these results also carry over to manual evaluation. we further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words.	2	21
um-corpus: a large english-chinese parallel corpus for statistical machine translation	parallel corpus is a valuable resource for crosslanguage information retrieval and datadriven natural language processing systems,especially for statistical machine translation (smt). however, most existing parallel corpora to chinese are subject to inhouse use,while others are domain specific and limited in size. to a certain degree, this limits the smt research. this paper describes the acquisitionof a large scale and high quality parallel corpora for english and chinese. the corpora constructed in this paper contain about 15 millionenglishchinese (ec) parallel sentences, and more than 2 million training data and 5,000 testing sentences are made publicly available.different from previous work, the corpus is designed to embrace eight different domains. some of them are further categorized intodifferent topics. the corpus will be released to the research community, which is available at the nlp 2 ct 1 website.	1	22
combination of stochastic understanding and machine translation systems for language portability of dialogue systems	in this paper, several approaches for language portability of dialogue systems are investigated with a focus on the spoken language understanding (slu) component. we show that the use of statistical machine translation (smt) can greatly reduce the time and cost of porting an existing system from a source to a target language. using automatically translated training data we study phrasebased machine translation as an alternative to conditional random fields for conceptual decoding to compensate for the loss of a precise conceptword alignment. also two ways to increase slu robustness to translation errors (smeared training data and translation post editing) are shown to improve performance when test data are translated then decoded in the source language. overall the combination of all these approaches allows to reduce even further the concept error rate. experiments were carried out on the french media dialogue corpus with a subset manually translated into italian.	1	22
improved neural machine translation with smt features	neural machine translation (nmt) conducts endtoend translation with a source language encoder and a target language decoder, making promising translation performance. however, as a newly emerged approach, the method has some limitation...	3	14
improving evaluation of machine translation quality estimation	quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. issues can arise during comparison of quality estimation prediction score distributions and gold label distributions, however. in this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in prediction score distributions. as an alternative, we propose the use of the unitfree pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. components ofwmt13 andwmt14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks.	2	18
attention-based multimodal neural machine translation	reacttext 467 in this work, synthesis of facial animation is done by modelling the mapping between facial motion and speech using the shared gaussian process latent variable model. both data are processed separately and subsequently coupled together to yield a shared latent space. this method allows coarticulation to be modelled by having a dynamical model on the latent space. synthesis of novel animation...  /reacttext  reacttext 468   /reacttext [show full ]	3	13
towards zero unknown word in neural machine translation	neural machine translation has shown promising results in recent years. in order to control the com putational complexity, nmt has to employ a small vocabulary, and massive rare words outside the vo cabulary are all replaced with a single unk symbol. besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the invocabulary words. to tackle this problem, we propose a novel substitutiontranslationrestoration method. in sub stitution step, the rare words in a testing sen tence are replaced with similar invocabulary words based on a similarity model learnt from monolin gual data. in translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. exper iments on chinesetoenglish translation demon strate that our proposed method can achieve more than 4 bleu points over the attentionbased nmt. when compared to the recently proposed method handling rare words in nmt, our method can also obtain an improvement by nearly 3 bleu points.	3	14
edinburgh’s phrase-based machine translation systems for wmt-14.	this paper describes the university of edinburgh's (uedin) phrasebased submissions to the translation and medical translation shared tasks of the 2014 workshop on statistical machine translation (wmt). we participated in all language pairs. we have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of english, ii) using unsupervised characterbased models to translate unknown words in russianenglish and hindienglish pairs, iii) synthesizing hindi data from closelyrelated urdu data, and iv) building huge language models on the common crawl corpus.	1	20
hybrid simplification using deep semantics and machine translation	we present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. the approach differs from previous work in two main ways. first, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. when compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	1	21
guided alignment training for topic-aware neural machine translation	in this paper, we propose an effective way for biasing the attention mechanism of a sequencetosequence neural machine translation (nmt) model towards the wellstudied statistical word alignment models. we show that our novel guided alignment training approach improves translation quality on reallife ecommerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. we also show that metadata associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. with both novel features, the bleu score of the nmt system on a product title set improves from 18.6 to 21.3%. even larger mt quality gains are obtained through domain adaptation of a general domain nmt system to ecommerce data. the developed nmt system also performs well on the iwslt speech translation task, where an ensemble of four variant systems outperforms the phrasebased baseline by 2.1% bleu absolute.	3	13
overcoming the curse of sentence length for neural machine translation using automatic segmentation	the authors of (cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrasebased translation systems. in this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. empirical results show a significant improvement in translation quality for long sentences.	1	21
learning to generate pseudo-code from source code using statistical machine translation (t)	pseudocode written in natural language can aid the comprehension of source code in unfamiliar programming languages. however, the great majority of source code has no corresponding pseudocode, because pseudocode is redundant and laborious to create. if pseudocode could be generated automatically and instantly from given source code, we could allow for ondemand production of pseudocode without human effort. in this paper, we propose a method to automatically generate pseudocode from source code, specifically adopting the statistical machine translation (smt) framework. smt, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudocode pairs, making it possible to create a pseudocode generator with less human effort. in experiments, we generated english or japanese pseudocode from python statements using smt, and find that the generated pseudocode is largely accurate, and aids code understanding.	2	19
arabic machine translation: a survey	although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. we discuss some of the linguistic characteristics of the arabic language. features of arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. this paper summarizes the major techniques used in machine translation from arabic into english, and discusses their strengths and weaknesses.	1	19
neural machine translation with reconstruction	although endtoend neural machine translation (nmt) has achieved remarkable progress in the past two years, it suffers from a major drawback translations generated by nmt systems often lack of adequacy. it has been widely observed that nmt tends to repeatedly translate some source words while mistakenly ignoring other words. to alleviate this problem, we propose a novel encoderdecoderreconstructor framework for nmt. the reconstructor, incorporated into the nmt model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. experiments show that the proposed framework significantly improves the adequacy of nmt output and achieves superior translation result over stateoftheart nmt and statistical mt systems.	3	12
fast domain adaptation for neural machine translation	neural machine translation (nmt) is a new approach for automatic translation of text from one human language into another. the basic concept in nmt is to train a large neural network that maximizes the translation performance on a given parallel corpus. nmt is gaining popularity in the research community because it outperformed traditional smt approaches in several translation tasks at wmt and other evaluation tasks/benchmarks at least for some language pairs. however, many of the enhancements in smt over the years have not been incorporated into the nmt framework. in this paper, we focus on one such enhancement namely domain adaptation. we propose an approach for adapting a nmt system to a new domain. the main idea behind domain adaptation is that the availability of large outofdomain training data and a small indomain training data. we report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs. with our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly. in addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the nmt model on the combined data which usually takes several days/weeks depending on the volume of the data.	3	13
massive exploration of neural machine translation architectures	neural machine translation (nmt) has shown remarkable progress over the past few years with production systems now being deployed to endusers. one major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of gpu time to converge. this makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. in this work, we present the first largescale analysis of nmt architecture hyperparameters. we report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 gpu hours on the standard wmt english to german translation task. our experiments lead to novel insights and practical advice for building and extending nmt architectures. as part of this contribution, we release an opensource nmt framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.	4	13
vocabulary selection strategies for neural machine translation	classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. in this paper we experiment with context and embeddingbased selection methods and extend previous work by examining speed and accuracy tradeoffs in more detail. we show that decoding time on cpus can be reduced by up to 90% and training time by 25% on the wmt15 englishgerman and wmt16 englishromanian tasks at the same or only negligible change in accuracy. this brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single cpu core for englishgerman.	3	12
polish - english speech statistical machine translation systems for the iwslt 2014	in this paper, we attempt to improve statistical machine translation (smt) systems on a very diverse set of language pairs (in both directions) czech  english, vietnamese  english, french  english and german  english. to accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our smt systems. innovative tools and data adaptation techniques were employed. the ted parallel text corpora for the iwslt 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. in addition, we prepared wikipediabased comparable corpora for use with our smt system. this data was specified as permissible for the iwslt 2015 evaluation. we explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the kenlm language modeling tool. to evaluate the effects of different preparations on translation results, we conducted experiments and used the bleu, nist and ter metrics. our results indicate that our approach produced a positive impact on smt quality.	2	17
proceedings of the 15th annual conference of the european association for machine translation	the blood coagulation mechanism consists of a series of concatenated chemical reactions, governed by the coagulation factors present in the blood plasma, after the activation of the clot mechanism. the last reaction corresponds to the fibrinogen conversion into fibrin, followed by the fibrin polymerisation and production of a stable fibrin network. during the clotting process, there is a solgel transformation of the medium. the subject of the present paper is the measurement of the ultrasonic attenuation coefficient for human blood plasma during the coagulation process, in the frequency range of 8 to 22 mhz. the clot was obtained after the procedure to measure the prothrombin time (6512 s) mixing 150 μl of reconstituted lyophilised normal plasma with 300 μl of reconstituted lyophilised thromboplastin immersed in a water bath with the temperature controlled at 36.5°c. the attenuation coefficient for pure plasma remained constant within the measurement period of 10 s and at frequencies of 8, 9, 10, 15, 20, 21 and 22 mhz. on the other hand, there is a detectable timedecay of the attenuation coefficient for samples of plasma going through the coagulation process and at frequencies of 8, 9, 10 and 15 mhz. the timedecay becomes less and less detectable as the frequency increases and it becomes completely undetectable at 20, 21 and 22 mhz. (email jcm@peb.ufrj.br )	1	19
log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing	this paper describes the submission of the amu (adam mickiewicz university) team to the automatic postediting (ape) task of wmt 2016. we explore the application of neural translation models to the ape problem and achieve good results by treating different models as components in a loglinear model, allowing for multiple inputs (the mtoutput and the source) that are decoded to the same target language (postedited translations). a simple stringmatching penalty integrated within the loglinear model can be used to control for higher faithfulness with regard to the tobecorrected machine translation input. our submission outperforms the uncorrected baseline on the unseen test set by 3.2% ter and +5.5% bleu.	3	12
log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing	this paper describes the submission of the amu (adam mickiewicz university) team to the automatic postediting (ape) task of wmt 2016. we explore the application of neural translation models to the ape problem and achieve good results by treating different models as components in a loglinear model, allowing for multiple inputs (the mtoutput and the source) that are decoded to the same target language (postedited translations). a simple stringmatching penalty integrated within the loglinear model is used to control for higher faithfulness with regard to the raw machine translation output. to overcome the problem of too little training data, we generate large amounts of artificial data. our submission improves over the uncorrected baseline on the unseen test set by 3.2\% ter and +5.5\% bleu and outperforms any other system submitted to the sharedtask by a large margin.	3	12
selection and use of nonstatistical translation components in a statistical machine translation framework	a system with a nonstatistical translation component integrated with a statistical translation component engine. the same corpus may be used for training the statistical engine and also for determining when to use the statistical engine and when to use the translation component. this training may use probabilistic techniques. both the statistical engine and the translation components may be capable of translating the same information, however the system determines which component to use based on the training. retraining can be carried out to add additional components, or when after additional translator training.	1	19
arabic machine translation: a survey	although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. we discuss some of the linguistic characteristics of the arabic language. features of arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. this paper summarizes the major techniques used in machine translation from arabic into english, and discusses their strengths and weaknesses.	1	18
factored neural machine translation	we present a new approach for neural machine translation (nmt) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. this architecture addresses two main problems occurring in mt, namely dealing with a large target language vocabulary and the out of vocabulary (oov) words. by the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). in addition, we can produce new words that are not in the vocabulary. we use a morphological analyser to get a factored representation of each word (lemmas, part of speech tag, tense, person, gender and number). we have extended the nmt approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. the final translation is built using some \textit{a priori} linguistic information. we compare our extension with a wordbased nmt system. the experiments, performed on the iwslt'15 dataset translating from english to french, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the oov rate. we observe up to 2% bleu point improvement in a simulated out of domain translation setup.	3	11
variational neural machine translation	models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. in this paper, we propose a variational model to learn this conditional distribution for neural machine translation a variational encoderdecoder model that can be trained endtoend. different from the vanilla encoderdecoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. in order to perform efficient posterior inference and largescale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. experiments on both chineseenglish and english german translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.	3	11
jane: open source machine translation system combination	different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they yield. system combination is a method for combining the output.	1	16
using discourse structure improves machine translation evaluation	we present experiments in using discourse structure for improving machine translation evaluation. we first design two discourseaware similarity measures, which use allsubtree kernels to compare discourse parse trees in accordance with the rhetorical structure theory. then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the systemlevel. rather than proposing a single new metric, we show that discourse information is complementary to the stateoftheart evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.	1	16
maximal lattice overlap in example-based machine translation	examplebased machine translation (ebmt) retrieves pretranslated phrases from a sentencealigned bilingual training corpus to translate new input sentences. ebmt uses long pretranslated phrases effectively but is subject to disfluencies at phrasal translation boundaries. we address this problem by introducing a novel method that exploits overlapping phrasal translations and the increased confidence in translation accuracy they imply. we specify an efficient algorithm for producing translations using overlap. finally, our empirical analysis indicates that this approach produces higher quality translations than the standard method of ebmt in a peaktopeak comparison.	4	11
exploiting source-side monolingual data in neural machine translation	neural machine translation (nmt) based on the encoderdecoder architecture has recently become a new paradigm. researchers have proven that the targetside monolingual data can greatly enhance the decoder model of nmt. however, the sourceside monolingual data is not fully explored although it should be useful to strengthen the encoder model of nmt, especially when the parallel corpus is far from sufficient. in this paper, we propose two approaches to make full use of the source side monolingual data in nmt. the first ap proach employs the selflearning algorithm to generate the synthetic largescale parallel data for nmt training. the second approach ap plies the multitask learning framework using two nmts to predict the translation and the reordered sourceside monolingual sentences simultaneously. the extensive experiments demonstrate that the proposed methods ob tain significant improvements over the strong attentionbased nmt.	3	11
neural network based bilingual language model growing for statistical machine translation	since larger ngram language model (lm) usually performs better in statistical machine translation (smt), how to con struct efficient large lm is an important topic in smt. however, most of the ex isting lm growing methods need an extra monolingual corpus, where additional lm adaption technology is necessary. in this paper, we propose a novel neural network based bilingual lm growing method, only using the bilingual parallel corpus in smt. the results show that our method can im prove both the perplexity score for lm e valuation and bleu score for smt, and significantly outperforms the existing lm growing methods without extra corpus.	1	16
don’t until the final verb wait: reinforcement learning for simultaneous machine translation	reacttext 434 we describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of perl!) and outperforms stateoftheart approaches on a range of datasets. moreover, it is trivially extended to a...  /reacttext  reacttext 435   /reacttext [show full ]	1	17
neural machine translation with supervised attention	the attention mechanisim is appealing for neural machine translation, since it is able to dynam ically encode a source sentence by generating a alignment between a target word and source words. unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. in this paper, we analyze and explain this issue from the point view of re ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. experiments on two chinesetoenglish translation tasks show that the super vised attention mechanism yields better alignments leading to substantial gains over the standard attention based nmt.	3	10
coverage-based neural machine translation	however, attentional nmt ignores past alignment information, which leads to overtranslation and undertranslation problems. in response to this problem, we maintain a coverage vector to keep track of the attention history. the coverage vector is fed to the attention model to help adjust the future attention, which guides nmt to pay more attention to the untranslated source words. experiments show that coveragebased nmt significantly improves both alignment and translation quality over nmt without coverage.	3	10
edinburgh's statistical machine translation systems for wmt16	this paper describes the university of ed inburgh's phrasebased and syntaxbased submissions to the shared translation tasks of the acl 2016 first conference on ma chine translation (wmt16). we sub mitted five phrasebased and five syntax based systems for the news task, plus one phrasebased system for the biomedical task.	3	10
pragmatic neural language modelling in machine translation	this paper presents an indepth investigation on integrating neural language models in translation systems. scaling neural language models is a difficult task, but crucial for realworld applications. this paper evaluates the impact on endtoend mt quality of both new and existing scaling techniques. we show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. we also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. we explore the tradeoffs between neural models and backoff ngram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. we conclude with a set of recommendations one should follow to build a scalable neural language model for mt.	2	14
incorporating pronoun function into statistical machine translation	pronouns are used frequently in language, and perform a range of functions. some pronouns are used to express coreference, and others are not. languages and genres differ in how and when they use pronouns and this poses a problem for statistical machine translation (smt) systems (le nagard and koehn, 2010; hardmeier and federico, 2010; novák, 2011; guillou, 2012; weiner, 2014; hardmeier, 2014). attention to date has focussed on coreferential (anaphoric) pronouns with np antecedents, which when translated from english into a language with grammatical gender, must agree with the translation of the head of the antecedent. despite growing attention to this problem, little progress has been made, and little attention has been given to other pronouns. the central claim of this thesis is that pronouns performing different functions in text should be handled differently by smt systems and when evaluating pronoun translation. this motivates the introduction of a new framework to categorise pronouns according to their function anaphoric/cataphoric reference, event reference, extratextual reference, pleonastic, addressee reference, speaker reference, generic reference, or other function.	3	10
proceedings of the second workshop on statistical machine translation	the special challenge of the wmt 2007 shared task was domain adaptation. we took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here news commentary), when most of the training data is from a different domain (here european parliament speeches). this paper also gives a description of the submission of the university of edinburgh to the shared task.	2	13
assessing the discourse factors that influence the quality of machine translation	we present a study of aspects of discourse structure  specifically discourse devices used to organize information in a sentence that significantly impact the quality of machine translation. our analysis is based on manual evaluations of translations of news from chinese and arabic to english. we find that there is a particularly strong mismatch in the notion of what constitutes a sentence in chinese and english, which occurs often and is associated with significant degradation in translation quality. also related to lower translation quality is the need to employ multiple explicit discourse connectives (because, but, etc.), as well as the presence of ambiguous discourse connectives in the english translation. furthermore, the mismatches between discourse expressions across languages significantly impact translation quality.	1	14
an empirical analysis of data selection techniques in statistical machine translation.	[en] domain adaptation has recently gained interest in statistical machine translation. one of the adaptation techniques is based in the selection data. data selection aims to select the best subset of the bilingual sentences from an available pool of sentences, with which to train a smt system. in this paper, we study how affect the bilingual corpora used for the data selection methods in the translation quality	2	14
neural machine translation with external phrase memory	in this paper, we propose phrasenet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. for any given source sentence, phrasenet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. the decoder utilizes a mixture of wordgenerating component and phrasegenerating component, with a specifically designed strategy to generate a sequence of multiple words all at once. the phrasenet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the wordbyword generation mechanism of recurrent neural network. our empirical study on chinesetoenglish translation shows that, with carefullychosen phrase table in memory, phrasenet yields 3.45 bleu improvement over the generic neural machine translator.	3	9
syntax-aware neural machine translation using ccg	neural machine translation (nmt) models are able to partially learn syntactic information from sequential lexical information. still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. this work aims to answer two questions 1) does explicitly modeling source or target language syntax help nmt? 2) is tight integration of words and syntax better than multitask training? we introduce syntactic information in the form of ccg supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. our results on wmt data show that explicitly modeling syntax improves machine translation quality for englishgerman, a highresource pair, and for englishromanian, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.	4	9
embedding word similarity with neural machine translation	neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. here we investigate the embeddings learned by neural machine translation models, a recentlydeveloped class of neural language model. we show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexicalsyntactic role. we further show that these effects hold when translating from both english to french and english to german, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. our embedding spaces can be queried in an online demo and downloaded from our web page. overall, our analyses indicate that translationbased embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) interword relatedness.	1	14
a convolutional encoder model for neural machine translation	the prevalent approach to neural machine translation relies on bidirectional lstms to encode the source sentence. in this paper we present a faster and simpler architecture based on a succession of convolutional layers. this allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. on wmt'16 englishromanian translation we achieve competitive accuracy to the stateoftheart and we outperform several recently published results on the wmt'15 englishgerman task. our models obtain almost the same accuracy as a very deep lstm setup on wmt'14 englishfrench translation. our convolutional encoder speeds up cpu decoding by more than two times at the same or higher accuracy as a strong bidirectional lstm baseline.	3	9
standardizing tweets with character-level machine translation	summary this paper presents the results of the standardization procedure of slovene tweets that are full of colloquial, dialectal and foreignlanguage elements. with the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient outofvocabulary (oov) tokens and used it to train a characterlevel statistical machine translation system (csmt). best results were obtained by combining the manually constructed lexicon and csmt as fallback with an overall improvement of 9.9\% increase on all tokens and 31.3\% on oov tokens. manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1\% pertoken accuracy with the former and 83.6\% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	1	14
standardizing tweets with character-level machine translation	this paper presents the results of the standardization procedure of slovene tweets that are full of colloquial, dialectal and foreignlanguage elements. with the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient outofvocabulary (oov) tokens and used it to train a characterlevel statistical machine translation system (csmt). best results were obtained by combining the manually constructed lexicon and csmt as fallback with an overall improvement of 9.9% increase on all tokens and 31.3% on oov tokens. manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1% pertoken accuracy with the former and 83.6% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	1	14
adaptation of machine translation for multilingual information retrieval in the medical domain	we investigate machine translation (mt) of user search queries in the context of crosslingual information retrieval (ir) in the medical domain. the main focus is on techniques to adapt mt to increase translation quality; however, we also explore mt adaptation to improve effectiveness of crosslingual ir. our mt system is moses, a stateoftheart phrasebased statistical machine translation system. the ir system is based on the bm25 retrieval model implemented in the lucene search engine. the mt techniques employed in this work include indomain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. the ir methods include morphological normalization and using multiple translation variants for query expansion. the experiments are performed and thoroughly evaluated on three language pairs czech–english, german–english, and french–english. mt quality is evaluated on data sets created within the khresmoi project and ir effectiveness is tested on the clef ehealth 2013 data sets.	1	14
learning to parse and translate improves neural machine translation	there has been relatively little attention to incorporating linguistic prior to neural machine translation. much of the previous work was further constrained to considering linguistic prior on the source side. in this paper, we propose a hybrid model, called nmt+rnng, that learns to parse and translate by combining the recurrent neural network grammar into the attentionbased neural machine translation. our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. extensive experiments with four language pairs show the effectiveness of the proposed nmt+rnng.	4	9
an empirical comparison of simple domain adaptation methods for neural machine translation	in this paper, we propose a novel domain adaptation method named mixed fine tuning for neural machine translation (nmt). we combine two existing approaches namely fine tuning and multi domain nmt. we first train an nmt model on an outofdomain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the indomain and outofdomain corpora. all corpora are augmented with artificial tags to indicate specific domains. we empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings.	4	9
machine translation: mining text for social theory	more of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online trans.	3	8
neural machine translation with recurrent attention modeling	knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. we improve upon the attention model of bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. this architecture easily captures informative features, such as fertility and regularities in relative distortion. in experiments, we show our parameterization of attention improves translation quality.	3	8
neural machine translation advised by statistical machine translation	neural machine translation (nmt) is a new approach to machine translation that has made great progress in recent years. however, recent studies show that nmt generally produces fluent but inadequate translations (tu et al. 2016b; tu et al. 2016a; he et al. 2016; tu et al. 2017). this is in contrast to conventional statistical machine translation (smt), which usually yields adequate but nonfluent translations. it is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate smt model into nmt framework. more specifically, at each decoding step, smt offers additional recommendations of generated words based on the decoding information from nmt (e.g., the generated partial translation and attention history). then we employ an auxiliary classifier to score the smt recommendations and a gating function to combine the smt recommendations with nmt generations, both of which are jointly trained within the nmt architecture in an endtoend manner. experimental results on chineseenglish translation show that the proposed approach achieves significant and consistent improvements over stateoftheart nmt and smt systems on multiple nist test sets.	3	9
towards string-to-tree neural machine translation	we present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. an experiment on the wmt16 germanenglish news translation task resulted in an improved bleu score when compared to a syntaxagnostic nmt baseline trained on the same dataset. an analysis of the translations from the syntaxaware system shows that it performs more reordering during translation in comparison to the baseline. a smallscale human evaluation also showed an advantage to the syntaxaware system.	4	9
doubly-attentive decoder for multi-modal neural machine translation	we introduce a multimodal neural machine translation model in which a doublyattentive decoder naturally incorporates spatial visual features obtained using pretrained convolutional neural networks, bridging the gap between image description and translation. our decoder learns to attend to sourcelanguage words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. we find that our model can efficiently exploit not just backtranslated indomain multimodal data but also large generaldomain textonly mt corpora. we also report stateoftheart results on the multi30k data set.	4	8
nmtpy: a flexible toolkit for advanced neural machine translation systems	in this paper, we present nmtpy, a flexible python toolkit based on theano for training neural machine translation and other neural sequencetosequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for lium's topranked submissions to wmt multimodal machine translation and news translation tasks in 2016 and 2017.	4	8
context-dependent word representation for neural machine translation 	we first observe a potential weakness of continuous vector representations of symbols in neural machine translation. that is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. this has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bagofwords representation of the source sentence. additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not wellsuited to be translated via continuous vectors. the experiments on en–fr and en–de reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.	4	8
systems and methods for tuning parameters in statistical machine translation	a method for tuning translation parameters in statistical machine translation based on ranking of the translation parameters is disclosed. according to one embodiment, the method includes sampling pairs of candidate translation units from a set of candidate translation units corresponding to a source unit, each candidate translation unit corresponding to numeric values assigned to one or more features, receiving an initial weighting value for each feature, comparing the pairs of candidate translation units to produce binary results, and using the binary results to adjust the initial weighting values to produce modified weighting values.	1	13
bilingual continuous-space language model growing for statistical machine translation	larger ngram language models (lms) perform better in statistical machine translation (smt). however, the existing approaches have two main drawbacks for constructing larger lms 1) it is not convenient to obtain larger corpora in the same domain as the bilingual parallel corpora in smt; 2) most of the previous studies focus on monolingual information from the target corpora only, and redundant ngrams have not been fully utilized in smt. nowadays, continuousspace language model (cslm), especially neural network language model (nnlm), has been shown great improvement in the estimation accuracies of the probabilities for predicting the target words. however, most of these cslm and nnlm approaches still consider monolingual information only or require additional corpus. in this paper, we propose a novel neural network based bilingual lm growing method. compared to the existing approaches, the proposed method enables us to use bilingual parallel corpus for lm growing in smt. the results show that our new method outperforms the existing approaches on both smt performance and computational efficiency significantly.	2	11
the operation sequence model – combining n-gram-based and phrase-based statistical machine translation	in this article, we present a novel machine translation model, the operation sequence model (osm), which combines the benefits of phrasebased and ngrambased statistical machine translation (smt) and remedies their drawbacks. the model represents the translation process as a linear sequence of operations. the sequence includes not only translation operations but also reordering operations. as in ngrambased smt, the model is (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption, and (iv) avoids the spurious phrasal segmentation problem. as in phrasebased smt, themodel (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically, and (iii) decodes with large translation units during search. the unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and longrange reorderings consistently. using bleu as a metric of translation accuracy, we found that our system performs significantly better than stateoftheart phrasebased systems (moses and phrasal) and ngrambased systems (ncode) on standard translation tasks. we compare the reordering component of the osm to the moses lexical reordering model by integrating it into moses. our results show that osm outperforms lexicalized reordering on all translation tasks.	2	12
machine translation for query expansion	methods, systems and apparatus, including computer program products, for expanding search queries. one method includes receiving a search query, selecting a synonym of a term in the search query based on a context of occurrence of the term in the received search query, the synonym having been derived from statistical machine translation of the term, and expanding the received search query with the synonym and using the expanded search query to search a collection of documents. alternatively, another method includes receiving a request to search a corpus of documents, the request specifying a search query, using statistical machine translation to translate the specified search query into an expanded search query, the specified search query and the expanded search query being in the same natural language, and in response to the request, using the expanded search query to search a collection of documents.	2	11
eu-bridge mt: combined machine translation	this paper describes one of the collaborative efforts within eubridge to further advance the state of the art in machine translation between two european language pairs, german→english and english→german. three research institutes involved in the eubridge project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the acl 2014 eighth workshop on statistical machine translation (wmt 2014). we combined up to nine different machine translation engines via system combination. rwth aachen university, the university of edinburgh, and karlsruhe institute of technology developed several individual systems which serve as system combination input. we devoted special attention to building syntaxbased systems and combining them with the phrasebased ones. the joint setups yield empirical gains of up to 1.6 points in bleu and 1.0 points in ter on the wmt newstest2013 test set compared to the best single systems.	1	12
submodularity for data selection in machine translation	we introduce submodular optimization to the problem of training data subset selection for statistical machine translation (smt). by explicitly formulating data selection as a submodular program, we ob tain fast scalable selection algorithms with mathematical performance guarantees, re sulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. we present a new class of submodular functions designed specifically for smt and evaluate them on two differ ent translation tasks. our results show that our best submodular method significantly outperforms several baseline methods, including the widelyused crossentropy based data selection method. in addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing.	1	12
statistical machine translation enhancements through linguistic levels: a survey	machine translation can be considered a highly interdisciplinary and multidisciplinary field because it is approached from the point of view of human trans.	1	12
the edinburgh/jhu phrase-based machine translation systems for wmt~2015	this paper describes the submission of the university of edinburgh and the johns hopkins university for the shared translation task of the emnlp 2015 tenth workshop on statistical machine translation (wmt 2015). we set up phrasebased statistical machine translation systems for all ten language pairs of this year鈥檚 evaluation campaign, which are english paired with czech, finnish, french, german, and russian in both translation directions.novel research directions we investigated include neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features.	2	11
the edinburgh/jhu phrase-based machine translation systems for wmt 2015	this paper describes the submission of the university of edinburgh and the johns hopkins university for the shared transla tion task of the emnlp 2015 tenth work shop on statistical machine translation (wmt 2015). we set up phrasebased sta tistical machine translation systems for all ten language pairs of this year's evaluation campaign, which are english paired with czech, finnish, french, german, and rus sian in both translation directions.	2	11
phrasal: a toolkit for new directions in statistical machine translation	we present a new version of phrasal, an opensource toolkit for statistical phrasebased machine translation. this revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) webbased interactive machine translation. a direct comparison with moses shows favorable results in terms of decoding speed and tuning time. 1	1	12
using joint models for domain adaptation in statistical machine translation	joint models have recently shown to improve the stateoftheart in machine translation (mt). we apply embased mixture modeling and data selection techniques using two joint models, namely the operation sequence model or osm — an ngrambased translation and reordering model, and the neural network joint model or nnjm — a continuous space translation model, to carry out domain adaptation for mt. the diversity of the two models, osm with inherit reordering information and nnjm with continuous space modeling makes them interesting to be explored for this task. our contribution in this paper is fusing the existing known techniques (linear interpolation, crossentropy) with the stateoftheart mt models (osm, nnjm). on a standard task of translating germantoenglish and arabictoenglish iwslt ted talks, we observed statistically significant improvements of up to +0.9 bleu points.	2	11
an empirical comparison of features and tuning for phrase-based machine translation	scalable discriminative training methods are now broadly available for estimating phrasebased, featurerich translation models. however, the sparse feature sets typically appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities they often overfit, do not generalize, or require complex and slow feature extractors. this paper introduces extended features, which are more specific than dense features yet more general than lexicalized sparse features. largescale experiments show that extended features yield robust bleu gains for both arabicenglish (+1.05) and chineseenglish (+0.67) relative to a strong featurerich baseline. we also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and languageindependent tools for implementing the features. 1	1	12
towards string-to-tree neural machine translation	goldberg. towards stringtotree neural machine translation. in acl, 2017.roee aharoni and yoav goldberg. 2017. towards stringtotree neural machine translation. arxiv eprints.aharoni and goldberg 2017] aharoni, r., and goldberg, ...	4	7
machine translation post-editing and effort. empirical studies on the post-editing process	this dissertation investigates the practice of machine translation postediting and the various aspects of effort involved in postediting work. through analyses of edits made by posteditors, the work described here examines three main questions 1) what types of machine translation errors or source text features cause particular effort in postediting, 2) what types of errors can or cannot be corrected without the help of the source text, and 3) how different indicators of effort vary between different posteditors. the dissertation consists of six previously published articles, and an introductory summary. five of the articles report original research, and involve analyses of postediting data to examine questions related to postediting effort as well as differences between posteditors. the sixth article is a survey presenting an overview of the research literature. the research reported is based on multiple datasets consisting of machine translations and their postedited versions, as well as process and evaluation data related to postediting effort. the dissertation presents a mixed methods study combining qualitative and quantitative approaches, as well as theoretical and analytical tools from the fields of language technology and translation studies. data on edits performed by posteditors, postediting time, keylogging data, and subjective evaluations of effort are combined with error analyses of the machine translations in question, and compared for various posteditors. the results of this dissertation provide evidence that, in addition to the number of edits performed.	3	7
latest trends in hybrid machine translation and its applications	this survey provides a detailed overview of the modification of the standard rulebased architecture to include statistical knowledge, the introduction of rules in corpusbased approaches, and the hybridization of approaches within this last single category. the principal aim here is to cover the leading research and progress in this field of mt and in several related applications.	2	10
an evaluation of machine translation for multilingual sentence-level sentiment analysis	sentiment analysis has become a key tool for several social media applications, including analysis of user's opinions about products and services, support to politics during campaigns and even for market trending. there are multiple existing sentiment analysis methods that explore different techniques, usually relying on lexical resources or learning approaches. despite the large interest on this theme and amount of research efforts in the field, almost all existing methods are designed to work with only english content. most existing strategies in specific languages consist of adapting existing lexical resources, without presenting proper validations and basic baseline comparisons. in this paper, we take a different step into this field. we focus on evaluating existing efforts proposed to do language specific sentiment analysis. to do it, we evaluated twentyone methods for sentencelevel sentiment analysis proposed for english, comparing them with two languagespecific methods. based on nine languagespecific datasets, we provide an extensive quantitative analysis of existing multilanguage approaches. our main result suggests that simply translating the input text on a specific language to english and then using one of the existing english methods can be better than the existing language specific efforts evaluated. we also rank those implementations comparing their prediction performance and identifying the methods that acquired the best results using machine translation across different languages. as a final contribution to the research community, we release our codes and datasets.	3	7
improving neural machine translation with conditional sequence generative adversarial nets	this paper proposes an approach for applying gans to nmt. we build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. the generator aims to generate sentences which are hard to be discriminated from humantranslated sentences ( i.e., the golden target sentences); and the discriminator makes efforts to discriminate the machinegenerated sentences from humantranslated ones. the two sub models play a minimax game and achieve the winwin situation when they reach a nash equilibrium. additionally, the static sentencelevel bleu is utilized as the reinforced objective for the generator, which biases the generation towards high bleu points. during training, both the dynamic discriminator and the static bleu objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. experimental results show that the proposed model consistently outperforms the traditional rnnsearch and the newly emerged stateoftheart transformer on englishgerman and chineseenglish translation tasks.	4	7
topic-based coherence modeling for statistical machine translation	coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. in this paper, we propose topicbased coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. we automatically extract a coherence chain for each source text to be translated. based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. we build two topicbased coherence models on the predicted target coherence chain 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. we integrate the two models into a stateoftheart phrasebased machine translation system. experiments on largescale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	2	10
learning to parse and translate improves neural machine translation	reacttext 343 in typical neural machine translation~(nmt), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of rnn. in this paper, we propose a new type of decoder for nmt, which splits the decode state into two parts and updates them in two different timescales. specifically, we first predict a chunk timescale state for phrasal modeling, on top of...  /reacttext  reacttext 344   /reacttext [show full ]	4	7
a survey of word reordering in statistical machine translation: computational models and language phenomena	word reordering is one of the most difficult aspects of statistical machine translation (smt), and an important factor of its quality and efficiency. despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials.to orient the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. the survey describes in detail how word reordering is modeled within different stringbased and treebased smt frameworks and as a standalone task, including systematic overviews of the literature in advanced reordering modeling.we then question why some approaches are more successful than others in different language pairs. we argue that besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. to this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. empirical results in the smt literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the smt framework that best suits them.	3	7
can machine translation systems be evaluated by the crowd alone	crowdsourced assessments of machine translation quality allow evaluations to be carried out cheaply and on a large scale. it is essential, however, that the crowd's work be filtered to avoid contamination of results through the inclusion of false assessments. one method is to filter via agreement with experts, but even amongst experts agreement levels may not be high. in this paper, we present a new methodology for crowdsourcing human assessments of translation quality, which allows individual workers to develop their own individual assessment strategy. agreement with experts is no longer required, and a worker is deemed reliable if they are consistent relative to their own previous work. individual translations are assessed in isolation from all others in the form of direct estimates of translation quality. this allows more meaningful statistics to be computed for systems and enables significance to be determined on smaller sets of assessments. we demonstrate the methodology's feasibility in largescale human evaluation through replication of the human evaluation component of workshop on statistical machine translation shared translation task for two language pairs, spanishtoenglish and englishtospanish. results for measurement based solely on crowdsourced assessments show system rankings in line with those of the original evaluation. comparison of results produced by the relative preference approach and the direct estimate method described here demonstrate that the direct estimate method has a substantially increased ability to identify significant differences between translation systems.	2	9
temporal attention model for neural machine translation	attentionbased neural machine translation (nmt) models suffer from attention deficiency issues as has been observed in recent research. we propose a novel mechanism to address some of these limitations and improve the nmt attention. specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. we compare our approach against the baseline nmt model and two other related approaches that address this issue either explicitly or implicitly. largescale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related nmt approaches. our model further outperforms strong smt baselines in some settings even without using ensembles.	3	7
response-based learning for grounded machine translation	we propose a novel learning approach for statistical machine translation (smt) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. we show how to generate responses by grounding smt in the task of executing a semantic parse of a translated query against a database. experiments on the geoquery database show an improvement of about 6 points in f1score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. in general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for smt.	1	11
topic-based coherence modeling for statistical machine translation	coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. in this paper, we propose topicbased coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. we automatically extract a coherence chain for each source text to be translated. based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. we build two topicbased coherence models on the predicted target coherence chain 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. we integrate the two models into a stateoftheart phrasebased machine translation system. experiments on largescale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	2	9
oxlm: a neural language modelling framework for machine translation	this paper presents an open source implementation1 of a neural language model for machine translation. neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. the language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. neural language models are notoriously slow to train and test. our framework is designed with scalability in mind and provides two optional techniques for reducing the computational cost the socalled class decomposition trick and a training algorithm based on noise contrastive estimation. our models may be extended to incorporate direct ngram features to learn weights for every ngram in the training data. our framework comes with wrappers for the cdec and moses translation toolkits, allowing our language models to be incorporated as normalized features in their decoders (inside the beam search).	1	11
sentence level dialect identification for machine translation system selection	reacttext 526 while modern standard arabic (msa) has many resources, arabic dialects, the primarily spoken local varieties of arabic, are quite impoverished in this regard. in this article, we present adam (analyzer for dialectal arabic morphology). adam is a poor man’s solution to quickly develop morphological analyzers for dialectal arabic. adam has roughly half the outofvocabulary rate of a...  /reacttext  reacttext 527   /reacttext [show full ]	1	11
involving language professionals in the evaluation of machine translation	significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. while automatic evaluation and scoring mechanisms such as bleu have enabled the fast development of systems, it is not clear how systems can meet realworld (quality) requirements in industrial translation scenarios today. tara x u project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. in a consortium of research and industry partners, taraxu project integrates human translators into the development process for rating and postediting of machine translation outputs collecting feedback for possible improvements.	1	11
modelling and optimizing on syntactic n-grams for statistical machine translation	the role of language models in smt is to promote fluent translation output, but traditional ngram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with stringlevel gaps. syntactic language models have the potential to fill this modelling gap. we propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order. it is trainable with neural networks, and not only improves over standard ngram language models, but also outperforms related syntactic language models. we empirically demonstrate its effectiveness in terms of perplexity and as a feature function in stringtotree smt from english to german and russian. we also show that using a syntactic evaluation metric to tune the loglinear parameters of an smt system further increases translation quality when coupled with a syntactic language model.	2	9
what’s in a domain? analyzing genre and topic differences in statistical machine translation	reacttext 371 as larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? this challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains....  /reacttext  reacttext 372   /reacttext [show full ]	2	9
non-projective dependency-based pre-reordering with recurrent neural network for machine translation	the quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. we propose a class of recurrent neural models which exploit sourceside dependency syntax features to reorder the words into a targetlike order. we evaluate these models on the germantoenglish and italiantoenglish language pairs, showing significant improvements over a phrasebased moses baseline. we also compare with state of the art germantoenglish prereordering rules, showing that our method obtains similar or better results.	2	9
pushdown automata in statistical machine translation	2014 association for computational linguistics.this article describes the use of pushdown automata (pda) in the context of statistical machine translation and alignment under a synchronous contextfree grammar. we use pdas to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence. generalpurpose pda algorithms for replacement, composition, shortest path, and expansion are presented. we describe hipdt, a hierarchical phrasebased decoder using the pda representation and these algorithms.we contrast the complexity of this decoder with a decoder based on a finite state automata representation, showing that pdas provide a more suitable framework to achieve exact decoding for larger synchronous contextfree grammars and smaller language models. we assess this experimentally on a largescale chinesetoenglish alignment and translation task. in translation, we propose a twopass decoding strategy involving a weaker language model in the firstpass to address the results of pda complexity analysis. we study in depth the experimental conditions and tradeoffs in which hipdt can achieve stateoftheart performance for largescale smt.	1	10
automatic spelling correction for machine translation	methods, systems, and apparatus, including computer program products, for correcting spelling in text. a text input is received for translation. one or more suspect words in the text input are identified. for each suspect word, one or more candidate words are identified. a score for the text input and scores for each of one or more candidate inputs are determined, where each candidate input is the text input with one or more of the suspect words each replaced by a respective candidate word. if any, a candidate input whose score is highest among the scores for the candidate inputs and is greater than the text input score by at least a threshold is selected. otherwise, the text input is selected. a translation of a selected candidate input or the selected text input is provided as the translation of the text input.	1	10
evaluation of machine translation systems at cls corporate language services ag	this paper describes the evaluation of machine translation (mt) system for use in a large company. to take into account the specific requirements of such an environment, a pragmatic approach for the evaluation was developed. it consists of five steps ranging from a specification of the evaluation process to the integration of the chosen mt system in a given infrastructure. the process includes a specification of mt evaluation criteria relevant to systems which have to be employed for a large customer base. the paper also shows the results of such an evaluation study which was recently carried out at cls corporate language services ag, where comprendium is in the meantime being employed as corporate mt system.	1	10
controlling politeness in neural machine translation via side constraints	many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). in machine translation from a language without honorifics such as english, it is difficult to...	3	6
models and inference for prefix-constrained machine translation	reacttext 435 many phrase alignment models operate over the combinatorial space of bijective phrase alignments. we prove that finding an optimal alignment in this space is nphard, while com puting alignment expectations is #phard. on the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to viterbi...  /reacttext  reacttext 436   /reacttext [show full ]	3	6
statistical machine translation in the translation curriculum: overcoming obstacles and empowering translators	in this paper we argue that the time is ripe for translator educators to engage with statistical machine translation (smt) in more profound ways than they have done to date. we explain the basic principles of smt and reflect on the role of humans in smt workflows. against a background of diverging opinions on the latter, we argue for a holistic approach to the integration of smt into translator training programmes, one that empowers rather than marginalises translators. we discuss potential barriers to the use of smt by translators generally and in translator training in particular, and propose some solutions to problems thus identified. more specifically, cloudbased services are proposed as a means of overcoming some of the technical and ethical challenges posed by more advanced uses of smt in the classroom. ultimately the paper aims to pave the way for the design and implementation of a new translatororiented smt syllabus at our own university and elsewhere.	1	10
rationalization: a neural machine translation approach to generating natural language explanations	we introduce ai rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. we describe a rationalization technique that uses neural machine translation to translate internal stateaction representations of the autonomous agent into natural language. we evaluate our technique in the frogger game environment. the natural language is collected from human players thinking out loud as they play the game. we motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.	4	6
online adaptation to post-edits for phrase-based statistical machine translation	recent research has shown that accuracy and speed of human translators can benefit from postediting output of machine translation systems, with larger benefits for higher quality output.	1	10
incorporating global visual features into attention-based neural machine translation	we introduce multimodal, attentionbased neural machine translation (nmt) models which incorporate visual features into different parts of both the encoder and the decoder. we utilise global image features extracted using a pretrained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. in our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. we also study the impact that adding synthetic multimodal, multilingual data brings and find that the additional data have a positive impact on multimodal models. we report new stateoftheart results and our best models also significantly improve on a comparable phrasebased statistical mt (pbsmt) model trained on the multi30k data set according to all metrics evaluated. to the best of our knowledge, it is the first time a purely neural model significantly improves over a pbsmt model on all metrics evaluated on this data set.	4	6
the new thot toolkit for fully-automatic and interactive statistical machine translation	we present the new thot toolkit for fully automatic and interactive statistical ma chine translation (smt). initial public ver sions of thot date back to 2005 and did only include estimation of phrasebased models. by contrast, the new version of fers several new features that had not been previously incorporated. the key innova tions provided by the toolkit are computer aided translation, including postediting and interactive smt, incremental learn ing and robust generation of alignments at phrase level. in addition to this, the toolkit also provides standard smt fea tures such as fullyautomatic translation, scalable and parallel algorithms for model training, clientserver implementation of the translation functionality, etc. the toolkit can be compiled in unixlike and windows platforms and it is released un der the gnu lesser general public li cense (lgpl).	1	10
bridging neural machine translation and bilingual dictionaries	neural machine translation (nmt) has become the new stateoftheart in several language pairs. however, it remains a challenging problem how to integrate nmt with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. in this paper, we propose two methods to bridge nmt and the bilingual dictionaries. the core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that nmt can distil latent bilingual mappings from the ample and repetitive phenomena. one method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.	3	6
compression of neural machine translation models via pruning	neural machine translation (nmt), like many other deep learning domains, typically suffers from overparameterization, resulting in large storage sizes. this paper examines three simple magnitudebased pruning schemes to compress nmt models, namely classblind, classuniform, and classdistribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the nmt architecture. we demonstrate the efficacy of weight pruning as a compression technique for a stateoftheart nmt system. we show that an nmt model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the wmt'14 englishgerman translation task. this sheds light on the distribution of redundancy in the nmt architecture. our main result is that with retraining, we can recover and even surpass the original performance with an 80%pruned model.	3	6
memory-enhanced decoder for neural machine translation	we propose to enhance the rnn decoder in a neural machine translator (nmt) with external memory, as a natural but powerful extension to the state in the decoding rnn. this memoryenhanced rnn decoder is called \textsc{memdec}. at each time during decoding, \textsc{memdec} will read from this memory and write to this memory once, both with contentbased addressing. unlike the unbounded memory in previous work\cite{rnnsearch} to store the representation of source sentence, the memory in \textsc{memdec} is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step. our empirical study on chineseenglish translation shows that it can improve by $4.8$ bleu upon groundhog and $5.3$ bleu upon on moses, yielding the best performance achieved with the same training set.	3	6
hume: human ucca-based evaluation of machine translation	human evaluation of machine translation normally uses sentencelevel measures such as relative ranking or adequacy scales. however, these provide no insight into possible errors, and do not scale well with sentence length. we argue for a semanticsbased evaluation, which captures what meaning components are retained in the mt output, thus providing a more finegrained analysis of translation quality, and enabling the construction and tuning of semanticsbased mt. we present a novel human semantic evaluation measure, human uccabased mt evaluation (hume), building on the ucca semantic representation scheme. hume covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled mt output. we experiment with four language pairs, demonstrating hume's broad applicability, and report good interannotator agreement rates and correlation with human adequacy scores.	3	6
toward multilingual neural machine translation with universal encoder and decoder	in this paper, we present our first attempts in building a multilingual neural machine translation framework under a unified approach. we are then able to employ attentionbased nmt for manytomany multilingual translation tasks. our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. our approach has shown its effectiveness in an underresourced translation scenario with considerable improvements up to 2.6 bleu points. in addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.	3	6
zero-resource machine translation by multimodal encoder–decoder network with multimedia pivot	we propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. using multimedia as the pivot, we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. this modalityagnostic representation is the key to bridging the gap between different modalities. putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. notably, in the testing phase, we need only source language texts as the input for translation. in experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. we compared and investigated several possible implementations and found that an endtoend model that simultaneously optimized both rank loss in multimodal encoders and crossentropy loss in decoders performed the best.	3	6
an evaluation methodology for english to sinhala machine translation	this paper presents evaluation methodology for english to sinhala machine tran slation system. the english to sinhala machine translation system has been developed by using multi agent approach and powered through the concept of “varanegeema”. translation system works through the communication among nine agents namely english morphological analyzer agent, english parser agent, english to sinhala base word translator agent, sinhala morphological generator agent, sinhala parser agent, transliteration agent, intermediate editor agent, message space agent and request agent. the evaluation was conducted through three steps. as the first step, evaluation was conducted through the white box testing approach and tested each module in the machine translation system through the developed testing tools. then, evaluated the system performance and calculated the error rate through the result of the evaluation test bed. finally, intelligibility and the accuracy test will be conducted through the human support. the experimental result shows 89% accuracy of the overall system and 7.2% word error rate and the 5.4% sentence error rate. details of the evaluation and results are given in the paper	3	6
a survey of word reordering in statistical machine translation: computational models and language phenomena	2016. a survey of word reordering in statistical machine translation computational models and language phenomena. computational linguistics, 42.arianna bisazza and marcello federico. 2016. a survey of word reordering in statistical machine ...	3	6
how grammatical is character-level neural machine translation? assessing mt quality with contrastive translation pairs	analysing translation quality in regards to specific linguistic phenomena has historically been difficult and timeconsuming. neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well nmt systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. the core idea is that we measure whether a reference translation is more probable under a nmt model than a contrastive translation which introduces a specific type of error. we present lingeval97, a largescale data set of 97000 contrastive translation pairs based on the wmt english>german translation task, with errors automatically created with simple rules. we report results for a number of systems, and find that recently introduced characterlevel nmt systems perform better at transliteration than models with bytepair encoding (bpe) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.	3	6
learning local word reorderings for hierarchical phrase-based statistical machine translation	statistical models for reordering source words have been used to enhance hierarchical phrasebased statistical machine translation. there are existing wordreordering models that learn reorderings for.	3	6
context gates for neural machine translation	in neural machine translation (nmt), generation of a target word depends on both source and target contexts. we find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. due to the lack of effective control over the influence from source and target contexts, conventional nmt tends to yield fluent but inadequate translations. to address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. in this way, we can enhance both the adequacy and fluency of nmt with more careful control of the information flow from contexts. experiments show that our approach significantly improves upon a standard attentionbased nmt system by +2.3 bleu points.	4	6
six challenges for neural machine translation	we explore six challenges for neural machine translation domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. we show both deficiencies and improvements over the quality of phrasebased statistical machine translation.	4	6
interactive attention for neural machine translation	conventional attentionbased neural machine translation (nmt) conducts dynamic alignment in generating the target sentence. by repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (bahdanau et al., 2015), the attention mechanism has greatly enhanced stateoftheart nmt. in this paper, we propose a new attention mechanism, called interactive attention, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. interactive attention can keep track of the interaction history and therefore improve the translation performance. experiments on nist chineseenglish translation task show that interactive attention can achieve significant improvements over both the previous attentionbased nmt baseline and some stateoftheart variants of attentionbased nmt (i.e., coverage models (tu et al., 2016)). and neural machine translator with our interactive attention can outperform the open source attentionbased nmt system groundhog by 4.22 bleu points and the open source phrasebased system moses by 3.94 bleu points averagely on multiple test sets.	3	6
real time adaptive machine translation for post-editing with cdec and transcenter	as sentences are translated, the models gain valuable context infor mation, allowing them toadapt to the specific tar  on line learning of loglinear weights in interactive ma chine translation online adaptation strategies for statistical machine translation in post editing scenarios	1	9
using syntax-based machine translation to parse english into abstract meaning representation	we present a parser for  meaning representation (amr). we treat englishtoamr conversion within the framework of stringtotree, syntaxbased machine translation (sbmt). to make this work, we transform the amr structure into a form suitable for the mechanics of sbmt and useful for modeling. we introduce an amrspecific language model and add data and features drawn from semantic resources. our resulting amr parser improves upon stateoftheart results by 7 smatch points.	2	8
enhancing statistical machine translation with bilingual terminology in a cat environment	in this paper, we address the problem of extracting and integrating bilingual terminology into a statistical machine translation (smt) system for a computer aided translation (cat) tool scenario. we develop a framework that, taking as input a small amount of parallel indomain data, gathers domainspecific bilingual terms and injects them in an smt system to enhance the translation productivity. therefore, we investigate several strategies to extract and align bilingual terminology, and to embed it into the smt. we compare two embedding methods that can be easily used at runtime without altering the normal activity of an smt system xml markup and the cachebased model. we tested our framework on two different domains showing improvements up to 15% bleu score points.	1	9
bilingual sentiment consistency for statistical machine translation	in this paper, we explore bilingual sentiment knowledge for statistical machine translation (smt). we propose to explicitly model the consistency of sentiment between the source and target side with a lexiconbased approach. the experiments show that the proposed model significantly improves chinesetoenglish nist translation over a competitive baseline.	1	9
dependency-based pre-ordering for chinese-english machine translation	in statistical machine translation (smt), syntaxbased preordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders. this paper introduces a novel preordering approach based on dependency parsing for chineseenglish smt. we present a set of dependencybased preordering rules which improved the bleu score by 1.61 on the nist 2006 evaluation data. we also investigate the accuracy of the rule set by conducting human evaluations.	1	9
optimizing instance selection for statistical machine translation with feature decay algorithms	we introduce fda5 for efficient parameterization, optimization, and implementation of feature decay algorithms (fda), a class of instance selection algorithms that use feature decay. fda increase the diversity of the selected training set by devaluing features (i.e., ngrams) that have already been included. fda5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with five parameters. we present optimization techniques that allow fda5 to adapt these functions to indomain and outofdomain translation tasks for different language pairs. in a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. in machine translation experiments performed on the 2 million sentence englishgerman section of the europarl corpus, we show that a subset of the training set selected by fda5 can gain up to 3.22 bleu points compared to a randomly selected subset of the same size, can gain up to 0.41 bleu points compared to using all of the available training data using only 15% of it, and can reach within 0.5 bleu points to the full training set result by using only 2.7% of the full training data.	2	8