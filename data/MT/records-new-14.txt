Neural Machine Translation of Rare Words with Subword Units	Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.	Machine Translation/Subword Units	2016	169	84.5
Neural Machine Translation by Jointly Learning to Align and Translate	Abstract:  Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.	Machine Translation	2014	1890	472.5
On the Properties of Neural Machine Translation: Encoder-Decoder Approaches	Abstract:  Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.	Machine Translation	2014	467	116.75
Findings of the 2014 Workshop on Statistical Machine Translation	This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries	Statistical Machine Translation	2014	391	97.75
Findings of the 2014 Workshop on Statistical Machine Translation	This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a sepa- rate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This ye...	Statistical Machine Translation	2014	195	48.75
Effective Approaches to Attention-based Neural Machine Translation	Abstract:  An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.	Machine Translation	2015	301	100.3333333333
Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation	"Abstract:  Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."	Machine Translation/Translation System	2016	223	111.5
On Using Very Large Target Vocabulary for Neural Machine Translation	Abstract:  Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method that allows us to use a very large target vocabulary without increasing training complexity, based on importance sampling. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English->German translation and almost as high performance as state-of-the-art English->French translation system.	Machine Translation	2014	188	47
Fast and Robust Neural Network Joint Models for Statistical Machine Translation	Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380, Baltimore, Maryland, USA, June 23-25 2014. cO2014 Association for Computational LinguisticsFast and Robust Neural Network Joint Models for Statistical Machine TranslationJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA {jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com AbstractRecent work	Statistical Machine Translation/Neural Network/Joint Models	2014	234	58.5
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation	Abstract:  In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.	Statistical Machine Translation	2014	1174	293.5
Modeling Coverage for Neural Machine Translation	Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attention-based NMT ignores past alignment information, which often leads to over-translation and under-translation. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which guides NMT to consider more about the untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over traditional attention-based NMT.	Machine Translation	2016	53	26.5
Minimum Risk Training for Neural Machine Translation	Abstract:  We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.	Machine Translation/Minimum Risk	2015	70	23.3333333333
Findings of the 2016 Conference on Machine Translation	This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT qual- ity), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 in- stitutions (plus 36 anonymized online sys- tems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments).	Machine Translation	2016	60	30
Addressing the Rare Word Problem in Neural Machine Translation	Abstract:  Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task.	Machine Translation/Word Problem/domestic animals	2014	156	39
Findings of the 2015 Workshop on Statistical Machine Translation	This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic post- editing task had a total of 4 teams, submitting 7 entries.	Statistical Machine Translation	2015	133	44.3333333333
Optimizing Chinese Word Segmentation for Machine Translation Performance	Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmen...	Machine Translation/Word Segmentation	2016	56	28
Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism	We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.	Machine Translation/Attention Mechanism	2016	60	30
Improved statistical machine translation for resource-poor languages using related resource-rich languages	"We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X_1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X_1-Y and a larger bi-text for X_2-Y for some resource-rich language X_2 that is closely related to X_1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X_1 and X_2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian- >English using Malay and for Spanish -> English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary ""real training data by a factor of 2--5."	cost evaluation/Machine Translation/convergence test	2014	82	20.5
A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation	Abstract:  The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.	Machine Translation	2016	86	43
A Character-level Decoder without Explicit Segmentation for Neural Machine Translation	Abstract The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.	Machine Translation	2016	55	27.5
Is machine translation ready yet?	"The default option of the Google Translator Toolkit (GTT), released in June 2009, is to ""pre-fill with machine translation"" all segments for which a 'no match' has been returned by the memories, while the window clearly advises that ""[m]ost users should not modify this"". To confirm whether this approach indeed benefits translators and translation quality, we designed and performed tests whereby trainee translators used the GTT to translate passages from English into Chinese either entirely from the source text, or after seeding of empty segments by the Google Translate engine as recommended. The translations were timed, and their quality assessed by independent experienced markers following Australian NAATI test criteria. Our results show that, while time differences were not significant, the machine translation seeded passages were more favourably assessed by the markers in thirty three of fifty six cases. This indicates that, at least for certain tasks and language combinations — and against the received wisdom of translation professionals and translator trainers — translating by proofreading machine translation may be advantageous. L'option par défaut de la (GTT), publiée en juin 2009, prévoit de tous les segments pour lesquels un a été retourné par les mémoires de traduction. Afin de vérifier si une telle approche est en effet susceptible de profiter au traducteur/à la qualité de la traduction, nous avons con04u et réalisé des tests dans lesquels des traducteurs en formation ont utilisé la GTT dans des traductions de l'anglais en le chinois, soit à partir du seul texte source, soit à partir de segments vides sélectionnés par la GTT selon les instructions prévues. Les traductions ont été chronométrées, et leur qualité évaluée par des marqueurs indépendants expérimentés, suivant les critères d'essai australiens (NAATI). Nos résultats montrent que, alors que les différences temporelles ne sont pas significatives, la traduction automatique sélective par segments a été évaluée de manière plus favorable dans trente-trois cas sur le total des cinquante-six. Ce qui indique que, au moins dans des t09ches particulières et pour certaines combinaisons de langue données, traduire avec le secours de la relecture par traduction automatique peut être avantageux."	MACHINE TRANSLATION/TRANSLATION MEMORY/TRANSLATION QUALITY	2016	43	21.5
Neural Machine Translation in Linear Time	Abstract:  We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.	Machine Translation/Linear Time	2016	41	20.5
Character-based Neural Machine Translation	We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.	Machine Translation	2015	70	23.3333333333
Character-based Neural Machine Translation	Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affixaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.	Machine Translation	2016	47	23.5
Edinburgh Neural Machine Translation Systems for WMT 16	Abstract:  We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: EnglishCzech, EnglishGerman, EnglishRomanian and EnglishRussian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.	Machine Translation	2016	40	20
Modeling Coverage for Neural Machine Translation	"Li, ""Modeling coverage for neural machine translation,"" CoRR, vol. abs/1601.04811, 2016. [Online]. Available: http://arxiv.org/abs/1601.04811Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling ..."	Machine Translation	2016	42	21
On Using Monolingual Corpora in Neural Machine Translation	Abstract:  Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.	Machine Translation	2015	58	19.3333333333
Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism	(2016). Multi-way, multilingual neural machine translation with a shared attention mechanism. In Proc. NAACL.Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with...	Machine Translation/Attention Mechanism	2016	40	20
Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation	Abstract:  We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\rightarrow$French and surpasses state-of-the-art results for English$\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\rightarrow$English and German$\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.	Translation System	2016	49	24.5
Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models	Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The two-fold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new state-of-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.	Machine Translation	2016	51	25.5
Machine translation of languages&nbsp;	No abstract is available for this article.	Machine translation	2014	49	12.25
Character-based Neural Machine Translation	Abstract Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English WMT task.	Machine Translation	2016	33	16.5
Improving Neural Machine Translation Models with Monolingual Data	Abstract:  Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task EnglishGerman (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.	Machine Translation	2015	53	17.6666666667
Minimum Risk Training for Neural Machine Translation	Abstract We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.	Machine Translation/Minimum Risk	2016	39	19.5
Computerized statistical machine translation with phrasal decoder	A computerized system for performing statistical machine translation with a phrasal decoder is provided. The system may include a phrasal decoder trained prior to run-time on a monolingual parallel corpus, the monolingual parallel corpus including a machine translation output of source language documents of a bilingual parallel corpus and a corresponding target human translation output of the source language documents, to thereby learn mappings between the machine translation output and the target human translation output. The system may further include a statistical machine translation engine configured to receive a translation input and to produce a raw machine translation output, at run-time. The phrasal decoder may be configured to process the raw machine translation output, and to produce a corrected translation output based on the learned mappings for display on a display associated with the system.	machine translation	2015	44	14.6666666667
Neural versus Phrase-Based Machine Translation Quality: a Case Study	Abstract:  Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.	Case Study/Translation Quality	2016	37	18.5
Neural versus Phrase-Based Machine Translation Quality: a Case Study	react-text: 270 Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a...  /react-text  react-text: 271   /react-text [Show full abstract]	Case Study/Translation Quality	2016	34	17
Knowledge-Based Question Answering as Machine Translation	Abstract A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.	Machine Translation/Question Answering	2014	49	12.25
Bilingually-constrained Phrase Embeddings for Machine Translation	We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn phrase embeddings (compact vector representations for phrases), which can distinguish the phrases in different semantic meanings. The BRAE is trained with the objective to minimize the semantic distance of translation equivalents and maximize the semantic distance of nontranslation pairs. The learned model can embed any phrase semantically in two languages and can transform semantic space in one language to the other. We evaluate the BRAE on two end-to-end SMT tasks (phrase table pruning and translation hypotheses reranking) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is spectacularly successful in these two tasks.	Machine Translation/heading time/spikelet number	2014	58	14.5
Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis	Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts. In recent years, due to the growth in the quantity and fast spreading of user-generated contents online and the impact such information has on events, people and companies worldwide, this task has been approached in an important body of research in the field. Despite different methods having been proposed for distinct types of text, the research community has concentrated less on developing methods for languages other than English. In the above-mentioned context, the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less/no resources are available for this task when compared to English, stressing upon the impact of translation quality on the sentiment classification performance. Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can, in combination to appropriate machine learning algorithms and carefully chosen features, be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.	Machine translation/Supervised Learning/Opinion mining	2014	48	12
Dual Learning for Machine Translation	Abstract:  While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.	Machine Translation	2016	31	15.5
Lium Smt Machine Translation System For Wmt 2010	This paper describes the development of French--English and English--French machine translation systems for the 2010 WMT shared task evaluation. These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data. We first collected bilingual data by performing automatic translations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.	Translation System	2017	29	29
Nematus: a Toolkit for Neural Machine Translation	Abstract:  We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.	Machine Translation	2017	29	29
OpenNMT: Open-Source Toolkit for Neural Machine Translation	Abstract:  We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.	Machine Translation	2017	31	31
Linguistic Input Features Improve Neural Machine Translation	Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to EnglishGerman, and English->Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An open-source implementation of our neural MT system is available, as are sample files and configurations.	Machine Translation	2016	31	15.5
Bilingually-constrained Phrase Embeddings for Machine Translation	中国科学院机构知识库(中国科学院机构知识库网格（CAS IR GRID）)以发展机构知识能力和知识管理能力为目标，快速实现对本机构知识资产的收集、长期保存、合理传播利用，积极建设对知识内容进行捕获、转化、传播、利用和审计的能力，逐步建设包括知识内容分析、关系分析和能力审计在内的知识服务能力，开展综合知识管理。	Machine Translation/heading time/spikelet number	2014	46	11.5
Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models	react-text: 428 In this paper, we describe FBK's neural machine translation (NMT) systems submitted at the International Workshop on Spoken Language Translation (IWSLT) 2016. The systems are based on the state-of-the-art NMT architecture that is equipped with a bi-directional encoder and an attention mechanism in the decoder. They leverage linguistic information such as lemmas and part-of-speech tags of the...  /react-text  react-text: 429   /react-text [Show full abstract]	Machine Translation	2016	29	14.5
A Recursive Recurrent Neural Network for Statistical Machine Translation	In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof- the-art baseline by about 1.5 points in BLEU.	Statistical Machine Translation/Recurrent Neural Network	2014	46	11.5
Integrating an Unsupervised Transliteration Model into Statistical Machine Translation	ABSTRACT  We investigate three methods for integrat-ing an unsupervised transliteration model into an end-to-end SMT system. We in-duce a transliteration model from parallel data and use it to translate OOV words. Our approach is fully unsupervised and language independent. In the methods to integrate transliterations, we observed improvements from 0.23-0.75 (62 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora.	Statistical Machine Translation	2014	43	10.75
Integrating an Unsupervised Transliteration Model into Statistical Machine Translation	Reduced mechanical stress to bone in bedridden patients and astronauts leads to bone loss and increase in fracture risk which is one of the major medical and health issues in modern aging society and space medicine. However, no molecule involved in the mechanisms underlying this phenomenon has been identified to date. Osteopontin (OPN) is one of the major noncollagenous proteins in bone matrix, but its function in mediating physical-force effects on bone in vivo has not been known. To investigate the possible requirement for OPN in the transduction of mechanical signaling in bone metabolism in vivo, we examined the effect of unloading on the bones of OPN 61/ 61 mice using a tail suspension model. In contrast to the tail suspension–induced bone loss in wild-type mice, OPN 61/ 61 mice did not lose bone. Elevation of urinary deoxypyridinoline levels due to unloading was observed in wild-type but not in OPN 61/ 61 mice. Analysis of the mechanisms of OPN deficiency–dependent reduction in bone on the cellular basis resulted in two unexpected findings. First, osteoclasts, which were increased by unloading in wild-type mice, were not increased by tail suspension in OPN 61/ 61 mice. Second, measures of osteoblastic bone formation, which were decreased in wild-type mice by unloading, were not altered in OPN 61/ 61 mice. These observations indicate that the presence of OPN is a prerequisite for the activation of osteoclastic bone resorption and for the reduction in osteoblastic bone formation in unloaded mice. Thus, OPN is a molecule required for the bone loss induced by mechanical stress that regulates the functions of osteoblasts and osteoclasts.	Amino Acids/Statistical Machine Translation/Bone Resorption	2014	43	10.75
Nematus: a Toolkit for Neural Machine Translation	react-text: 452 Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than...  /react-text  react-text: 453   /react-text [Show full abstract]	Machine Translation	2017	26	26
Tree-to-Sequence Attentional Neural Machine Translation	Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.	Machine Translation	2016	25	12.5
Coverage Embedding Models for Neural Machine Translation	In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.	Machine Translation	2016	27	13.5
Semantics, Discourse and Statistical Machine Translation	2014. Discourse in Statistical Machine Translation. Ph.D. thesis, Uppsala Univer- sity, Department of Linguistics and Philology, Upp- sala, Sweden.Christian Hardmeier. Discourse in Statistical Machine Translation, volume 15 of Studia ...	Statistical Machine Translation	2014	36	9
Coverage Embedding Models for Neural Machine Translation	Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. Coverage Embedding Models for Neural Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing. Association f...	Machine Translation	2016	23	11.5
Linguistic Input Features Improve Neural Machine Translation	Abstract Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to EnglishGerman neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3.	Machine Translation	2016	26	13
Fully Character-Level Neural Machine Translation without Explicit Segmentation	Abstract:  Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.	Machine Translation	2017	28	28
Incorporating Discrete Translation Lexicons into Neural Machine Translation	Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.	Machine Translation	2016	24	12
Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation	Abstract:  Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.	Machine Translation	2016	24	12
Encoding Source Language with Convolutional Neural Network for Machine Translation	Abstract:  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average	Machine Translation/Source Language	2015	32	10.6666666667
Machine translation instant messaging applications	An instant messaging translation plug-in interacts with an instant messaging program to intercept incoming messages and forward these messages to a language translation service. The plug-in then displays a translation received from the service along with the original message. This provides translation which can be used by instant messaging users to communicate across language barriers, and without local translation or knowledge of the internal workings of the translation services used. Additionally, the translation plug-in also provides for manual translation of messages, which allows communication with users who use a different language but do not use the translation plug-in. Messages are modified before translation in order to correct spelling, to prevent particular words or phrases from being translated, and to change instant messaging language into standard language form. The techniques can be performed on various messaging services, including instant messaging on computers or mobile devices, as well as SMS.	Machine translation/instant messaging	2014	34	8.5
Montreal Neural Machine Translation Systems for WMT’15	react-text: 555 We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine...  /react-text  react-text: 556   /react-text [Show full abstract]	Machine Translation	2015	30	10
Syntactically Guided Neural Machine Translation	We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores...	Machine Translation	2016	23	11.5
Vocabulary Manipulation for Neural Machine Translation	In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).	Machine Translation	2016	20	10
Semi-Supervised Learning for Neural Machine Translation	While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.	Semi-Supervised Learning/Machine Translation	2016	20	10
A Coverage Embedding Model for Neural Machine Translation	In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.	Machine Translation	2016	23	11.5
Transfer Learning for Low-Resource Neural Machine Translation	Abstract:  The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.	Machine Translation/Transfer Learning	2016	21	10.5
Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions	In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.	Case Study/Machine Translation	2016	20	10
Proceedings of the Ninth Workshop on Statistical Machine Translation	react-text: 446 Personalize machine translation to faithfully preserve original text & author characteristics and to adapt for the preferences of the reader of the translation.  /react-text  react-text: 447   /react-text	Statistical Machine Translation	2014	33	8.25
Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation	Abstract Using machine translation output as a starting point for human translation has become an increasingly common application of MT.We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step. Individually, these techniques can substantially improve MT quality, even over strong baselines. Moreover, we see super-Additive improvements when all three techniques are used in tandem.	Statistical Machine Translation	2014	30	7.5
Customizable machine translation service	Embodiments of the present invention provide a system and method for providing a translation service. The method comprises providing a translation interface accessible via a network. The translation interface receives specialized data associated with a domain from a member. A text string written in a source language is received from the member via the translation interface. A domain-based translation engine is selected. The domain-based translation engine may be associated with a source language, a target language, and a domain. The text string is translated into the target language using, at least in part, the selected domain-based translation engine. The translated text string is transmitted to the member via the Internet. In some embodiments, a translation memory is generated based on the specialized data.	machine translation	2014	25	6.25
Optimizing parameters for machine translation	Methods, systems, and apparatus, including computer program products, for language translation are disclosed. In one implementation, a method is provided. The method includes accessing a hypothesis space, where the hypothesis space represents a plurality of candidate translations; performing decoding on the hypothesis space to obtain a translation hypothesis that minimizes an expected error in classification calculated relative to an evidence space; and providing the obtained translation hypothesis for use by a user as a suggested translation in a target translation.	machine translation	2014	27	6.75
Supervised Attentions for Neural Machine Translation	Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016. Supervised attentions for neural machine translation. In Proceedings of EMNLP.Mi, H., Wang, Z., Ge, N., Ittycheriah, A.: Supervised attentions for neural machine translation. ...	Machine Translation	2016	17	8.5
Supervised Attentions for Neural Machine Translation	"In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the ""true"" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system."	Machine Translation	2016	17	8.5
Edinburgh's Phrase-based Machine Translation Systems for WMT-14	ABSTRACT  This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.		2014	26	6.5
A Shared Task on Multimodal Machine Translation and Crosslingual Image Description	This paper introduces and summarises the findings of a new shared task at the in- tersection of Natural Language Process- ing and Computer Vision: the generation of image descriptions in a target language, given an image and/or one or more de- scriptions in a different (source) language. This challenge was organised along with the Conference on Machine Translation (WMT16), and called for system submis- sions for two task variants: (i) a transla- tion task, in which a source language im- age description needs to be translated to a target language, (optionally) with addi- tional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image. In this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams.	Machine Translation/Image Description	2016	19	9.5
On the Elements of an Accurate Tree-to-String Machine Translation System	While tree-to-string (T2S) translation theoretically holds promise for efficient, accurate translation, in previous reports T2S systems have often proven inferior to other machine translation (MT) methods such as phrase-based or hierarchical phrase-based MT. In this paper, we attempt to clarify the reason for this performance gap by investigating a number of peripheral elements that affect the accuracy of T2S systems, including parsing, alignment, and search. Based on detailed experiments on the English-Japanese and Japanese- English pairs, we show how a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof- the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems.	Machine Translation System	2014	24	6
Zero-Resource Translation with Multi-Lingual Neural Machine Translation	In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.	Machine Translation	2016	17	8.5
Mutual Information and Diverse Decoding Improve Neural Machine Translation	Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, both mechanisms offer a consistent performance boost on both standard LSTM and attention-based neural MT architectures. The result is the best published performance for a single (non-ensemble) neural MT system, as well as the potential application of our diverse decoding algorithm to other NLP re-ranking tasks.	Machine Translation/Mutual Information	2016	17	8.5
Agreement-based joint training for bidirectional attention-based neural machine translation	The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently,our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on Chinese-English and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.	Machine Translation	2016	19	9.5
SYSTRAN's Pure Neural Machine Translation Systems	"Abstract:  Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work. Our ultimate goal is to share our expertise to build competitive production systems for ""generic"" translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems."	Machine Translation	2016	14	7
Semi-Supervised Learning for Neural Machine Translation	Abstract While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.	Semi-Supervised Learning/Machine Translation	2016	14	7
Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions	react-text: 471 This paper presents a machine translation tool – based on Moses – developed for the International Maritime Organization (IMO) for the automatic translation of documents from Spanish, French, Rus-sian and Arabic to/from English. The main challenge lies in the insufficient size of in-house corpora (especially for Russian and Arabic). The United Nations (UN) granted IMO the right to use UN...  /react-text  react-text: 472   /react-text [Show full abstract]	Case Study/Machine Translation	2016	15	7.5
Hybrid Simplification using Deep Semantics and Machine Translation	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	Machine Translation	2015	21	7
A Sense-Based Translation Model for Statistical Machine Translation	ABSTRACT  The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation.	Statistical Machine Translation/Translation Model	2014	23	5.75
Zero-Resource Translation with Multi-Lingual Neural Machine Translation	"react-text: 452 Researchers in Computer Science discipline put a great effort for developing machine intelligence techniques, inspired from human intelligence and to cultivate elegant mathematical tools to design …""  /react-text  react-text: 453   /react-text [more]"	Machine Translation	2016	14	7
Collecting and Using Comparable Corpora for Statistical Machine Translation	Lack of sufficient parallel data for many languages and domains is currently one of the major obstacles to further advancement of automated translation. The ACCURAT project is addressing this issue by researching methods how to improve machine translation systems by using comparable corpora. In this paper we present tools and techniques developed in the ACCURAT project that allow additional data needed for statistical machine translation to be extracted from comparable corpora. We present methods and tools for acquisition of comparable corpora from the Web and other sources, for evaluation of the comparability of collected corpora, for multi-level alignment of comparable corpora and for extraction of lexical and terminological data for machine translation. Finally, we present initial evaluation results on the utility of collected corpora in domain-adapted machine translation and real-life applications.	machine translation/Statistical Machine Translation	2015	20	6.6666666667
Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015	This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words.	Machine Translation/Subjective Quality	2015	21	7
UM-Corpus: a large English-Chinese parallel corpus for statistical machine translation	Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems,especially for Statistical Machine Translation (SMT). However, most existing parallel corpora to Chinese are subject to in-house use,while others are domain specific and limited in size. To a certain degree, this limits the SMT research. This paper describes the acquisitionof a large scale and high quality parallel corpora for English and Chinese. The corpora constructed in this paper contain about 15 millionEnglish-Chinese (E-C) parallel sentences, and more than 2 million training data and 5,000 testing sentences are made publicly available.Different from previous work, the corpus is designed to embrace eight different domains. Some of them are further categorized intodifferent topics. The corpus will be released to the research community, which is available at the NLP 2 CT 1 website.	Statistical Machine Translation/Parallel Corpus	2014	22	5.5
Combination of Stochastic Understanding and Machine Translation Systems for Language Portability of Dialogue Systems	In this paper, several approaches for language portability of dialogue systems are investigated with a focus on the spoken language understanding (SLU) component. We show that the use of statistical machine translation (SMT) can greatly reduce the time and cost of porting an existing system from a source to a target language. Using automatically translated training data we study phrase-based machine translation as an alternative to conditional random fields for conceptual decoding to compensate for the loss of a precise concept-word alignment. Also two ways to increase SLU robustness to translation errors (smeared training data and translation post editing) are shown to improve performance when test data are translated then decoded in the source language. Overall the combination of all these approaches allows to reduce even further the concept error rate. Experiments were carried out on the French MEDIA dialogue corpus with a subset manually translated into Italian.	Machine Translation/word processing	2014	22	5.5
Improved Neural Machine Translation with SMT Features	Neural machine translation (NMT) conducts end-to-end translation with a source language encoder and a target language decoder, making promising translation performance. However, as a newly emerged approach, the method has some limitation...	Machine Translation	2016	14	7
Improving evaluation of machine translation quality estimation	ABSTRACT  Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during comparison of quality estimation prediction score distributions and gold label distributions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components ofWMT-13 andWMT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks.	Machine Translation/Quality Estimation	2015	18	6
Attention-based Multimodal Neural Machine Translation	react-text: 467 In this work, synthesis of facial animation is done by modelling the mapping between facial motion and speech using the shared Gaussian process latent variable model. Both data are processed separately and subsequently coupled together to yield a shared latent space. This method allows coarticulation to be modelled by having a dynamical model on the latent space. Synthesis of novel animation...  /react-text  react-text: 468   /react-text [Show full abstract]	Machine Translation	2016	13	6.5
Towards zero unknown word in neural machine translation	Neural Machine translation has shown promising results in recent years. In order to control the com- putational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vo- cabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In sub- stitution step, the rare words in a testing sen- tence are replaced with similar in-vocabulary words based on a similarity model learnt from monolin- gual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. Exper- iments on Chinese-to-English translation demon- strate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.	machine translation	2016	14	7
Edinburgh’s Phrase-based Machine Translation Systems for WMT-14.	ABSTRACT This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.		2014	20	5
Accurate Evaluation of Segment-level Machine Translation Metrics	Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2015. Accurate evaluation of segment-level machine translation metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics ...	Machine Translation/Accurate Evaluation	2015	18	6
Hybrid Simplification using Deep Semantics and Machine Translation	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.	Machine Translation	2014	21	5.25
Guided Alignment Training for Topic-Aware Neural Machine Translation	In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.	Machine Translation	2016	13	6.5
Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation	Abstract:  The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.	Machine Translation/Sentence Length/Automatic Segmentation	2014	21	5.25
Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)	Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.	Source Code/Machine Translation/Using Statistical	2015	19	6.3333333333
Arabic machine translation: a survey	Although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. Thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. We discuss some of the linguistic characteristics of the Arabic language. Features of Arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. This paper summarizes the major techniques used in machine translation from Arabic into English, and discusses their strengths and weaknesses.	machine translation	2014	19	4.75
Neural Machine Translation with Reconstruction	Abstract:  Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems.	Machine Translation	2016	12	6
Fast Domain Adaptation for Neural Machine Translation	Neural Machine Translation (NMT) is a new approach for automatic translation of text from one human language into another. The basic concept in NMT is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is gaining popularity in the research community because it outperformed traditional SMT approaches in several translation tasks at WMT and other evaluation tasks/benchmarks at least for some language pairs. However, many of the enhancements in SMT over the years have not been incorporated into the NMT framework. In this paper, we focus on one such enhancement namely domain adaptation. We propose an approach for adapting a NMT system to a new domain. The main idea behind domain adaptation is that the availability of large out-of-domain training data and a small in-domain training data. We report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs. With our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly. In addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the NMT model on the combined data which usually takes several days/weeks depending on the volume of the data.	Machine Translation/Domain Adaptation	2016	13	6.5
Massive Exploration of Neural Machine Translation Architectures	Abstract:  Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.	Machine Translation	2017	13	13
Vocabulary Selection Strategies for Neural Machine Translation	Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.	Machine Translation	2016	12	6
Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2014	In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech - English, Vietnamese - English, French - English and German - English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality.	Machine Translation/English Speech	2015	17	5.6666666667
Proceedings of the 15th Annual Conference of the European Association for Machine Translation	The blood coagulation mechanism consists of a series of concatenated chemical reactions, governed by the coagulation factors present in the blood plasma, after the activation of the clot mechanism. The last reaction corresponds to the fibrinogen conversion into fibrin, followed by the fibrin polymerisation and production of a stable fibrin network. During the clotting process, there is a sol-gel transformation of the medium. The subject of the present paper is the measurement of the ultrasonic attenuation coefficient for human blood plasma during the coagulation process, in the frequency range of 8 to 22 MHz. The clot was obtained after the procedure to measure the prothrombin time (6512 s): mixing 150 μL of reconstituted lyophilised normal plasma with 300 μL of reconstituted lyophilised thromboplastin immersed in a water bath with the temperature controlled at 36.5°C. The attenuation coefficient for pure plasma remained constant within the measurement period of 10 s and at frequencies of 8, 9, 10, 15, 20, 21 and 22 MHz. On the other hand, there is a detectable time-decay of the attenuation coefficient for samples of plasma going through the coagulation process and at frequencies of 8, 9, 10 and 15 MHz. The time-decay becomes less and less detectable as the frequency increases and it becomes completely undetectable at 20, 21 and 22 MHz. (E-mail: jcm@peb.ufrj.br )	Annual Conference/Machine Translation	2014	19	4.75
Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing	Abstract This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model can be used to control for higher faithfulness with regard to the to-be-corrected machine translation input. Our submission outperforms the uncorrected baseline on the unseen test set by -3.2% TER and +5.5% BLEU.	Machine Translation	2016	12	6
Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing	Abstract:  This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2\% TER and +5.5\% BLEU and outperforms any other system submitted to the shared-task by a large margin.	Machine Translation	2016	12	6
Selection and use of nonstatistical translation components in a statistical machine translation framework	A system with a nonstatistical translation component integrated with a statistical translation component engine. The same corpus may be used for training the statistical engine and also for determining when to use the statistical engine and when to use the translation component. This training may use probabilistic techniques. Both the statistical engine and the translation components may be capable of translating the same information, however the system determines which component to use based on the training. Retraining can be carried out to add additional components, or when after additional translator training.	machine translation	2014	19	4.75
Arabic machine translation: a survey	Although there is no machine learning technique that fully meets human requirements, finding a quick and efficient translation mechanism has become an urgent necessity, due to the differences between the languages spoken in the world鈥檚 communities and the vast development that has occurred worldwide, as each technique demonstrates its own advantages and disadvantages. Thus, the purpose of this paper is to shed light on some of the techniques that employ machine translation available in literature, to encourage researchers to study these techniques. We discuss some of the linguistic characteristics of the Arabic language. Features of Arabic that are related to machine translation are discussed in detail, along with possible difficulties that they might present. This paper summarizes the major techniques used in machine translation from Arabic into English, and discusses their strengths and weaknesses.	machine translation	2014	18	4.5
Factored Neural Machine Translation	We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. The final translation is built using some \textit{a priori} linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.	Machine Translation	2016	11	5.5
Variational Neural Machine Translation	Abstract:  Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.	Machine Translation	2016	11	5.5
Jane: Open Source Machine Translation System Combination	Different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they yield. System combination is a method for combining the output of ...	Machine Translation/Open Source/System Combination	2014	16	4
Using Discourse Structure Improves Machine Translation Evaluation	ABSTRACT  We present experiments in using dis-course structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment-and at the system-level. Rather than proposing a single new metric, we show that discourse information is com-plementary to the state-of-the-art evalu-ation metrics, and thus should be taken into account in the development of future richer evaluation metrics.	Discourse Structure/Machine Translation Evaluation	2014	16	4
Maximal Lattice Overlap in Example-Based Machine Translation	Example-Based Machine Translation (EBMT) retrieves pre-translated phrases from a sentence-aligned bilingual training corpus to translate new input sentences. EBMT uses long pre-translated phrases effectively but is subject to disfluencies at phrasal translation boundaries. We address this problem by introducing a novel method that exploits overlapping phrasal translations and the increased confidence in translation accuracy they imply. We specify an efficient algorithm for producing translations using overlap. Finally, our empirical analysis indicates that this approach produces higher quality translations than the standard method of EBMT in a peak-to-peak comparison.	Example-Based Machine Translation	2017	11	11
Efficient Top-Down BTG Parsing for Machine Translation Preordering	2015. Efficient top-down BTG parsing for machine translation preordering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (...	Machine Translation	2015	15	5
Exploiting Source-side Monolingual Data in Neural Machine Translation	Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the source- side monolingual data in NMT. The first ap- proach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach ap- plies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods ob- tain significant improvements over the strong attention-based NMT.	Machine Translation	2016	11	5.5
Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation	Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to con- struct efficient large LM is an important topic in SMT. However, most of the ex- isting LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can im- prove both the perplexity score for LM e- valuation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.	Statistical Machine Translation/Neural Network/Language Model	2014	16	4
Don’t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation	react-text: 434 We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a...  /react-text  react-text: 435   /react-text [Show full abstract]		2014	17	4.25
Neural Machine Translation with Supervised Attention	The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.	Machine Translation	2016	10	5
Coverage-based Neural Machine Translation	However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.	Machine Translation	2016	10	5
Jane: Open Source Machine Translation System Combination.	Markus Freitag, Matthias Huck and Hermann Ney. (2014). Jane: Open Source Machine Translation Sys- tem Combination. In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational ...	Machine Translation/Open Source/System Combination	2014	16	4
Edinburgh's Statistical Machine Translation Systems for WMT16	This paper describes the University of Ed- inburgh's phrase-based and syntax-based submissions to the shared translation tasks of the ACL 2016 First Conference on Ma- chine Translation (WMT16). We sub- mitted five phrase-based and five syntax- based systems for the news task, plus one phrase-based system for the biomedical task.	Machine Translation	2016	10	5
Pragmatic Neural Language Modelling in Machine Translation	This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.	Machine Translation	2015	14	4.6666666667
Incorporating pronoun function into statistical machine translation	Pronouns are used frequently in language, and perform a range of functions. Some pronouns are used to express coreference, and others are not. Languages and genres differ in how and when they use pronouns and this poses a problem for Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Novák, 2011; Guillou, 2012; Weiner, 2014; Hardmeier, 2014). Attention to date has focussed on coreferential (anaphoric) pronouns with NP antecedents, which when translated from English into a language with grammatical gender, must agree with the translation of the head of the antecedent. Despite growing attention to this problem, little progress has been made, and little attention has been given to other pronouns. The central claim of this thesis is that pronouns performing different functions in text should be handled differently by SMT systems and when evaluating pronoun translation. This motivates the introduction of a new framework to categorise pronouns according to their function: Anaphoric/cataphoric reference, event reference, extra-textual reference, pleonastic, addressee reference, speaker reference, generic reference, or other function. Labelling pronouns according to their function also helps to resolve instances of functional ambiguity arising from the same pronoun in the source language having multiple functions, each with different translation requirements in the target language. The categorisation framework is used in corpus annotation, corpus analysis, SMT system development and evaluation. I have directed the annotation and conducted analyses of a parallel corpus of English-German texts called ParCor (Guillou et al., 2014), in which pronouns are manually annotated according to their function. This provides a first step toward understanding the problems that SMT systems face when translating pronouns. In the thesis, I show how analysis of manual translation can prove useful in identifying and understanding systematic differences in pronoun use between two languages and can help inform the design of SMT systems. In particular, the analysis revealed that the German translations in ParCor contain more anaphoric and pleonastic pronouns than their English originals, reflecting differences in pronoun use. This raises a particular problem for the evaluation of pronoun translation. Automatic evaluation methods that rely on reference translations to assess pronoun translation, will not be able to provide an adequate evaluation when the reference translation departs from the original source-language text. I also show how analysis of the output of state-of-the-art SMT systems can reveal how well current systems perform in translating different types of pronouns and indicate where future efforts would be best directed. The analysis revealed that biases in the training data, for example arising from the use of “it” and “es” as both anaphoric and pleonastic pronouns in both English and German, is a problem that SMT systems must overcome. SMT systems also need to disambiguate the function of those pronouns with ambiguous surface forms so that each pronoun may be translated in an appropriate way. To demonstrate the value of this work, I have developed an automated post-editing system in which automated tools are used to construct ParCor-style annotations over the source-language pronouns. The annotations are then used to resolve functional ambiguity for the pronoun “it” with separate rules applied to the output of a baseline SMT system for anaphoric vs. non-anaphoric instances. The system was submitted to the DiscoMT 2015 shared task on pronoun translation for English-French. As with all other participating systems, the automatic post-editing system failed to beat a simple phrase-based baseline. A detailed analysis, including an oracle experiment in which manual annotation replaces the automated tools, was conducted to discover the causes of poor system performance. The analysis revealed that the design of the rules and their strict application to the SMT output are the biggest factors in the failure of the system. The lack of automatic evaluation metrics for pronoun translation is a limiting factor in SMT system development. To alleviate this problem, Christian Hardmeier and I have developed a testing regimen called PROTEST comprising (1) a hand-selected set of pronoun tokens categorised according to the different problems that SMT systems face and (2) an automated evaluation script. Pronoun translations can then be automatically compared against a reference translation, with mismatches referred for manual evaluation. The automatic evaluation was applied to the output of systems submitted to the DiscoMT 2015 shared task on pronoun translation. This again highlighted the weakness of the post-editing system, which performs poorly due to its focus on producing gendered pronoun translations, and its inability to distinguish between pleonastic and event reference pronouns.	statistical machine translation	2016	10	5
Proceedings of the Second Workshop on Statistical Machine Translation	ABSTRACT  The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical ma-chine translation systems to a special do-main (here: news commentary), when most of the training data is from a dif-ferent domain (here: European Parliament speeches). This paper also gives a descrip-tion of the submission of the University of Edinburgh to the shared task.	Statistical Machine Translation/language barrier	2015	13	4.3333333333
ReVal: A Simple and Effective Machine Translation Evaluation Metric Based on Recurrent Neural Networks	..	Machine Translation	2015	13	4.3333333333
Assessing the Discourse Factors that Influence the Quality of Machine Translation	Abstract We present a study of aspects of discourse structure - specifically discourse devices used to organize information in a sentence- that significantly impact the quality of machine translation. Our analysis is based on manual evaluations of translations of news from Chinese and Arabic to English. We find that there is a particularly strong mismatch in the notion of what constitutes a sentence in Chinese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to employ multiple explicit discourse connectives (because, but, etc.), as well as the presence of ambiguous discourse connectives in the English translation. Furthermore, the mismatches between discourse expressions across languages significantly impact translation quality.	Machine Translation	2014	14	3.5
An empirical analysis of data selection techniques in statistical machine translation.	[EN] Domain adaptation has recently gained interest in statistical machine translation. One of the adaptation techniques is based in the selection data. Data selection aims to select the best subset of the bilingual sentences from an available pool of sentences, with which to train a SMT system. In this paper, we study how affect the bilingual corpora used for the data selection methods in the translation quality	Statistical Machine Translation/aspirin induced asthma	2015	14	4.6666666667
Neural Machine Translation with External Phrase Memory	In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.	Machine Translation	2016	9	4.5
Syntax-aware Neural Machine Translation Using CCG	Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.	Machine Translation	2017	9	9
Embedding Word Similarity with Neural Machine Translation	Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness.	Machine Translation/Word Similarity	2014	14	3.5
A Convolutional Encoder Model for Neural Machine Translation	Abstract:  The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.	Machine Translation	2016	9	4.5
Standardizing Tweets with Character-Level Machine Translation	Summary: This paper presents the results of the standardization procedure of Slovene tweets that are full of colloquial, dialectal and foreign-language elements. With the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient out-of-vocabulary (OOV) tokens and used it to train a character-level statistical machine translation system (CSMT). Best results were obtained by combining the manually constructed lexicon and CSMT as fallback with an overall improvement of 9.9\% increase on all tokens and 31.3\% on OOV tokens. Manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. Finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1\% per-token accuracy with the former and 83.6\% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	Machine Translation	2014	14	3.5
Standardizing Tweets with Character-Level Machine Translation	This paper presents the results of the standardization procedure of Slovene tweets that are full of colloquial, dialectal and foreign-language elements. With the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient out-of-vocabulary (OOV) tokens and used it to train a character-level statistical machine translation system (CSMT). Best results were obtained by combining the manually constructed lexicon and CSMT as fallback with an overall improvement of 9.9% increase on all tokens and 31.3% on OOV tokens. Manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. Finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1% per-token accuracy with the former and 83.6% with the latter, thus demonstrating that standardization has significant benefits for upstream processing.	machine translation	2014	14	3.5
Adaptation of machine translation for multilingual information retrieval in the medical domain	We investigate machine translation (MT) of user search queries in the context of cross-lingual information retrieval (IR) in the medical domain. The main focus is on techniques to adapt MT to increase translation quality; however, we also explore MT adaptation to improve effectiveness of cross-lingual IR. Our MT system is Moses, a state-of-the-art phrase-based statistical machine translation system. The IR system is based on the BM25 retrieval model implemented in the Lucene search engine. The MT techniques employed in this work include in-domain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. The IR methods include morphological normalization and using multiple translation variants for query expansion. The experiments are performed and thoroughly evaluated on three language pairs: Czech–English, German–English, and French–English. MT quality is evaluated on data sets created within the Khresmoi project and IR effectiveness is tested on the CLEF eHealth 2013 data sets. The search query translation results achieved in our experiments are outstanding – our systems outperform not only our strong baselines, but also Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech–English, from 23.03 to 40.82 for German–English, and from 32.67 to 40.82 for French–English. This is a 55% improvement on average. In terms of the IR performance on this particular test collection, a significant improvement over the baseline is achieved only for French–English. For Czech–English and German–English, the increased MT quality does not lead to better IR results. Most of the MT techniques employed in our experiments improve MT of medical search queries. Especially the intelligent training data selection proves to be very successful for domain adaptation of MT. Certain improvements are also obtained from German compound splitting on the source language side. Translation quality, however, does not appear to correlate with the IR performance – better translation does not necessarily yield better retrieval. We discuss in detail the contribution of the individual techniques and state-of-the-art features and provide future research directions.	machine translation/Statistical machine translation	2014	14	3.5
Learning to Parse and Translate Improves Neural Machine Translation	Abstract:  There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.	Machine Translation	2017	9	9
MTE-NN at SemEval-2016 Task 3: Can Machine Translation Evaluation Help Community Question Answering?	react-text: 456 Medical Machine Translation for Doctor-Patient Communication in Qatar  /react-text  react-text: 457   /react-text	Question Answering/Translation Evaluation	2016	9	4.5
An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation	"In this paper, we propose a novel domain adaptation method named ""mixed fine tuning"" for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."	Machine Translation/Domain Adaptation	2017	9	9
Machine Translation: Mining Text for Social Theory	More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online trans	Machine Translation/Cell Division/Culture Media/Cell Membrane	2016	8	4
Neural Machine Translation with Recurrent Attention Modeling	Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.	Machine Translation	2016	8	4
Neural Machine Translation Advised by Statistical Machine Translation	Abstract:  Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu et al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.	Machine Translation/Statistical Machine Translation	2016	9	4.5
Towards String-to-Tree Neural Machine Translation	We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.	Machine Translation	2017	9	9
Doubly-Attentive Decoder for Multi-modal Neural Machine Translation	We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.	Machine Translation	2017	8	8
NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems	In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017.	Machine Translation	2017	8	8
Context-dependent word representation for neural machine translation	We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En–Fr and En–De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.		2017	8	8
Systems and methods for tuning parameters in statistical machine translation	A method for tuning translation parameters in statistical machine translation based on ranking of the translation parameters is disclosed. According to one embodiment, the method includes sampling pairs of candidate translation units from a set of candidate translation units corresponding to a source unit, each candidate translation unit corresponding to numeric values assigned to one or more features, receiving an initial weighting value for each feature, comparing the pairs of candidate translation units to produce binary results, and using the binary results to adjust the initial weighting values to produce modified weighting values.	tuning parameters/statistical machine translation	2014	13	3.25
Bilingual Continuous-Space Language Model Growing for Statistical Machine Translation	Larger n-gram language models (LMs) perform better in statistical machine translation (SMT). However, the existing approaches have two main drawbacks for constructing larger LMs: 1) it is not convenient to obtain larger corpora in the same domain as the bilingual parallel corpora in SMT; 2) most of the previous studies focus on monolingual information from the target corpora only, and redundant n-grams have not been fully utilized in SMT. Nowadays, continuous-space language model (CSLM), especially neural network language model (NNLM), has been shown great improvement in the estimation accuracies of the probabilities for predicting the target words. However, most of these CSLM and NNLM approaches still consider monolingual information only or require additional corpus. In this paper, we propose a novel neural network based bilingual LM growing method. Compared to the existing approaches, the proposed method enables us to use bilingual parallel corpus for LM growing in SMT. The results show that our new method outperforms the existing approaches on both SMT performance and computational efficiency significantly.	Language Model/Statistical Machine Translation	2015	11	3.6666666667
The Operation Sequence Model – Combining N-Gram-Based and Phrase-Based Statistical Machine Translation	In this article, we present a novel machine translation model, the Operation Sequence Model (OSM), which combines the benefits of phrase-based and N-gram-based statistical machine translation (SMT) and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption, and (iv) avoids the spurious phrasal segmentation problem. As in phrase-based SMT, themodel (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically, and (iii) decodes with large translation units during search. The unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM.	Statistical Machine Translation/Operation Sequence	2015	12	4
Machine translation for query expansion	Methods, systems and apparatus, including computer program products, for expanding search queries. One method includes receiving a search query, selecting a synonym of a term in the search query based on a context of occurrence of the term in the received search query, the synonym having been derived from statistical machine translation of the term, and expanding the received search query with the synonym and using the expanded search query to search a collection of documents. Alternatively, another method includes receiving a request to search a corpus of documents, the request specifying a search query, using statistical machine translation to translate the specified search query into an expanded search query, the specified search query and the expanded search query being in the same natural language, and in response to the request, using the expanded search query to search a collection of documents.	Machine translation/query expansion	2015	11	3.6666666667
EU-BRIDGE MT: Combined Machine Translation	Abstract This paper describes one of the col-laborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two Euro-pean language pairs, German→English and English→German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combina-tion. RWTH Aachen University, the Uni-versity of Edinburgh, and Karlsruhe In-stitute of Technology developed several individual systems which serve as sys-tem combination input. We devoted spe-cial attention to building syntax-based sys-tems and combining them with the phrase-based ones. The joint setups yield em-pirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT news-test2013 test set compared to the best sin-gle systems.	Machine Translation	2014	12	3
Submodularity for Data Selection in Machine Translation	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT). By explicitly formulating data selection as a submodular program, we ob- tain fast scalable selection algorithms with mathematical performance guarantees, re- sulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. We present a new class of submodular functions designed specifically for SMT and evaluate them on two differ- ent translation tasks. Our results show that our best submodular method significantly outperforms several baseline methods, including the widely-used cross-entropy based data selection method. In addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing.	Statistical Machine Translation/Data Selection	2014	12	3
Statistical machine translation enhancements through linguistic levels: A survey	Machine translation can be considered a highly interdisciplinary and multidisciplinary field because it is approached from the point of view of human trans...	machine translation/statistical machine translation	2014	12	3
The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT~2015	This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year鈥檚 evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions.Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features.	Machine Translation	2015	11	3.6666666667
The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015	This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared transla- tion task of the EMNLP 2015 Tenth Work- shop on Statistical Machine Translation (WMT 2015). We set up phrase-based sta- tistical machine translation systems for all ten language pairs of this year's evaluation campaign, which are English paired with Czech, Finnish, French, German, and Rus- sian in both translation directions.	Machine Translation	2015	11	3.6666666667
Phrasal: A Toolkit for New Directions in Statistical Machine Translation	We present a new version of Phrasal, an open-source toolkit for statistical phrase-based machine translation. This revision includes features that support emerging re-search trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive ma-chine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1	Statistical Machine Translation	2014	12	3
Using Joint Models for Domain Adaptation in Statistical Machine Translation	ABSTRACT  Joint models have recently shown to improve the state-of-the-art in machine translation (MT). We apply EM-based mixture modeling and data selection techniques using two joint models, namely the Operation Sequence Model or OSM — an ngram-based translation and reordering model, and the Neural Network Joint Model or NNJM — a continuous space translation model, to carry out domain adaptation for MT. The diversity of the two models, OSM with inherit reordering information and NNJM with continuous space modeling makes them interesting to be explored for this task. Our contribution in this paper is fusing the existing known techniques (linear interpolation, cross-entropy) with the state-of-the-art MT models (OSM, NNJM). On a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points.	Statistical Machine Translation/Domain Adaptation	2015	11	3.6666666667
An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation	Scalable discriminative training methods are now broadly available for estimating phrase-based, feature-rich translation mod-els. However, the sparse feature sets typi-cally appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities: they often overfit, do not gen-eralize, or require complex and slow fea-ture extractors. This paper introduces ex-tended features, which are more specific than dense features yet more general than lexicalized sparse features. Large-scale ex-periments show that extended features yield robust BLEU gains for both Arabic-English (+1.05) and Chinese-English (+0.67) rel-ative to a strong feature-rich baseline. We also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and language-independent tools for implementing the features. 1	machine translation	2014	12	3
Manawi: Using Multi-Word Expressions and Named Entities to Improve Machine Translation	Publication » Manawi: Using Multi-Word Expressions and Named Entities to Improve Machine Translation.	Named Entities/Machine Translation	2014	12	3
Towards String-To-Tree Neural Machine Translation	Goldberg. Towards string-to-tree neural machine translation. In ACL, 2017.Roee Aharoni and Yoav Goldberg. 2017. Towards String-to-Tree Neural Machine Translation. ArXiv e-prints.Aharoni and Goldberg 2017] Aharoni, R., and Goldberg, ...	Machine Translation	2017	7	7
Migrating code with statistical machine translation	Publication » Migrating code with statistical machine translation.	Statistical Machine Translation	2014	12	3
Machine Translation Post-editing and Effort. Empirical Studies on the Post-editing Process	Abstract This dissertation investigates the practice of machine translation post-editing and the various aspects of effort involved in post-editing work. Through analyses of edits made by post-editors, the work described here examines three main questions: 1) what types of machine translation errors or source text features cause particular effort in post-editing, 2) what types of errors can or cannot be corrected without the help of the source text, and 3) how different indicators of effort vary between different post-editors. The dissertation consists of six previously published articles, and an introductory summary. Five of the articles report original research, and involve analyses of post-editing data to examine questions related to post-editing effort as well as differences between post-editors. The sixth article is a survey presenting an overview of the research literature. The research reported is based on multiple datasets consisting of machine translations and their post-edited versions, as well as process and evaluation data related to post-editing effort. The dissertation presents a mixed methods study combining qualitative and quantitative approaches, as well as theoretical and analytical tools from the fields of language technology and translation studies. Data on edits performed by post-editors, post-editing time, keylogging data, and subjective evaluations of effort are combined with error analyses of the machine translations in question, and compared for various post-editors. The results of this dissertation provide evidence that, in addition to the number of edits performed, post-editing effort is affected by the type of edits as well as source text features. Secondly, the results show that while certain language errors can be corrected even without access to the source text, certain other types that more severely affect the meaning cannot. Thirdly, the results show that post-editors' speed and the amount of editing they perform differ, and that various profiles can be identified in terms of how the edits are planned and carried out by the post-editors. The results of this work may have both theoretical and practical implications for the measurement and estimation of post-editing effort.	Machine Translation	2016	7	3.5
Latest trends in hybrid machine translation and its applications ☆	This survey provides a detailed overview of the modification of the standard rule-based architecture to include statistical knowledge, the introduction of rules in corpus-based approaches, and the hybridization of approaches within this last single category. The principal aim here is to cover the leading research and progress in this field of MT and in several related applications.	Latest trends/machine translation	2015	10	3.3333333333
An evaluation of machine translation for multilingual sentence-level sentiment analysis	Sentiment analysis has become a key tool for several social media applications, including analysis of user's opinions about products and services, support to politics during campaigns and even for market trending. There are multiple existing sentiment analysis methods that explore different techniques, usually relying on lexical resources or learning approaches. Despite the large interest on this theme and amount of research efforts in the field, almost all existing methods are designed to work with only English content. Most existing strategies in specific languages consist of adapting existing lexical resources, without presenting proper validations and basic baseline comparisons. In this paper, we take a different step into this field. We focus on evaluating existing efforts proposed to do language specific sentiment analysis. To do it, we evaluated twenty-one methods for sentence-level sentiment analysis proposed for English, comparing them with two language-specific methods. Based on nine language-specific datasets, we provide an extensive quantitative analysis of existing multi-language approaches. Our main result suggests that simply translating the input text on a specific language to English and then using one of the existing English methods can be better than the existing language specific efforts evaluated. We also rank those implementations comparing their prediction performance and identifying the methods that acquired the best results using machine translation across different languages. As a final contribution to the research community, we release our codes and datasets. We hope our effort can help sentiment analysis to become English independent.	machine translation	2016	7	3.5
An evaluation of machine translation for multilingual sentence-level sentiment analysis	Sentiment analysis has become a key tool for several social media applications, including analysis of user's opinions about products and services, support to politics during campaigns and even for market trending. There are multiple existing sentiment analysis methods that explore different techniques, usually relying on lexical resources or learning approaches. Despite the large interest on this theme and amount of research efforts in the field, almost all existing methods are designed to work with only English content. Most existing strategies in specific languages consist of adapting existing lexical resources, without presenting proper validations and basic baseline comparisons. In this paper, we take a different step into this field. We focus on evaluating existing efforts proposed to do language specific sentiment analysis. To do it, we evaluated twenty-one methods for sentence-level sentiment analysis proposed for English, comparing them with two language-specific methods. Based on nine language-specific datasets, we provide an extensive quantitative analysis of existing multi-language approaches. Our main result suggests that simply translating the input text on a specific language to English and then using one of the existing English methods can be better than the existing language specific efforts evaluated. We also rank those implementations comparing their prediction performance and identifying the methods that acquired the best results using machine translation across different languages. As a final contribution to the research community, we release our codes and datasets. We hope our effort can help sentiment analysis to become English independent.	machine translation	2016	7	3.5
Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets	Abstract:  This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.	Machine Translation	2017	7	7
Topic-Based Coherence Modeling for Statistical Machine Translation	Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	Statistical Machine Translation/Text coherence/topic modeling	2015	10	3.3333333333
Learning to Parse and Translate Improves Neural Machine Translation	react-text: 343 In typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of...  /react-text  react-text: 344   /react-text [Show full abstract]	Machine Translation	2017	7	7
A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena	Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency. Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs. Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials.To orient the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon. The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling.We then question why some approaches are more successful than others in different language pairs. We argue that besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair. To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge. Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.	Machine Translation	2016	7	3.5
Can machine translation systems be evaluated by the crowd alone	Crowd-sourced assessments of machine translation quality allow evaluations to be carried out cheaply and on a large scale. It is essential, however, that the crowd's work be filtered to avoid contamination of results through the inclusion of false assessments. One method is to filter via agreement with experts, but even amongst experts agreement levels may not be high. In this paper, we present a new methodology for crowd-sourcing human assessments of translation quality, which allows individual workers to develop their own individual assessment strategy. Agreement with experts is no longer required, and a worker is deemed reliable if they are consistent relative to their own previous work. Individual translations are assessed in isolation from all others in the form of direct estimates of translation quality. This allows more meaningful statistics to be computed for systems and enables significance to be determined on smaller sets of assessments. We demonstrate the methodology's feasibility in large-scale human evaluation through replication of the human evaluation component of Workshop on Statistical Machine Translation shared translation task for two language pairs, Spanish-to-English and English-to-Spanish. Results for measurement based solely on crowd-sourced assessments show system rankings in line with those of the original evaluation. Comparison of results produced by the relative preference approach and the direct estimate method described here demonstrate that the direct estimate method has a substantially increased ability to identify significant differences between translation systems.	machine translation	2015	9	3
Temporal Attention Model for Neural Machine Translation	Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles.	Machine Translation/Attention Model	2016	7	3.5
Response-based Learning for Grounded Machine Translation	Abstract We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database. Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.	Machine Translation	2014	11	2.75
Topic-based coherence modeling for statistical machine translation	Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.	statistical machine translation/text coherence/topic modeling	2015	9	3
OxLM: A Neural Language Modelling Framework for Machine Translation	This paper presents an open source implementation1 of a neural language model for machine translation. Neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. The language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. Neural language models are notoriously slow to train and test. Our framework is designed with scalability in mind and provides two optional techniques for reducing the computational cost: the so-called class decomposition trick and a training algorithm based on noise contrastive estimation. Our models may be extended to incorporate direct n-gram features to learn weights for every n-gram in the training data. Our framework comes with wrappers for the cdec and Moses translation toolkits, allowing our language models to be incorporated as normalized features in their decoders (inside the beam search).	Machine Translation	2014	11	2.75
Sentence Level Dialect Identification for Machine Translation System Selection	react-text: 526 While Modern Standard Arabic (MSA) has many resources, Arabic Dialects, the primarily spoken local varieties of Arabic, are quite impoverished in this regard. In this article, we present ADAM (Analyzer for Dialectal Arabic Morphology). ADAM is a poor man’s solution to quickly develop morphological analyzers for dialectal Arabic. ADAM has roughly half the out-of-vocabulary rate of a...  /react-text  react-text: 527   /react-text [Show full abstract]	Machine Translation/Sentence Level/System Selection	2014	11	2.75
Involving language professionals in the evaluation of machine translation	Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. TARA X U project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. In a consortium of research and industry partners, TARAXU project integrates human translators into the development process for rating and post-editing of machine translation outputs collecting feedback for possible improvements.	Machine translation/Error analysis	2014	11	2.75
Modelling and optimizing on syntactic n-grams for statistical machine translation	The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps. Syntactic language models have the potential to fill this modelling gap. We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order. It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models. We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian. We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.	Statistical Machine Translation	2015	9	3
What’s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation	react-text: 371 As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains....  /react-text  react-text: 372   /react-text [Show full abstract]		2015	9	3
Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation	Abstract The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-To-English and Italian-To-English language pairs, showing significant improvements over a phrasebased Moses baseline. We also compare with state of the art German-To-English pre-reordering rules, showing that our method obtains similar or better results.	Machine Translation/Recurrent Neural Network	2015	9	3
Pushdown automata in statistical machine translation	漏 2014 Association for Computational Linguistics.This article describes the use of pushdown automata (PDA) in the context of statistical machine translation and alignment under a synchronous context-free grammar. We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence. General-purpose PDA algorithms for replacement, composition, shortest path, and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the PDA representation and these algorithms.We contrast the complexity of this decoder with a decoder based on a finite state automata representation, showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context-free grammars and smaller language models. We assess this experimentally on a large-scale Chinese-to-English alignment and translation task. In translation, we propose a two-pass decoding strategy involving a weaker language model in the first-pass to address the results of PDA complexity analysis. We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art performance for large-scale SMT.	Statistical Machine Translation/Pushdown Automata	2014	10	2.5
Automatic spelling correction for machine translation	Methods, systems, and apparatus, including computer program products, for correcting spelling in text. A text input is received for translation. One or more suspect words in the text input are identified. For each suspect word, one or more candidate words are identified. A score for the text input and scores for each of one or more candidate inputs are determined, where each candidate input is the text input with one or more of the suspect words each replaced by a respective candidate word. If any, a candidate input whose score is highest among the scores for the candidate inputs and is greater than the text input score by at least a threshold is selected. Otherwise, the text input is selected. A translation of a selected candidate input or the selected text input is provided as the translation of the text input.	machine translation	2014	10	2.5
Evaluation of machine translation systems at CLS Corporate Language Services AG	ABSTRACT  This paper describes the evaluation of Machine Translation (MT) System for use in a large company. To take into account the specific requirements of such an environment, a pragmatic approach for the evaluation was developed. It consists of five steps ranging from a specification of the evaluation process to the integration of the chosen MT system in a given infrastructure. The process includes a specification of MT evaluation criteria relevant to systems which have to be employed for a large customer base. The paper also shows the results of such an evaluation study which was recently carried out at CLS Corporate Language Services AG, where COMPRENDIUM is in the meantime being employed as corporate MT system.	Machine Translation	2014	10	2.5
Controlling Politeness in Neural Machine Translation via Side Constraints	Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to...	Machine Translation	2016	6	3
Models and Inference for Prefix-Constrained Machine Translation	react-text: 435 Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while com- puting alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi...  /react-text  react-text: 436   /react-text [Show full abstract]	Machine Translation	2016	6	3
Statistical machine translation in the translation curriculum: overcoming obstacles and empowering translators	In this paper we argue that the time is ripe for translator educators to engage with Statistical Machine Translation (SMT) in more profound ways than they have done to date. We explain the basic principles of SMT and reflect on the role of humans in SMT workflows. Against a background of diverging opinions on the latter, we argue for a holistic approach to the integration of SMT into translator training programmes, one that empowers rather than marginalises translators. We discuss potential barriers to the use of SMT by translators generally and in translator training in particular, and propose some solutions to problems thus identified. More specifically, cloud-based services are proposed as a means of overcoming some of the technical and ethical challenges posed by more advanced uses of SMT in the classroom. Ultimately the paper aims to pave the way for the design and implementation of a new translator-oriented SMT syllabus at our own University and elsewhere.	machine translation/curriculum design	2014	10	2.5
Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations	We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.	Translation Approach/Natural Language	2017	6	6
Online adaptation to post-edits for phrase-based statistical machine translation	Recent research has shown that accuracy and speed of human translators can benefit from post-editing output of machine translation systems, with larger benefits for higher quality output. We present a	Statistical machine translation	2014	10	2.5
Incorporating Global Visual Features into Attention-Based Neural Machine Translation	We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.	Machine Translation/Visual Features	2017	6	6
The New Thot Toolkit for Fully-Automatic and Interactive Statistical Machine Translation	We present the new THOT toolkit for fully- automatic and interactive statistical ma- chine translation (SMT). Initial public ver- sions of THOT date back to 2005 and did only include estimation of phrase-based models. By contrast, the new version of- fers several new features that had not been previously incorporated. The key innova- tions provided by the toolkit are computer- aided translation, including post-editing and interactive SMT, incremental learn- ing and robust generation of alignments at phrase level. In addition to this, the toolkit also provides standard SMT fea- tures such as fully-automatic translation, scalable and parallel algorithms for model training, client-server implementation of the translation functionality, etc. The toolkit can be compiled in Unix-like and Windows platforms and it is released un- der the GNU Lesser General Public Li- cense (LGPL).	Statistical Machine Translation	2014	10	2.5
Bridging Neural Machine Translation and Bilingual Dictionaries	Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.	Machine Translation	2016	6	3
Compression of Neural Machine Translation Models via Pruning	Abstract:  Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.	Machine Translation	2016	6	3
Memory-enhanced Decoder for Neural Machine Translation	We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \textsc{MemDec}. At each time during decoding, \textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\cite{RNNsearch} to store the representation of source sentence, the memory in \textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set.	Machine Translation	2016	6	3
HUME: Human UCCA-Based Evaluation of Machine Translation	Abstract:  Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.	Machine Translation	2016	6	3
Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder	Abstract:  In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.	Machine Translation	2016	6	3
Zero-resource machine translation by multimodal encoder–decoder network with multimedia pivot	"Abstract:  We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the ""pivot"", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best."		2016	6	3
An evaluation methodology for English to Sinhala machine translation	This paper presents evaluation methodology for English to Sinhala machine tran slation system. The English to Sinhala machine translation system has been developed by using Multi Agent Approach and powered through the concept of “Varanegeema”. Translation system works through the communication among nine agents namely English Morphological Analyzer Agent, English Parser Agent, English to Sinhala Base Word Translator Agent, Sinhala Morphological Generator Agent, Sinhala Parser agent, Transliteration agent, Intermediate Editor agent, Message Space Agent and Request agent. The evaluation was conducted through three steps. As the first step, evaluation was conducted through the white box testing approach and tested each module in the machine translation system through the developed testing tools. Then, evaluated the system performance and calculated the error rate through the result of the evaluation test bed. Finally, Intelligibility and the Accuracy test will be conducted through the human support. The experimental result shows 89% accuracy of the overall system and 7.2% word error rate and the 5.4% sentence error rate. Details of the evaluation and results are given in the paper	machine translation/evaluation methodology	2016	6	3
A survey of word reordering in statistical machine translation: Computational models and language phenomena	2016. A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42.Arianna Bisazza and Marcello Federico. 2016. A Survey of Word Reordering in Statistical Machine ...	machine translation	2016	6	3
How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs	Abstract:  Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English->German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.	Machine Translation	2016	6	3
Learning local word reorderings for hierarchical phrase-based statistical machine translation	Statistical models for reordering source words have been used to enhance hierarchical phrase-based statistical machine translation. There are existing word-reordering models that learn reorderings for	statistical machine translation	2016	6	3
Context Gates for Neural Machine Translation	In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.	Machine Translation	2017	6	6
Six Challenges for Neural Machine Translation	We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.	Machine Translation	2017	6	6
The Impact of Machine Translation Quality on Human Post-Editing	****	Translation Quality	2014	9	2.25
Interactive Attention for Neural Machine Translation	Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.	Machine Translation	2016	6	3
Real Time Adaptive Machine Translation for Post-Editing with cdec and TransCenter	As sentences are translated, the models gain valuable context infor- mation, allowing them toadapt to the specific tar  On- line learning of log-linear weights in interactive ma- chine translation Online adaptation strategies for statistical machine translation in post- editing scenarios	Machine Translation/Real Time	2014	9	2.25
Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation	We present a parser for Abstract Meaning Representation (AMR). We treat English-to-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser improves upon state-of-the-art results by 7 Smatch points.	Machine Translation/computational linguistics	2015	8	2.6666666667
Enhancing Statistical Machine Translation with Bilingual Terminology in a CAT Environment	ABSTRACT  In this paper, we address the problem of extracting and integrating bilingual terminology into a Statistical Machine Translation (SMT) system for a Computer Aided Translation (CAT) tool scenario. We develop a framework that, taking as input a small amount of parallel in-domain data, gathers domain-specific bilingual terms and injects them in an SMT system to enhance the translation productivity. Therefore, we investigate several strategies to extract and align bilingual terminology, and to embed it into the SMT. We compare two embedding methods that can be easily used at run-time without altering the normal activity of an SMT system: XML markup and the cache-based model. We tested our framework on two different domains showing improvements up to 15% BLEU score points.	Machine Translation	2014	9	2.25
Bilingual Sentiment Consistency for Statistical Machine Translation	Abstract In this paper, we explore bilingual sentiment knowledge for statistical machine translation (SMT). We propose to explicitly model the consistency of sentiment between the source and target side with a lexicon-based approach. The experiments show that the proposed model significantly improves Chinese-to-English NIST translation over a competitive baseline.	Statistical Machine Translation	2014	9	2.25
Dependency-based Pre-ordering for Chinese-English Machine Translation	Abstract In statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders. This paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT. We present a set of dependency-based preordering rules which improved the BLEU score by 1.61 on the NIST 2006 evaluation data. We also investigate the accuracy of the rule set by conducting human evaluations.	Chinese-English Machine Translation	2014	9	2.25
Optimizing instance selection for statistical machine translation with feature decay algorithms	We introduce FDA5 for efficient parameterization, optimization, and implementation of feature decay algorithms (FDA), a class of instance selection algorithms that use feature decay. FDA increase the diversity of the selected training set by devaluing features (i.e., n-grams) that have already been included. FDA5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with five parameters. We present optimization techniques that allow FDA5 to adapt these functions to in-domain and out-of-domain translation tasks for different language pairs. In a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. In machine translation experiments performed on the 2 million sentence English-German section of the Europarl corpus, we show that a subset of the training set selected by FDA5 can gain up to 3.22 BLEU points compared to a randomly selected subset of the same size, can gain up to 0.41 BLEU points compared to using all of the available training data using only 15% of it, and can reach within 0.5 BLEU points to the full training set result by using only 2.7% of the full training data. FDA5 peaks at around 8M words or 15% of the full training set. In an active learning setting, FDA5 minimizes the human effort by identifying the most informative sentences for translation and FDA gains up to 0.45 BLEU points using 3/5 of the available training data compared to using all of it and 1.12 BLEU points compared to random training set. In translation tasks involving English and Turkish, a morphologically rich language, FDA5 can gain up to 11.52 BLEU points compared to a randomly selected subset of the same size, can achieve the same BLEU score using as little as 4% of the data compared to random instance selection, and can exceed the full dataset result by 0.78 BLEU points. FDA5 is able to reduce the time to build a statistical machine translation system to about half with 1M words using only 3% of the space for the phrase table and 8% of the overall space when compared with a baseline system using all of the training data available yet still obtain only 0.58 BLEU points difference with the baseline system in out-of-domain translation.	domain adaptation/machine translation/information retrieval	2015	8	2.6666666667
